{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887b7310",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44fca659c1e85aa18c1a44bcb17af98a",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd7fa9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25a2663f7e80ba30383d15a954b5ac20",
     "grade": false,
     "grade_id": "heading",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 04: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00691499",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1320a7ca90271a4b79fc220d9055164",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 12, 2024**. If you need help (and Google and other resources were not enough), use the StudIP forum, contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac58175",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11b4627bdfadab21d37409967b4656b4",
     "grade": false,
     "grade_id": "cell-03251c978769cd79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 1: Kmeans Clustering (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9388ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f139256a7e6c3f38e098b7b39e352cd0",
     "grade": false,
     "grade_id": "cell-b0db1893cc822d0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Perform K-means clustering to divide the following datset into 2 clusters by hand.\n",
    "\n",
    "| x1 | x2 |x3 |\n",
    "|----|----|---|\n",
    "|  1 | 1  | 1 |\n",
    "|  2 | 1  | 1 |\n",
    "|  3 | 1  | 2 |\n",
    "|  2 | 3  | 4 |\n",
    "|  5 | 5  | 5 |\n",
    "|  3 | 2  | 1 |\n",
    "\n",
    "Start with the following centroids for the two clusters: $(1,1,1)$ and $(5,5,5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c932cd3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "059f2a347cc1d1db7a0fe4a68d3fc537",
     "grade": true,
     "grade_id": "cell-987bb819c6249404",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "#distances aller Punkte zum clustercenter berechnen, schauen zu welchem center jeder Punkt näher ist, daraus Zuordnung bestimmen und neues center berechnen, bis sich clustercentren nicht mehr (stark) verändern\n",
    "-> do it in two-dimensional space (2-norm, euclidian distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146bc61",
   "metadata": {},
   "source": [
    "**b)** $k$-mean clustering of a given dataset can result in different outcomes. Explain, at which point the algorithms is indeterministic and explain how to compare the quallity of different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bbe0c5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63fd9fe723e9d06f0a636c23ba5fcb11",
     "grade": true,
     "grade_id": "cell-00ab43266b028866",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The only point in time when the algorithm is indeterministic is the first step, in which the K reference vectors are chosen randomly. After this step, every other one is determined by the datapoints and the cluster-centers (the reference vectors).\n",
    "In order to compare the quality of different outcomes one can take a look at the number of points assigned to each cluster. For example should empty clusters be avoided. Another method for comparing is taking a look at a visualisation of the datapoints and assignment to clusters (if there are more than 2 dimensions, one can consider using PCA in order to reduce dimensions). Another method of comparison is looking at intra- vs inter-cluster distances. Here, intra-cluster distance should be smaller than inter-cluster distances, since then we know that the points assigned to each cluster are closer to points in their cluster than to those points in other clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcca234",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f5a48a0bd979712b9396c6f8c994573",
     "grade": false,
     "grade_id": "cell-ad9b9b0920004fe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** The pseudocode for k-means algorithm on (ML-05, slide 27) uses as the termination condition ($\\exists k \\in[1,K]: \\|\\vec{w}_k(t)-\\vec{w}_k(t-1)\\|>\\varepsilon$). Discuss this condition from a theoretical and practical perspective and name alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88bdba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d6acbe921b42499c0f97fca06bfc9fa",
     "grade": true,
     "grade_id": "cell-d93c92396260c1f3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The condition tries to determine wether the assignment of the points to the clusters still changes. It does this via comparing the calculated centroid for each cluster with the centroid of that cluster in the previous step. If the chosen $\\varepsilon$ is too small or even negative, the algorithm might never terminate, for the distance between two points cannot be negative. Another case is when there are datapoints which oscillate between the clusters in the final step and keep changing the centroids. If the $\\varepsilon$ is too big on the other hand the termination condition might be too coarse and it might stop the clustering too early.\n",
    "Alternatives are checking if the number of points assigned to each cluster still changes. This however would stop the clustering prematurely too often, since no quantitative change might occur while qualitatively (which point is assigned to which cluster) things are still changing. One could also check for every datapoint to which cluster it is assigned and stop the clustering when e.g. 98 % of the datapoints have stopped changing their assignment. This is a worse variation of our original criterion, since it needs more memory as every point has to remember their previous cluster instead of just keeping track of the centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f7c63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "461a7d778007326be46ab3e34bd87970",
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: k-means Clustering (7 points)\n",
    "\n",
    "## a) Implement k-means clustering. Plot the results for $k = 7$ and $k = 3$ in colorful scatter plots.\n",
    "\n",
    "How could one handle situations when one or more clusters end up containing 0 elements? Handle these situtation in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d9b9b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf00647c6d6351d3d3cb5ade72ddb9c0",
     "grade": true,
     "grade_id": "3_code",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def kmeans(data, k=3):\n",
    "    \"\"\"\n",
    "    Applies k-means clustering to given data.\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): a numpy array of shape (n, 2), providing n\n",
    "                        2-dimensional datapoints\n",
    "        k (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        labels (ndarray): Numpy array containing numbers (=labels) with the same order as the data points\n",
    "                          e.g. (1,1,3,5,5,5,...)\n",
    "        centroids(ndarray): vector representation of cluster centers of shape (k, 2)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # probably look online for implementations\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e23787",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "data = np.loadtxt('clusterData.txt')\n",
    "\n",
    "# Test experiments with a different number of clusters and different \n",
    "# number of runs here.\n",
    "# You can define the number of clusters and how often k-means is called\n",
    "# allowing to investigate the inlfuenece of the number of clusters and \n",
    "# of the different random cluster initializations per run.\n",
    "\n",
    "\n",
    "# Experiment 1: One run with k=2\n",
    "experiment = ((2,1),)\n",
    "# Experiment 2: One run with k=3, one run with k=7\n",
    "#experiment = ((3,1),(7,1))\n",
    "# Experiment 3: Five run with k=7\n",
    "#experiment = ((7,5),)\n",
    "\n",
    "\n",
    "for params in  experiment:\n",
    "    k = params[0]\n",
    "    for i in range(1, params[1]+1):\n",
    "        labels, centroids = kmeans(data, k)\n",
    "        \n",
    "        assert isinstance(labels, np.ndarray), \"'labels' should be a numpy array!\"       \n",
    "        assert isinstance(centroids, np.ndarray), \"'centroids' should be a numpy array!\"    \n",
    "        assert labels.shape==(data.shape[0],), \"Each data point needs a label!\"\n",
    "        assert centroids.shape==(k,data.shape[1]), (\"k centroids with the same dimensionality \"\n",
    "            \"as the data are needed!\")\n",
    "        \n",
    "        kmeans_fig = plt.figure('k-means with k={}, i={}'.format(k,i))\n",
    "        plt.scatter(data[:,0], data[:,1], c=labels)\n",
    "        plt.scatter(centroids[:,0], centroids[:,1], \n",
    "                    c=list(set(labels)), alpha=.1, marker='o',\n",
    "                    s=np.array([len(labels[labels==label]) for label in set(labels)])*100)\n",
    "        plt.title('k = {}, i = {}'.format(k, i))\n",
    "        kmeans_fig.canvas.draw()\n",
    "        plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8dbafb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "395d2bbef77d708a98cf427b9acbf0b4",
     "grade": false,
     "grade_id": "cell-fa945b11f93016c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b) Why might the clustering for k=7 not look optimal? \n",
    "What happens if you run the algorithm several times?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22814bcc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdd4510850bd4d93e9099c9c21de9198",
     "grade": true,
     "grade_id": "cell-16925a76b18a76fe",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b795d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84a260ffca76ef1315b343510fa14d05",
     "grade": false,
     "grade_id": "cell-384825e6e33de9dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 3: Scipy Clustering (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34de92e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e39043f8641812b1970edc8a6b115ea",
     "grade": false,
     "grade_id": "cell-0ba00056c5771233",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The documentation of [Scikit-Learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) compares the characteristics of different cluster algorithms by showing their performance on various toy datasets. [Hierarchical](https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py) is clustering is shown in detail in a seperate notebook.\n",
    "The lecture focuses on the clustering methods for agglomerative with single, complete, average and ward (ML-5 Slide: 8-12), kmeans (ML-5 Slide: 26) and gaussian mixture (ML-5 Slide: 36), which are implemented in the code below.\n",
    "You can modify the random state for the dataset generation, clustering approach and number of samples to get a deeper understanding for the different clustering algorithms.\n",
    "Below the code are questions about the datasets and clustering algorithms which you should answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from itertools import cycle, islice\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### MODIFY VALUES ###\n",
    "\n",
    "random_state_dataset = 170\n",
    "random_state_clustering = 170\n",
    "n_samples = 1500\n",
    "\n",
    "### CREATE DATASETS ###\n",
    "\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=random_state_dataset\n",
    ")\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=random_state_dataset)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=random_state_dataset)\n",
    "rng = np.random.RandomState(random_state_dataset)\n",
    "no_structure = rng.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state_dataset)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state_dataset\n",
    ")\n",
    "\n",
    "### Start CLUSTERING ###\n",
    "\n",
    "# Set up cluster parameters\n",
    "plt.figure(figsize=(9 * 1.3 + 2, 14.5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\"n_neighbors\": 10, \"n_clusters\": 3, \"random_state\": random_state_clustering}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {\"n_clusters\": 2}),\n",
    "    (noisy_moons, {\"n_clusters\": 2}),\n",
    "    (varied, {\"n_neighbors\": 2}),\n",
    "    (aniso, {\"n_neighbors\": 2}),\n",
    "    (blobs, {}),\n",
    "    #(no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    single = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"single\"\n",
    "    )\n",
    "    average = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"average\"\n",
    "    )\n",
    "    complete = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"complete\"\n",
    "    )\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"ward\"\n",
    "    )\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params[\"n_clusters\"],\n",
    "        covariance_type=\"full\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    #kmeans = cluster.MiniBatchKMeans(\n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "        n_init=\"auto\"\n",
    "    )\n",
    "    \n",
    "    clustering_algorithms = (\n",
    "        (\"Single Linkage\", single),\n",
    "        (\"Average Linkage\", average),\n",
    "        (\"Complete Linkage\", complete),\n",
    "        (\"Ward Linkage\", ward),\n",
    "        (\"Gaussian Mixture\", gmm),\n",
    "        (\"KMeans\", kmeans)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-3.5, 3.5)\n",
    "        plt.ylim(-3.5, 3.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.22,\n",
    "            #0.50,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "            #verticalalignment=\"top\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793fdb50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d888373a012f2ab439d3cc13596d04e",
     "grade": false,
     "grade_id": "cell-d96b57370b25de72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## a) Which of the clusters obtained by the different algorithms would be seen as good for the different toy datasets and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d4eb4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "081cc68bb9b3e02703c899bde221f9f5",
     "grade": true,
     "grade_id": "cell-603787a25e76cca3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5073ab6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdf86726c99af9fd24fb3eaff5e74b88",
     "grade": false,
     "grade_id": "cell-4943e19cbd81a2ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b) Explain why single linkage clustering creates good results for dataset one and two, but bad results for dataset 3, 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e9712",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a4b7ae6f06ba30b11c162236cdd1139",
     "grade": true,
     "grade_id": "cell-661a6bd70f6524d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f5219",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d04d6f783d36feb21d7303efbac60faf",
     "grade": false,
     "grade_id": "cell-36f8e97a405ffda2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## c) Why does ward work good on dataset 3 but bad on dataset 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017e87e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "836782ba4549b3d2d0c7734896432923",
     "grade": true,
     "grade_id": "cell-018e1d2afe53999a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9114e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b2ae0ea1f71cefe8b5f10b8f797a8fd",
     "grade": false,
     "grade_id": "cell-3fcc447f5481b905",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) explain the difference between gaussian mixture and kmeans clustering for dataset 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd122e6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1791d040bf7aaa6761428f01f25975c0",
     "grade": true,
     "grade_id": "cell-b250f83157c533de",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c3c9c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68370cbdcf484a2805a14c007f10ed48",
     "grade": false,
     "grade_id": "cell-0c3d7c90e76d3e69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## e) Why do the methods for gaussian mixture and kmeans need a random state and the single, average complete and ward linkage do not require these?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d663e6f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c142838eba854ca3e211d6ff545e3ee9",
     "grade": true,
     "grade_id": "cell-7996ff50e00d0953",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
