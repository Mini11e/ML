{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2b7a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e6288",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "heading",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 04: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212017b1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 12, 2024**. If you need help (and Google and other resources were not enough), use the StudIP forum, contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87711ec",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03251c978769cd79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 1: Kmeans Clustering (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167abecc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0db1893cc822d0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Perform K-means clustering to divide the following datset into 2 clusters by hand.\n",
    "\n",
    "| x1 | x2 |x3 |\n",
    "|----|----|---|\n",
    "|  1 | 1  | 1 |\n",
    "|  2 | 1  | 1 |\n",
    "|  3 | 1  | 2 |\n",
    "|  2 | 3  | 4 |\n",
    "|  5 | 5  | 5 |\n",
    "|  3 | 2  | 1 |\n",
    "\n",
    "Start with the following centroids for the two clusters: $(1,1,1)$ and $(5,5,5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19822e5e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-987bb819c6249404",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Step 1: Initialize the centroids for the two clusters randomly. Let's say we choose the following centroids:\n",
    "\n",
    "Cluster 1 centroid = (1, 1, 1)\n",
    "Cluster 2 centroid = (5, 5, 5)\n",
    "\n",
    "Step 2: Assign each data point to the nearest centroid. We can calculate the distance between each data point and the centroids using the Euclidean distance formula.\n",
    "\n",
    "The distances between each data point and the centroids are as follows:\n",
    "\n",
    "\n",
    "| Data Point | Distance to Cluster 1 centroid | Distance to Cluster 2 centroid | Assigned Cluster |\n",
    "|:----------:|:------------------------------:|:------------------------------:|:----------------:|\n",
    "| (1, 1, 1)  | 0                              | 5.196                          | Cluster 1        |\n",
    "| (2, 1, 1)  | 1                              | 4.899                          | Cluster 1        |\n",
    "| (3, 1, 2)  | 1.414                          | 4.242                          | Cluster 1        |\n",
    "| (2, 3, 4)  | 5.744                          | 1.732                          | Cluster 2        |\n",
    "| (5, 5, 5)  | 7.071                          | 0                              | Cluster 2        |\n",
    "| (3, 2, 1)  | 1.732                          | 4.899                          | Cluster 1        |\n",
    "\n",
    "Based on the distances, we assign each data point to the nearest centroid. Data points (1, 1, 1), (2, 1, 1), (3, 1, 2), and (3, 2, 1) are assigned to Cluster 1, and data points (2, 3, 4) and (5, 5, 5) are assigned to Cluster 2.\n",
    "\n",
    "Step 3: Recalculate the centroids for each cluster by taking the mean of all the data points in the cluster.\n",
    "\n",
    "The new centroids for the two clusters are as follows:\n",
    "\n",
    "Cluster 1 centroid = ((1+2+3+3)/4, (1+1+1+2)/4, (1+1+2+1)/4) = (2.25, 1.25, 1.25)\n",
    "Cluster 2 centroid = ((2+5)/2, (3+5)/2, (4+5)/2) = (3.5, 4, 4.5)\n",
    "\n",
    "Step 4: Repeat steps 2 and 3 until convergence is reached.\n",
    "\n",
    "We can see that after one iteration, the assigned clusters have changed. Data points (1, 1, 1), (2, 1, 1), and (3, 1, 2) are now assigned to Cluster 1, and data points (2, 3, 4), (5, 5, 5), and (3, 2, 1) are assigned to Cluster 2.\n",
    "\n",
    "We can now recalculate the centroids for each cluster:\n",
    "\n",
    "Cluster 1 centroid = ((1+2+3)/3, (1+1+1)/3, (1+1+2)/3) = (2, 1, 1.33)\n",
    "Cluster 2 centroid = ((2+5+3)/3, (3+5+2)/3, (4+5+1)/3) = (3.33, 3.33, 3.33)\n",
    "\n",
    "\n",
    "After the second iteration, the assigned clusters do not change, so we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c273d",
   "metadata": {},
   "source": [
    "**b)** $k$-mean clustering of a given dataset can result in different outcomes. Explain, at which point the algorithms is indeterministic and explain how to compare the quallity of different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4a679",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-00ab43266b028866",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The algorithm is indeterministic because of the initial choice of cluster centers.  All subsequent steps are fully deterministic. \n",
    "\n",
    "Different outcomes can be assessed by the cumulated distance of points to their respective cluster centers.  The smaller that distance, the better the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda88ac",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad9b9b0920004fe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** The pseudocode for k-means algorithm on (ML-05, slide 27) uses as the termination condition ($\\exists k \\in[1,K]: \\|\\vec{w}_k(t)-\\vec{w}_k(t-1)\\|>\\varepsilon$). Discuss this condition from a theoretical and practical perspective and name alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d032594",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d93c92396260c1f3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The conditions states that the algorithm should terminate when the cluster centers do not more than a given threshold.\n",
    "\n",
    "It is not even clear, that the steps of the algorithm do decrease (is it possible to create a simple example demonstrating that idea?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7841a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: k-means Clustering (7 points)\n",
    "\n",
    "## a) Implement k-means clustering. Plot the results for $k = 7$ and $k = 3$ in colorful scatter plots.\n",
    "\n",
    "How could one handle situations when one or more clusters end up containing 0 elements? Handle these situtation in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ee489",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3_code",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def kmeans(data, k=3):\n",
    "    \"\"\"\n",
    "    Applies k-means clustering to given data.\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): a numpy array of shape (n, 2), providing n\n",
    "                        2-dimensional datapoints\n",
    "        k (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        labels (ndarray): Numpy array containing numbers (=labels) with the same order as the data points\n",
    "                          e.g. (1,1,3,5,5,5,...)\n",
    "        centroids(ndarray): vector representation of cluster centers of shape (k, 2)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Initial centroids are k random samples from the data.\n",
    "    centroids = data[np.random.randint(0, data.shape[0], k)]\n",
    "    old_centroids = np.zeros(centroids.shape)\n",
    "    \n",
    "    # Initial labels are all.. something.\n",
    "    labels = np.ndarray(data.shape[0])\n",
    "    \n",
    "    # Lets keep count of our iterations to avoid infinite loops.\n",
    "    iterations = 0\n",
    "    \n",
    "    while np.any(np.abs(centroids - old_centroids) > np.finfo(float).eps) and iterations < 1000:\n",
    "        # Keep count of iterations and remember current centroids for change calculation.\n",
    "        iterations += 1\n",
    "        # Copy the centroids and keep them for break condition check.\n",
    "        old_centroids = np.copy(centroids)\n",
    "        \n",
    "        # Calculate new labels. Labels are the index of their minimal distance to any centroid.\n",
    "        labels = np.argmin(cdist(centroids, data), axis=0)\n",
    "        \n",
    "        # Update centroids using the new cluster labels.\n",
    "        for label in range(k): \n",
    "            # Check for empty clusters.\n",
    "            if np.any(labels == label):\n",
    "                # Cluster is not empty, move its centroid to new mean.\n",
    "                centroids[label, :] = np.mean(data[labels == label], axis=0)\n",
    "            else:\n",
    "                # Cluster is empty, set its centroid to the furthest outlier.\n",
    "                blacksheep = np.argmax(cdist(centroids, data), axis=0)\n",
    "                centroids[label, :] = data[blacksheep, :]\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba46f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "data = np.loadtxt('clusterData.txt')\n",
    "\n",
    "# Test experiments with a different number of clusters and different \n",
    "# number of runs here.\n",
    "# You can define the number of clusters and how often k-means is called\n",
    "# allowing to investigate the inlfuenece of the number of clusters and \n",
    "# of the different random cluster initializations per run.\n",
    "\n",
    "\n",
    "# Experiment 1: One run with k=2\n",
    "experiment = ((2,1),)\n",
    "# Experiment 2: One run with k=3, one run with k=7\n",
    "#experiment = ((3,1),(7,1))\n",
    "# Experiment 3: Five run with k=7\n",
    "#experiment = ((7,5),)\n",
    "\n",
    "\n",
    "for params in  experiment:\n",
    "    k = params[0]\n",
    "    for i in range(1, params[1]+1):\n",
    "        labels, centroids = kmeans(data, k)\n",
    "        \n",
    "        assert isinstance(labels, np.ndarray), \"'labels' should be a numpy array!\"       \n",
    "        assert isinstance(centroids, np.ndarray), \"'centroids' should be a numpy array!\"    \n",
    "        assert labels.shape==(data.shape[0],), \"Each data point needs a label!\"\n",
    "        assert centroids.shape==(k,data.shape[1]), (\"k centroids with the same dimensionality \"\n",
    "            \"as the data are needed!\")\n",
    "        \n",
    "        kmeans_fig = plt.figure('k-means with k={}, i={}'.format(k,i))\n",
    "        plt.scatter(data[:,0], data[:,1], c=labels)\n",
    "        plt.scatter(centroids[:,0], centroids[:,1], \n",
    "                    c=list(set(labels)), alpha=.1, marker='o',\n",
    "                    s=np.array([len(labels[labels==label]) for label in set(labels)])*100)\n",
    "        plt.title('k = {}, i = {}'.format(k, i))\n",
    "        kmeans_fig.canvas.draw()\n",
    "        plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f204b37",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa945b11f93016c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b) Why might the clustering for k=7 not look optimal? \n",
    "What happens if you run the algorithm several times?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76786d7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-16925a76b18a76fe",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "K-Means works best for datasets in which clusters have the same circular shape and the same amount of datapoints per cluster. Opposed to a Mixture of Gaussians, in which the different distributions/clusters are weighted and the standard deviations may differ between distributions and also between different dimensions, in K-Means the variance is fixed and the same metric is used for all clusters. \n",
    "\n",
    "In this example inter- and intra cluster variance varies together with the number of datapoints per cluster.\n",
    "\n",
    "Moreover, the outcome of K-Means is a local minima, depending on the random initialization of the cluster centers. This local minimal may not be the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ea46b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-384825e6e33de9dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 3: Scipy Clustering (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370e9a2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ba00056c5771233",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The documentation of [Scikit-Learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) compares the characteristics of different cluster algorithms by showing their performance on various toy datasets. [Hierarchical](https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py) is clustering is shown in detail in a seperate notebook.\n",
    "The lecture focuses on the clustering methods for agglomerative with single, complete, average and ward (ML-5 Slide: 8-12), kmeans (ML-5 Slide: 26) and gaussian mixture (ML-5 Slide: 36), which are implemented in the code below.\n",
    "You can modify the random state for the dataset generation, clustering approach and number of samples to get a deeper understanding for the different clustering algorithms.\n",
    "Below the code are questions about the datasets and clustering algorithms which you should answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6987df",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ce2051c485d91a93",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from itertools import cycle, islice\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### MODIFY VALUES ###\n",
    "\n",
    "random_state_dataset = 170\n",
    "random_state_clustering = 170\n",
    "n_samples = 1500\n",
    "\n",
    "### CREATE DATASETS ###\n",
    "\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=random_state_dataset\n",
    ")\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=random_state_dataset)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=random_state_dataset)\n",
    "rng = np.random.RandomState(random_state_dataset)\n",
    "no_structure = rng.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state_dataset)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state_dataset\n",
    ")\n",
    "\n",
    "### Start CLUSTERING ###\n",
    "\n",
    "# Set up cluster parameters\n",
    "plt.figure(figsize=(9 * 1.3 + 2, 14.5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\"n_neighbors\": 10, \"n_clusters\": 3, \"random_state\": random_state_clustering}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {\"n_clusters\": 2}),\n",
    "    (noisy_moons, {\"n_clusters\": 2}),\n",
    "    (varied, {\"n_neighbors\": 2}),\n",
    "    (aniso, {\"n_neighbors\": 2}),\n",
    "    (blobs, {}),\n",
    "    #(no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    single = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"single\"\n",
    "    )\n",
    "    average = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"average\"\n",
    "    )\n",
    "    complete = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"complete\"\n",
    "    )\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], \n",
    "        linkage=\"ward\"\n",
    "    )\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params[\"n_clusters\"],\n",
    "        covariance_type=\"full\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    #kmeans = cluster.MiniBatchKMeans(\n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "        n_init=\"auto\"\n",
    "    )\n",
    "    \n",
    "    clustering_algorithms = (\n",
    "        (\"Single Linkage\", single),\n",
    "        (\"Average Linkage\", average),\n",
    "        (\"Complete Linkage\", complete),\n",
    "        (\"Ward Linkage\", ward),\n",
    "        (\"Gaussian Mixture\", gmm),\n",
    "        (\"KMeans\", kmeans)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-3.5, 3.5)\n",
    "        plt.ylim(-3.5, 3.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.22,\n",
    "            #0.50,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "            #verticalalignment=\"top\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781462cd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d96b57370b25de72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## a) Which of the clusters obtained by the different algorithms would be seen as good for the different toy datasets and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2127288",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-603787a25e76cca3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "| Dataset | Single linkage | Average Linkage | Complete Linkage | Ward Linkage | Gaussian Mixture | KMeans |\n",
    "|---------|----------------|-----------------|------------------|--------------|------------------|--------|\n",
    "| 1       | good           | bad             | bad              | bad          | bad              | bad    |\n",
    "| 2       | good           | bad             | bad              | bad          | bad              | bad    |\n",
    "| 3       | bad            | bad             | bad              | good         | good             | bad    |\n",
    "| 4       | bad            | bad             | bad              | bad          | good             | bad    |\n",
    "| 5       | bad            | good            | good             | good         | good             | good   |\n",
    "\n",
    "A good cluster can be described as by it having distinctly different characteristics. These can be described by density or distance to other clusters. A well chosen cluster algorithm is able to use these characteristics by modelling the underlying conditions that created the data. For example: While Single linkage works well on datasets, that show a large minimum distance between clusters (like 1 and 2), gaussian mixture clustering works well on clusters that stem from a gaussian distribution (like 3, 4 and 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84ef55",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4943e19cbd81a2ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b) Explain why single linkage clustering creates good results for dataset one and two, but bad results for dataset 3, 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485f4ba",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-661a6bd70f6524d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "single linkage tends to create clusters by chaining, by which points close to each other are combined into a cluster. Dataset 1 and 2 shows clusters which have a large minimum distance between the clusters, while the density inside a cluster is high, which results in good clustering results.\n",
    "\n",
    "The results for datasets 3, 4, 5 show bad results since the minimum distance between clusters are low, while the density of a cluster can vary. For these datasets, single linkage tends to build clusters out of individual points or small clusters, which are further apart from the rest of the points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18a570",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36f8e97a405ffda2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## c) Why does ward work good on dataset 3 but bad on dataset 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32928800",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-018e1d2afe53999a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ward tries to merge the clusters in a way by which the total variance is minimized.\n",
    "In dataset 3 the the total variance is smaller if one cluster contains the wide spread points in blue while the other clusters contain the more conentrated clusters in orange and green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f033695",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3fcc447f5481b905",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) explain the difference between gaussian mixture and kmeans clustering for dataset 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37c6c0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b250f83157c533de",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "gaussian mixture models try to find optimal matching gaussian (bivariate) distributions to the data.\n",
    "Dataset 3 and 4 stem from bivariate distributions.\n",
    "Gaussian mixture models allow to modify the underlying mean and covariance matrix, by which the distributions can be spread further out or be streched in one direction (shaped by an affine transformation).\n",
    "This leads to the effect, that the gaussian mixture model creates good clusters for datasets 3 and 4.\n",
    "\n",
    "In contrast kmeans only creates spherical clusters (creates a voronoi tesselation of the space).\n",
    "Kmeans is not able to create different underlying distributions for each cluster but only attends to the distance between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883662fc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c3d7c90e76d3e69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## e) Why do the methods for gaussian mixture and kmeans need a random state and the single, average complete and ward linkage do not require these?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde2d2d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7996ff50e00d0953",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "single, average, complete and ward linkage do not sample from random distributions. \n",
    "They only work on the given data and do not rely on an initial state.\n",
    "\n",
    "Gaussuan mixture and kmeans rely on an initial state and the given data. They operate by iteratively updating this state until reaching a local optimum. Their optimum may depend on the initial state and this different local optima can be reached."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
