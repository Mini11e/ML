{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b99cb42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44fca659c1e85aa18c1a44bcb17af98a",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c4f94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9e7d36d014ba44abbf2fe8283bbe76c",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118ada3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bca0d5cf17eceaaa63368885a9076b77",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before end of **Sunday, June 30th, 2024**. If you need help (and Google and other resources were not enough), ask in the StudIP forum or feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder.\n",
    "\n",
    "**NOTE:** This is the last sheet. If you have passed already nine sheets successfully, you are not obliged to submit this sheet. However we highly recommend to have at least a look at it, as the content may still be relevant for the exam. You may also meet with your tutor to discuss the sheet.  In any case, if you do not intend to meet with your tutor, **please inform them in advance** to allow them to adapt their plans!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a38417",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e68973690b189766fcf97daf389a6c6",
     "grade": false,
     "grade_id": "cell-rl-th",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: Reinforcement Learning Theory (4 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d4fa0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efbbf95ba774832e36bacca726a6f90a",
     "grade": false,
     "grade_id": "cell-rl-th-aq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### a) Weak teacher\n",
    "\n",
    "Reinforcement learning is often described as being different from both supervised and unsupervised learning by providing a \"weak teacher\". Who is this \"teacher\" and why is she \"weak\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e43ae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b384f523c205c8c94386fa15d8c6e60",
     "grade": true,
     "grade_id": "cell-rl-th-aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb6851",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eefe866ea4d7bc42c8887a742121c16b",
     "grade": false,
     "grade_id": "cell-rl-th-bq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### b) Markov decision process\n",
    "\n",
    "Reinforcement learning is usually restricted to first-order Markov decision processes. What does this mean and what are the practical consequences. How would the formulae change when resorting to second-order Markov decision processes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f329b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14abbd9b4e749d4ccd0c3c2e3c79565c",
     "grade": true,
     "grade_id": "cell-rl-th-ba",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b642c25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47f562bbfa8fae7084d2c6a9eb6266a3",
     "grade": false,
     "grade_id": "cell-rl-th-cq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### c) The Q-function\n",
    "\n",
    "The Q-function can be written as $Q(s,a) = r(s,a) + \\gamma \\operatorname{max}_{a'} Q(\\delta(s,a),a')$.\n",
    "Explain this function in your own words. What does the Q-value represent? What is the problem with that formula and how is this problem resolved in Q-learning? How would you represent this function when implementing Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4556ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "babfe21efffd611cd7252578e269842a",
     "grade": true,
     "grade_id": "cell-rl-th-ca",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f5f5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9577c1bdc8868cf051858798346d2c9e",
     "grade": false,
     "grade_id": "cell-rl-th-dq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### d) Goal state\n",
    "\n",
    "In game playing there often is a goal state (game won/lost), and so is in the maze example from the lecture slides. Discuss the role of this goal state for the Q-Learning algorithm. Describe a learning scenario without a goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80433bdc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fcf6634b11dedab5f8da9fc05871864",
     "grade": true,
     "grade_id": "cell-rl-th-da",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34ee1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5f5f8f62afeda018703e45e4dc01a84",
     "grade": false,
     "grade_id": "ex-rl",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Reinforcement Learning (6 points)\n",
    "\n",
    "In this assignment you will have a look at the Q-Learning algorithm described in the lecture (ML-10 Slide 18). For this we generate a field with random rewards. A learning agent is then exploring the field and learns the optimal path to navigate through it. The code below is again filled with some ``TODO``s that should be filled by you in order to implement the Q-Learning algorithm. \n",
    "\n",
    "Below the code there are some questions! You also find a free-code field for a complete own implementation. You may use your own test mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x (int):            x dimension of the field\n",
    "        y (int):            y dimension of the field \n",
    "        num_rewards (int):  the number of rewards that should be randomly placed\n",
    "        max_reward (int):   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        ndarray: A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    \n",
    "    # Change or comment out to get different random data in each run\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    field = np.zeros((y,x), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(y), rand.randint(x)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f335052",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5392fc176308b0b1cd2d039240fe0329",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field, actions, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        All q values are stored in self.q - this is an array that has\n",
    "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
    "        in each field. The starting position self.pos is randomly initialized.\n",
    "        \n",
    "        Args:\n",
    "            field (ndarray):  the map\n",
    "            actions (list):   the available actions\n",
    "            gamma (float):    the gamma in the lecture slides\n",
    "        \n",
    "        Returns:\n",
    "            QLearning: An instance that can be used for Q-Learning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field.\n",
    "        self.field = field\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Remember the map extend for further navigation.\n",
    "        self.map_y = self.field.shape[0]\n",
    "        self.map_x = self.field.shape[1]\n",
    "        \n",
    "        # Create q value matrix.\n",
    "        self.q = np.zeros((len(self.actions), self.map_y, self.map_x))\n",
    "\n",
    "        # Start on a random position in the field.\n",
    "        self.pos = [np.random.randint(self.map_y), np.random.randint(self.map_x)]\n",
    "        self.fig, self.axes = plt.subplots(3, 3, num='QLearning State')\n",
    "        for ax in self.axes.flat:\n",
    "            ax.axis('off')\n",
    "\n",
    "    def get_coordinates(self, position, action):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain action, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \n",
    "        Args:\n",
    "            position (pair):  the current position\n",
    "            action (string):  the action that should be performed (one of: 'up', 'down', ...)\n",
    "            \n",
    "        Returns:\n",
    "            pair of int: the updated coordinates\n",
    "        \"\"\"\n",
    "        # return the right new coordinates depending on the position\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18. Note that you have attributes available as specified in the\n",
    "        __init__ method of this class, in addition to that is the FIELD variable that\n",
    "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
    "        stores the available actions.\n",
    "        \"\"\"\n",
    "        # Select a random action that should be performed next.\n",
    "        # Be careful to handle the case where you hit the wall!\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Receive the reward for the new position from the field.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Update the q-value for the performed action.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Update the position of the player to the new field.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the current state.\n",
    "        \"\"\"\n",
    "        fs = 8\n",
    "        for i, action in enumerate(self.actions):\n",
    "            ax = self.axes.flat[2*i + 1]\n",
    "            ax.cla()\n",
    "            ax.set(title=action)\n",
    "            ax.set_xticks(np.arange(self.q[i,:,:].shape[1]))\n",
    "            ax.set_yticks(np.arange(self.q[i,:,:].shape[0]))\n",
    "            ax.imshow(self.q[i,:,:], interpolation='None')\n",
    "            for j in range(self.q.shape[1]):\n",
    "                for k in range(self.q.shape[2]):\n",
    "                    text = ax.text(k, j, \"{:.1f}\".format(self.q[i,j,k],1),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=fs)\n",
    "                    plt.setp(text, path_effects=[\n",
    "        PathEffects.withStroke(linewidth=1, foreground=\"w\")])\n",
    "\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0396fe7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0773556e616a63404f47e24a57807b5",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "\n",
    "# Determine the size of the field, change this parameters as you like\n",
    "m_x = 5\n",
    "m_y = 4\n",
    "\n",
    "steps = 500\n",
    "\n",
    "actions = ['up','left','right','down']  # Those are the availabe actions for the QLearning.\n",
    "field = generate_field(m_x, m_y, num_rewards=5, max_reward=10) # The field that is used for learning.\n",
    "\n",
    "# Plotting the generated field\n",
    "fs = 18\n",
    "figure, ax = plt.subplots()\n",
    "#plt.axis('off')\n",
    "ax.imshow(field, interpolation='none')\n",
    "ax.set_xticks(np.arange(field.shape[1]))\n",
    "ax.set_yticks(np.arange(field.shape[0]))\n",
    "for j in range(field.shape[0]):\n",
    "    for k in range(field.shape[1]):\n",
    "        text = plt.text(k, j, field[j,k], ha=\"center\", va=\"center\", color=\"black\", fontsize=fs)\n",
    "        plt.setp(text, path_effects=[PathEffects.withStroke(linewidth=3, foreground=\"w\")])\n",
    "\n",
    "figure.suptitle(\"Field\",fontsize=fs)          \n",
    "figure.canvas.draw()\n",
    "\n",
    "\n",
    "# Generate a QLearning instance with the right parameters.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Now we perform steps many learning iterations on the field with\n",
    "# the generated QLearning instance.\n",
    "for i in range(steps):     \n",
    "    player.update()\n",
    "    player.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5e96d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abb7d97c008b533c8d2649cd3e54f3c6",
     "grade": false,
     "grade_id": "ex2_x",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Explain in your own words, how the algorithm works. What is depicted on the resulting plots. How can an action policy be derived from these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72768a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b61f070ffb0153e5dd4767692d90b83",
     "grade": true,
     "grade_id": "ex2c_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a36e6c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebf1427a0356c02f45ce383edafe01b5",
     "grade": false,
     "grade_id": "ex2_0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You are also free to write your complete own implementation of the QLearning algorithm (instead of completing the code above). Use the following cell for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594418f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3580d7ecdc9edca9b27f5493dde82a0b",
     "grade": true,
     "grade_id": "ex2_alternative",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c92a26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92e700fe976e3db108f3f729659f07ff",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assignment 3: Uncertainty and probability (6 points)\n",
    "\n",
    "This exercise will focus on concepts introduced in the first part of lecture (ML-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8fefb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc668602c44bc302af6f47876a0da1d2",
     "grade": false,
     "grade_id": "ex1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Modeling uncertainty\n",
    "\n",
    "In the lecture it is claimed that probabilities can summarize several factors:\n",
    "\n",
    "1. missing knowledge\n",
    "1. incapability to devise complete models of complex domains\n",
    "1. chance\n",
    "\n",
    "Think of an example for each of these points and explain how probabilities can be applied in modeling your example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59abf99",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72f109d9312c8ee012453726ed0c9c61",
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcdb9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1548fbd1548a2eb2423953e02d256add",
     "grade": false,
     "grade_id": "ex1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Inference by enumeration\n",
    "\n",
    "Given the full joint distribution below, calculate the following:\n",
    "\n",
    "\n",
    "|   -    |  toothache $\\wedge$ catch| toothache $\\wedge$ $\\neg$catch |  $\\neg$toothache $\\wedge$ catch | $\\neg$toothache $\\wedge$ $\\neg$catch |\n",
    "|---|---|---|---|---|\n",
    "| cavity  | 0.108 | 0.012 | 0.072 | 0.008 |\n",
    "| $\\neg$cavity | 0.016 | 0.064 | 0.144 | 0.576 |\n",
    "\n",
    "\n",
    "1. $P(\\neg \\text{toothache})$\n",
    "1. $P(\\text{cavity})$\n",
    "1. $P(\\text{toothache} \\mid \\text{cavity})$\n",
    "1. $P(\\text{cavity} \\mid \\text{toothache} \\vee \\text{catch})$\n",
    "\n",
    "If you are familiar with `pandas` you can use the dataframe below to find the solutions. You can of course also write code without using pandas or calculate the answers manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854dbae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96c23f82ac2bbe8d6e71640d3430fd6c",
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd4d3a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55059fab9be54700c3aca677b7efecf5",
     "grade": true,
     "grade_id": "prob-compute-impl",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = pd.MultiIndex.from_product((('toothache', '¬¨toothache'), ('catch', '¬¨catch'))) \n",
    "index = ('cavity', '¬¨cavity')\n",
    "data = [[0.108, 0.012, 0.072, 0.008],\n",
    "        [0.016, 0.064, 0.144, 0.576]]\n",
    "joint_distribution = pd.DataFrame(data, index, columns)\n",
    "print(joint_distribution)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b4012",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b25b61bb60edb7da843465ca33a00173",
     "grade": false,
     "grade_id": "ex1c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Conditional probability\n",
    "\n",
    "\n",
    "For each of the following statements, either prove it is true or give a counterexample.\n",
    "1. If P (a | b, c) = P (b | a, c), then P (a | c) = P (b | c)\n",
    "1. If P (a | b, c) = P (a), then P (b | c) = P (b)\n",
    "1. If P (a | b) = P (a), then P (a | b, c) = P (a | c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117973fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b305930b2b6a6b420481a5f98bda610",
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a443a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "711f158a7b5412d3f8bdf93e090c717f",
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### d) Independence and conditional independence\n",
    "\n",
    "\n",
    "It is quite often useful to consider the effect of some specific propositions in the\n",
    "context of some general background evidence that remains fixed, rather than in the complete\n",
    "absence of information. The following questions ask you to prove more general versions of\n",
    "the product rule and Bayes‚Äô rule, with respect to some background evidence e:\n",
    "\n",
    "1. Prove the conditionalized version of the general product rule:\n",
    "$$P(X, Y \\mid e) = P(X \\mid Y, e)\\cdot P(Y \\mid e) .$$\n",
    "1. Prove the conditionalized version of Bayes‚Äô rule:\n",
    "$$P(Y \\mid X, e) = \\frac{P(X \\mid Y, e)\\cdot P(Y \\mid e)}{P(X \\mid e)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad11fb7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d82e8ee6329c1cd2884acc53f7c1447",
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc907b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcbbb6cd30e796ddab280cf98ed6c95d",
     "grade": false,
     "grade_id": "ex1e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### e) Naive Bayes models\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of\n",
    "categories on the basis of the text it contains. Naive Bayes models are often used for this\n",
    "task. In these models, the query variable is the document category, and the ‚Äúeffect‚Äù variables\n",
    "are the presence or absence of each word in the language; the assumption is that words occur\n",
    "independently in documents, with frequencies determined by the document category.\n",
    "1. Explain precisely how such a model can be constructed, given as ‚Äútraining data‚Äù a set of documents that have been assigned to categories.\n",
    "1. Explain precisely how to categorize a new document.\n",
    "1. Is the conditional independence assumption reasonable? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71cad7c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f38aa21179d3265a3cf2547518cae962",
     "grade": true,
     "grade_id": "ex1e_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132500ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b2473d43535be77799efc35ff4c2268",
     "grade": false,
     "grade_id": "ex-bayesnet",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Assignment 4: Bayes networks (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12deb835",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c63791ba7aed93e520f8243cb76bce32",
     "grade": false,
     "grade_id": "ex-bayesnet-a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Bayes networks\n",
    "\n",
    "Explain in your own words the idea of a Bayes network. How is conditional independence represented in such a network? How can the full joint distribution be regained from such a network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adf758",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "350dcdb7d6721d2622cdfd6f45c61a50",
     "grade": true,
     "grade_id": "ex-bayesnet-a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ba761",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ff5ea2c44562ac635130458335c2534",
     "grade": false,
     "grade_id": "ex-bayesnet-2b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Independence in Bayes networks\n",
    "\n",
    "Consider the Bayes network in (ML-11 slide 32):\n",
    "1. If no evidence is observed, are Burglary and Earthquake independent? Prove this from the numerical semantics and from the topological semantics.\n",
    "1. If we observe Alarm = true, are Burglary and Earthquake independent? Justify your answer by calculating whether the probabilities involved satisfy the definition of conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc889c0c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e336b2a3743a3a1778cf5ac8c6efca3",
     "grade": true,
     "grade_id": "ex-bayesnet-2b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
