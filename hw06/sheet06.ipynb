{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7789d24f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44fca659c1e85aa18c1a44bcb17af98a",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099caf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d76d6899843a583f64a659a0e55d39c2",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 26, 2022**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c3ce3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6872b47c66deb517b87dabaa8857791",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Curse of Dimensionality (7 points)\n",
    "\n",
    "For the following exercise, be detailed in your answers and provide some examples. Think about keywords like: random vectors in high dimensional space, manifolds and Bertillonage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6d260",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edb370f033b49f58a5eb149c48dd3201",
     "grade": false,
     "grade_id": "ex1_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What are the curse of dimensionality and its implication for pattern classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f42b7",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d26d8a53a81b62acb56efb18efe45591",
     "grade": true,
     "grade_id": "ex1_a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba03dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3ea2c6702a7f9c037c56ec86e64f9da",
     "grade": false,
     "grade_id": "ex1_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Explain how this phenomenom could be used to one's advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5acdec",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8b9810e6a1f28473be5497241413e7b",
     "grade": true,
     "grade_id": "ex1_b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094ec00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "131868a391e6695a8f1273960e3dc171",
     "grade": false,
     "grade_id": "ex1_c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** Explain in your own words the concepts of descriptive and intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b4011",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e9bae1c3fb4af5de4cba4f757e00584",
     "grade": true,
     "grade_id": "ex1_c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23e8f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7746eba43118373283840b64631343a0",
     "grade": false,
     "grade_id": "cell-4b2a7291c0d81d1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** The $n$-dimensional unit cube: A cube with edge length $d=1$ in the $n$-dimensional space $\\mathbb{R}^n$. Compute the volume and the length of the diagonal for $n = 1, 2, 3, 4, 5, 10, 100, 1000$. Do the same for $d=\\frac{1}{2}$. Discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a1a40",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58694a7912a5df0a4741953a1013ebfc",
     "grade": true,
     "grade_id": "cell-70999ba1b1b35036",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab96f7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "949f02f5f8e7a6f31b80900a3782c888",
     "grade": true,
     "grade_id": "cell-16d9afe9225178e8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecd263",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fa4c13c725e6a70b690a33dd6490058",
     "grade": false,
     "grade_id": "cell-f981fd573a23eed2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**e)** The $n$-dimensional unit ball: A ball with radius $r=1$ in the $n$-dimensional space $\\mathbb{R}^n$. Compute the volume for $n = 1,2,3,5,10,100,300,400$. Use the following formula to compute the volume $V$ for a given dimension $n$ and radius $R$:\n",
    "\n",
    "$$ V_{n}(R)\\sim {\\frac {1}{\\sqrt {n\\pi }}}\\left({\\frac {2\\pi e}{n}}\\right)^{\\frac {n}{2}}R^{n}$$\n",
    "\n",
    "Refer to https://en.wikipedia.org/wiki/Volume_of_an_n-ball for more details.\n",
    "\n",
    "Consider a $n$-dimensional unit orange consisting of the peel and the pulp: Let the thickness of the peel be $1\\%$ of the radius. Compute the volume of the peel and compare to the volume of the whole orange for the same values of $n$. State the implications of your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacf53c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "662e42fb811b8fe6f900a68798fbd134",
     "grade": true,
     "grade_id": "cell-8d659c961b351eac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b96b0f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a9d6857bb4857c7d9d68384be570bb2",
     "grade": true,
     "grade_id": "cell-bee514604d4d11a9",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fcd8e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2d26df5da2f42739d220f508f90c449",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Implement and Apply PCA (8 points)\n",
    "\n",
    "In this assignment you will implement PCA from the ground up and apply it to the `cars` dataset (simplified from the JSE [2004 New Car and Truck Data](http://jse.amstat.org/jse_data_archive.htm)). This dataset consists of measurements taken on 97 different cars. The eleven features measured are: Suggested retail price (USD), Price to dealer (USD), Engine size (liters), Number of engine cylinders, Engine horsepower, City gas mileage, Highway gas mileage, Weight (pounds), Wheelbase (inches), Length (inches) and Width (inches).\n",
    "\n",
    "We would like to visualize these high dimensional features to get a feeling for how the cars relate to each other so we need to find a subspace of dimension two or three into which we can project the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e9058",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "325114529c4799bcf74afdf8403f3b99",
     "grade": true,
     "grade_id": "ex2_a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Load the cars dataset in cars.csv .\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert cars.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f379ec4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d4f4d4287acb712e068a0cf6f5e7c32",
     "grade": false,
     "grade_id": "cell-f34c576eec4a5968",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Excecute the following code which will create a scatter plot matrix (it might take some time to execute). This should give you an idea about trends and correlations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29925432",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "638d8d78d53022d460e00af541e220e3",
     "grade": false,
     "grade_id": "cell-dea96f209e45dc32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "cols = ['Suggested retail price (USD)', 'Price to dealer (USD)',\n",
    "          'Engine size (liters)', 'Number of engine cylinders',\n",
    "          'Engine horsepower', 'City gas mileage' ,\n",
    "          'Highway gas mileage', 'Weight (pounds)',\n",
    "          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\n",
    "\n",
    "df = pd.DataFrame(cars, columns=cols)\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89a40c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08e2c7c4cc087ce3c13fa60537eefbd6",
     "grade": false,
     "grade_id": "cell-272ac17492ca363a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Looking at the scatter plot matrix above:\n",
    "1. Why does the forth column/row look so different compared to the others?\n",
    "2. If you would have to pick only two attributes to describe the entire dataset: Which two attributes would be the worst two pick? Why?\n",
    "3. If you would have to pick only two attributes to describe the entire dataset: Which two attributes would be a better pick? Why?\n",
    "4. Guess how many principle components are needed to explain 75% of the variance in the dataset if you would apply PCA to the dataset. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193af4d7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae5aa08fd798aa833436eec0cd009d7d",
     "grade": true,
     "grade_id": "cell-36eaea23b2766923",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bdb5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a280b18e2eb0e303544ab1a18443e81",
     "grade": false,
     "grade_id": "ex2_c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "PCA finds a subspace that maximizes the variance by determining the eigenvectors of the covariance matrix. So we need to calculate the covariance matrix and afterwards the eigenvalues. When the data is normalized the covariance is calculated as\n",
    "\n",
    "$$C = \\frac{1}{n-1}((\\bf{X}-\\bar{x})^T(\\bf{X}-\\bar{x})) $$\n",
    "\n",
    "with $X$ being an $n \\times d$ matrix with $n$ samples and $d$ features, when $\\bar{X}$ is the mean vector of features\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}x_i.$$  \n",
    "\n",
    "The entry $c_{i,j}$ in $C$ (a $d\\times d$ matrix) tells you how much feature $i$ correlates with feature $j$.\n",
    "\n",
    "\n",
    "**Note**: When the features have different scales, for achieving comparable covariance values, **first and before calculating the covariance matrix**, we need to standardize $X$ respecting its features\n",
    "\n",
    "$${\\bf{X}}_{norm} = \\frac{\\bf{X}}{\\sigma}$$\n",
    "\n",
    "when $\\sigma$ is the standard deviation vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870efd83",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e48aceae5916279d3e3bddd1d7d8715f",
     "grade": true,
     "grade_id": "ex2_c_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the covariance matrix and store it into covar\n",
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert covar.shape == (11, 11)\n",
    "\n",
    "# TODO: Compute the eigenvalues and eigenvectors and store them into eigenval and eigenvec\n",
    "#       (Figure out a function to do this for you)\n",
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert eigenval.shape == (11,)\n",
    "assert eigenvec.shape == (11, 11)\n",
    "for ev in eigenvec: np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed951b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2a471db1d7416f070a272c89e9be4ae",
     "grade": false,
     "grade_id": "cell-3573629c4379a229",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Plot the spectrum of the eigenvalues and make sure that they are sorted by their magnitude (in descending order).\n",
    "\n",
    "**Note:** Sorting should be done respecting eigenvalues vector, but the order of eigenvectors should also be updated such that the corresponding pairs of eigenvalue-eigenvector be accessible with the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e92dad",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4e474d62ebbe2fe5754112a1dc6707f",
     "grade": true,
     "grade_id": "cell-53f8a29f237fc60e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert not isinstance(eigenval, np.matrix)\n",
    "assert not isinstance(eigenvec, np.matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927bc74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebf3a45f88feff7cfe6469c5d5e492b3",
     "grade": false,
     "grade_id": "cell-a45f6490d33968b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How many principal components should you include based on the spectrum plot?\n",
    "\n",
    "One method to decide about the number of components is the \"explained variance.\" The amount of data variance captured by each principal components is the magnitude of its corresponding eigenvalue. Therefore, in the explained variance method, we calculate the proportion of each the eigenvalue respective to the total sum of the eigenvalues. That gives us the percentage of data variance explained by each corresponding principal component. And the cumulative sum of these percentages shows how much more of the dataset information (variance) is presentable with taking one more component. Knowing that on one hand, and the computational cost and the difficulty of visualization of one more dimension gives a clue for the decision.\n",
    "\n",
    "Execute the cells below and decide about an efficient number of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed9d89",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1e58b951f1b5c0ddbe8522a380fcacd",
     "grade": true,
     "grade_id": "cell-bda18cf1de7d69c0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65554c0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "905b266e512bfee703c376bad6f982c2",
     "grade": false,
     "grade_id": "cell-61a2bf801d811bd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "var_exp = [val*100/sum(eigenval) for val in eigenval]\n",
    "cum_sum = np.cumsum(var_exp)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ind = range(1, len(eigenval)+1)\n",
    "plt.bar(ind, var_exp, label='Explained Variance')\n",
    "plt.step(ind, cum_sum, 'r:',\n",
    "         where='mid', label='Cumulative Sum')\n",
    "plt.ylabel('Explained Variance (%)')\n",
    "plt.xlabel('Principal Components')\n",
    "ax.set_xticks(ind)\n",
    "plt.legend(loc='center right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03910716",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "469fa23e6bb85557779355ebbc55867b",
     "grade": false,
     "grade_id": "ex2_d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, with the first eigenvalue/component we can describe about 70% of the variance in our dataset; taking the first two will increase it to more than 80%. For the sake of a convenient visualization we will go with the first two components, because the increase of the variation is not that much after the second component.\n",
    "\n",
    "Now you should have a matrix full of eigenvectors. We can now do two things: project the data down onto the two dimensional subspace to visualize it and we can also plot the two first principle component vectors as eleven two dimensional points to get a feeling for how the features are projected into the subspace. Execute the two cells below and describe what you see. Is PCA a good method for this problem? Was it justifiable that we only considered the first two principle components? What kinds of cars are in the four quadrants of the second plot? (**put your answer in the cell below of this code cell**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a6ccd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "677e27490dcf885a28fea9784f854123",
     "grade": false,
     "grade_id": "cell-a2968be21ca0a092",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project the data down into the two dimensional subspace\n",
    "proj = cars_norm @ eigenvec[:,:2]\n",
    "\n",
    "\n",
    "# Plot projected data\n",
    "plt.title('Data Projected Onto First two Principal Components')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('PC1 ({}%)'.format(round(var_exp[0])))\n",
    "plt.ylabel('PC2 ({}%)'.format(round(var_exp[1])))\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=.4)\n",
    "\n",
    "# Plot the PC Vectors\n",
    "# Project them and scale them by the standard deviation.\n",
    "eigenvec_p = eigenvec.T @ eigenvec * np.sqrt(eigenval)\n",
    "origin = np.mean(proj, axis=0)\n",
    "\n",
    "plt.quiver(*origin, eigenvec_p[0,0], eigenvec_p[0,1], angles='xy', scale_units='xy', scale=1)\n",
    "plt.quiver(*origin, eigenvec_p[1,0], eigenvec_p[1,1], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "\n",
    "plt.text(-12,-17.5, 'PC1')\n",
    "plt.text(-18,-15, 'PC2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb32a84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8607dc8efa73eac84a5849d3eb3e76b",
     "grade": false,
     "grade_id": "cell-a2968be21ca0a091",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the plot\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.title('Eigenvectors Plot')\n",
    "plt.axhline(0, color='green', linestyle=':')\n",
    "plt.axvline(0, color='green', linestyle=':')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('PC1 ({}%)'.format(round(var_exp[0])))\n",
    "plt.ylabel('PC2 ({}%)'.format(round(var_exp[1])))\n",
    "\n",
    "\n",
    "# plot centered projected data\n",
    "proj = proj - np.mean(proj, axis=0)\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=.4)\n",
    "\n",
    "# scale eigenvectors\n",
    "eigenvec_s = eigenvec * np.sqrt(eigenval)\n",
    "\n",
    "# Plot the eigenvector and add the labels\n",
    "for idx, eivec in enumerate(eigenvec_s[:,:2]):\n",
    "    plt.arrow(0, 0, eivec[0]*5, eivec[1]*5, alpha=.8, \n",
    "              color=plt.get_cmap('Set3')(idx), \n",
    "              width=0.03, head_width=.2, label=cols[idx])   \n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74d186",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "749c07b360603589ae4238d26df56914",
     "grade": true,
     "grade_id": "ex2_d_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c21e9c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2dfc16ceda35f939807894eb1ef0610",
     "grade": false,
     "grade_id": "ex3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: PCA (7 points)\n",
    "\n",
    "In this exercise we investigate the statement from the lecture that PCA finds the subspace that captures most of the data variance. To be more precise, we show that the orthonormal projection onto an $m$-dimensional subspace that maximizes the variance of the projected data is defined by the principal components, i.e. by the $m$ eigenvectors of the covariance matrix $C$ corresponding to the $m$ largest eigenvalues. The proof consists of two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027087aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28a3121e5d2eb5f3453dfc40d3bcc8fe",
     "grade": false,
     "grade_id": "ex3_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a) First step:** Consider a one dimensional subspace: Determine a (unit) vector $\\vec{p}$, such that the variance of the data, when projected onto the subspace determined by that vector, is maximal.\n",
    "\n",
    "The covariance matrix $C$ allows to compute the variance of the projected data as $\\vec{p}^{T}C\\vec{p}$. We want to maximize this expression. To avoid $\\|\\vec{p}\\|\\to\\infty$ we will only consider unit vectors, i.e. we constrain $\\vec{p}$ to be normalized: $\\vec{p}^T\\vec{p}=1$. Maximize the expression with this constraint (which can be done using a Lagrangian multiplier). Conclude that a suitable $\\vec{p}$ has to be an eigenvector of $C$ and describe which of the eigenvectors is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76842511",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e80b023dcc7cccd3fb26e1a54161529",
     "grade": true,
     "grade_id": "ex3_a_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0afae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bc973f217c68a1e4c4794fb21e89f93",
     "grade": false,
     "grade_id": "ex3_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b) Second step:** Now proof the statement for the general case of an $m$-dimensional projection space.\n",
    "\n",
    "Use an inductive argument: assume the statement has been shown for the $(m-1)$-dimensional projection space, spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$. Now find a (unit) vector $\\vec{p}_m$, orthogonal to the existing vectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, that maximizes the projected variance $\\vec{p}_m^TC\\vec{p}_m$. Proceed similar to case (a), but with additional Lagrangian multipliers to enforce the orthogonality constraint. Show that the new vector $\\vec{p}_m$ is an eigenvector of $C$. Finally show that the variance is maximized for the eigenvector corresponding to the $m$-th largest eigenvalue $\\lambda_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c82db3",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24289a25e09f6f242db8c2793a892853",
     "grade": true,
     "grade_id": "ex3_b_solution",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f22d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80c1a14412da5d0446e268037d3f148",
     "grade": false,
     "grade_id": "cell-a72f7bcab5038668",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Projection Pursuit (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a2727b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b66f6c9c1528e5eba6ed57172c1c139",
     "grade": false,
     "grade_id": "cell-0f4a68ed40351836",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Explain in your own words the idea of projection pursuit. Is it a linear or non-linear method for dimension reduction? Discuss why high variance, non-Gaussianness and clusters indicate an interesting feature. What is the relation to PCA? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef0b58",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3028ab0b371066fc80b8ef2f471bbcc",
     "grade": true,
     "grade_id": "cell-1470e1527ff53784",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90bbf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6f281ffef5673295d1a364b5fca4d9c",
     "grade": false,
     "grade_id": "cell-689588368225bde9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Explain how the different indices (Friedman-Tukey, Hermite, Natural Hermite, and Entropy) detect interesting features and discuss advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5b076",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b88e7c703b4f7fd54a17b1a32493e9fb",
     "grade": true,
     "grade_id": "cell-6f4df9ba863794b0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa36aee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15718e000b94e272c4222d2db4e06663",
     "grade": false,
     "grade_id": "cell-e42e41c4def87006",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** Explain the idea of projection pursuit for clustering (ML-06, slides 56-58). How is the index computed and why does maximizing that index yield good clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76ef08",
   "metadata": {},
   "source": [
    "Standard approaches to clustering use all variables of the data, as the similarity metrics (e.g., Euclidean) are definined in the full dimension of the data space.  This may lead to suboptimal clusters, for examples if the cluster structure is contained in a subspace of the data while other dimensions contain noise hiding the actual cluster structure.  Projection pursuit clustering aims at recovering clusters in lower dimensional subspaces by simultanously performing dimension reduction and clustering.\n",
    "\n",
    "Projection Pursuit Clustering aims at finding a direction $\\vec{k}$ that maximizes the projection index (e.g. Friedman-Tukey, see above) and simultaniously forms clusters. Clustering proceeds in a non-hierarchical fashion, assuming a fixed number of $K$ clusters, to which (projected) data are assigned such that the within-group sum-of-square distance is minimized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
