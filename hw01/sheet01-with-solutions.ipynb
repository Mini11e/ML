{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2b7a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600001f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-175c5b8f652e026d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Sheet 01: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1362b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0779efa0bccb27ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This is the first official exercise sheet. The homework sheets will usually be available at the beginning of the week and are supposed to be solved in groups of three. They have to be handed in by the end of Sunday of that week. The exercises are then presented to your tutor in a small feedback session. To acquire the admission for the final exam, you will have to pass $N-2$ of the weekly provided exercise sheets.\n",
    "\n",
    "Sign up for a group on Stud.IP (See `Participants` -> `Functions/Groups`). The times mentioned there are the times for the feedback session of your group. If none of them fits, send any of the tutors an e-mail so we can try to arrange something.\n",
    "\n",
    "Your group will have a group folder in Stud.IP under `Files`. Upload your solutions there to hand them in.\n",
    "\n",
    "All exercise sheets will use [Jupyter Notebooks](http://jupyter-notebook.readthedocs.org/en/latest/notebook.html). To be able to run these on your system, you will need to install Python and a few packages. We suggest the newest version of Python 3 and installing the conda environment as explained in the practice session and in the file \"ml-install.txt\".\n",
    "\n",
    "This week's sheet should be solved and handed in before end of **Sunday, April 21, 2024**. \n",
    "Please upload your results to your group's Stud.IP folder. In case you cannot do this first sheet (due to technical or organizational problems) please upload a description of your problem instead. Your tutor will help you to solve the problems in the first feedback session and you may hand in this sheet together with the second sheet one week later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c29294",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Decision Trees [4 Points]\n",
    "Draw the decision trees for the following boolean functions. Either use pen and paper and scan/photograph the result or employ your ASCII artist within below.\n",
    "\n",
    "Note: $\\oplus := xor$, that means one of the operands has to be true, while the other one has to be false:\n",
    "\n",
    "|$$\\oplus$$ | $$B$$ | $$\\neg B$$|\n",
    "|:---------|:-----|:---------|\n",
    "|$$A$$      |  f  |    t|\n",
    "|$$\\neg A$$ |  t  |    f|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35ec5a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** $\\neg A \\wedge B$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc18625",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "         A\n",
    "        / \\\n",
    "     t /   \\ f\n",
    "      /     \\\n",
    "     No      B\n",
    "            / \\\n",
    "         t /   \\ f\n",
    "          /     \\\n",
    "        Yes     No\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fef70",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** $A \\oplus B$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b99ec",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1b_answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "               A\n",
    "              / \\\n",
    "         t  /     \\  f\n",
    "          /         \\\n",
    "         B           B\n",
    "        / \\         / \\\n",
    "     t /   \\ f   t /   \\ f\n",
    "      /     \\     /     \\\n",
    "     No     Yes Yes     No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248953a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** $A \\vee (B \\wedge C) \\vee (\\neg C \\wedge D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a55af",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "            A\n",
    "           / \\\n",
    "      t  /     \\  f\n",
    "       /         \\\n",
    "     Yes          C\n",
    "                 / \\\n",
    "            t  /     \\  f\n",
    "             /         \\\n",
    "            B           D\n",
    "           / \\         / \\\n",
    "        t /   \\ f   t /   \\ f\n",
    "         /     \\     /     \\\n",
    "       Yes     No  Yes     No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6625af",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** $(A \\rightarrow (B \\wedge \\neg C)) \\vee (A \\wedge B)$\n",
    "\n",
    "hint: remember material implication from logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125bc5b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "$= (\\neg A \\vee (B \\wedge \\neg C)) \\vee (A \\wedge B)$\n",
    "\n",
    "$= \\neg A \\vee (A \\wedge B) \\vee (B \\wedge \\neg C)$\n",
    "\n",
    "$= \\neg A \\vee (A \\wedge B)$\n",
    "\n",
    "$= \\neg A \\vee  B$\n",
    "\n",
    "              A\n",
    "             / \\\n",
    "          t /   \\ f\n",
    "           /     \\\n",
    "          B      Yes\n",
    "         / \\\n",
    "      t /   \\ f\n",
    "       /     \\\n",
    "     Yes     No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd48e1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Entropy and Information Gain [8 Points]\n",
    "\n",
    "In many machine learning applications it is crucial to determine which criterions are necessary for a good classification. Decision trees have those criterions close to the root, imposing an order from significant to less significant criterions. One way to select the most important criterion is to compare its information gain or its entropy to others. The following dataset is a hands-on example for this method.\n",
    "\n",
    "Consider the following attributes with their possible values:\n",
    "\n",
    "  * $raining = \\{yes, no\\}$\n",
    "  * $tired = \\{yes, no\\}$\n",
    "  * $late = \\{yes, no\\}$\n",
    "  * $distance = \\{short, medium, long\\}$\n",
    "\n",
    "And a training data set consisting of those attributes:\n",
    "\n",
    "| #  | raining | tired | late | distance | attend_party |\n",
    "|----|---------|-------|------|----------|--------------|\n",
    "| 1  | yes     | no    | no   | short    | **yes**      |\n",
    "| 2  | yes     | no    | yes  | medium   | **no**       |\n",
    "| 3  | no      | yes   | no   | long     | **no**       |\n",
    "| 4  | yes     | yes   | yes  | short    | **no**       |\n",
    "| 5  | yes     | no    | no   | short    | **yes**      |\n",
    "| 6  | no      | no    | no   | medium   | **yes**      |\n",
    "| 7  | no      | yes   | no   | long     | **no**       |\n",
    "| 8  | yes     | no    | yes  | short    | **no**       |\n",
    "| 9  | yes     | yes   | no   | short    | **yes**      |\n",
    "| 10 | no      | yes   | no   | medium   | **no**       |\n",
    "| 11 | no      | yes   | no   | long     | **no**       |\n",
    "| 12 | no      | yes   | yes  | short    | **no**       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17578e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Build the root node of a decision tree from the training samples given in the table above by calculating the information gain for all four attributes (raining, tired, late, distance).\n",
    "\n",
    "$$\\operatorname{Gain}(S,A) = \\operatorname{Entropy}(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|}\\operatorname{Entropy}(S_v)$$\n",
    "\n",
    "$$\\operatorname{Entropy}(S) = -p_{\\oplus} log_{2} p_{\\oplus} - p_{\\ominus} log_{2} p_{\\ominus}$$\n",
    "\n",
    "$S$ is the set of all data samples. $S_v$ is the subset for which attribute $A$ has value $v$. An example for attribute **tired** with value $yes$ would be:\n",
    "$$|S_{yes}| = 7, S_{yes}:[1+, 6−]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a921c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "2a-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*Caution: some pretty intense rounding was applied to the following numbers. So your results might differ, but they should be in the same ballpark!*\n",
    "\n",
    "Entropy of the whole dataset:\n",
    "\n",
    "$$Entropy\\left(S\\right) = -\\frac{4}{12} \\log_{2} \\frac{4}{12} - \\frac{8}{12} \\log_{2} \\frac{8}{12} \\approx 0.92$$\n",
    "\n",
    "Attribute **raining**:\n",
    "$$\\left|S_{yes}\\right|:6 , S_{yes}:[3+,3-]$$\n",
    "\n",
    "$$Entropy\\left(S_{yes}\\right) = -\\frac{3}{6} \\log_{2} \\frac{3}{6}-\\frac{3}{6} \\log_{2} \\frac{3}{6} = 1$$\n",
    "\n",
    "$$\\left|S_{no}\\right|:6 , S_{no}:[1+,5-]$$\n",
    "\n",
    "$$Entropy\\left(S_{no}\\right) = -\\frac{1}{6} \\log_{2} \\frac{1}{6}-\\frac{5}{6} \\log_{2} \\frac{5}{6} \\approx 0.65$$\n",
    "\n",
    "$$Gain\\left(S,raining\\right) \\approx 0.92 - \\left(\\frac{6}{12}\\cdot 1 + \\frac{6}{12}\\cdot 0.65\\right) = 0.095$$\n",
    "\n",
    "Attribute **tired**:\n",
    "$$\\left|S_{yes}\\right|:7 , S_{yes}:[1+,6-]$$\n",
    "\n",
    "$$Entropy\\left(S_{yes}\\right) = -\\frac{1}{7} \\log_{2} \\frac{1}{7}-\\frac{6}{7} \\log_{2} \\frac{6}{7} \\approx 0.59$$\n",
    "\n",
    "$$\\left|S_{no}\\right|:5 , S_{no}:[3+,2-]$$\n",
    "\n",
    "$$Entropy\\left(S_{no}\\right) = -\\frac{3}{5} \\log_{2} \\frac{3}{5}-\\frac{2}{5} \\log_{2} \\frac{2}{5} \\approx 0.97$$\n",
    "\n",
    "$$Gain\\left(S,tired\\right) \\approx 0.92 - \\left(\\frac{7}{12}\\cdot 0.59 + \\frac{5}{12}\\cdot 0.97\\right) \\approx 0.171$$\n",
    "\n",
    "Attribute **late**:\n",
    "$$\\left|S_{yes}\\right|:4 , S_{yes}:[0+,4-]$$\n",
    "\n",
    "$$Entropy\\left(S_{yes}\\right) = -\\frac{0}{4} \\log_{2} \\frac{0}{4}-\\frac{4}{4} \\log_{2} \\frac{4}{4} = 0$$\n",
    "\n",
    "$$\\left|S_{no}\\right|:8 , S_{no}:[4+,4-]$$\n",
    "\n",
    "$$Entropy\\left(S_{no}\\right) = -\\frac{4}{8} \\log_{2} \\frac{4}{8}-\\frac{4}{8} \\log_{2} \\frac{4}{8} = 1$$\n",
    "\n",
    "$$Gain\\left(S,late\\right) \\approx 0.92 - \\left(\\frac{4}{12}\\cdot 0 + \\frac{8}{12}\\cdot 1\\right) \\approx 0.253$$\n",
    "\n",
    "Attribute **distance**:\n",
    "$$\\left|S_{short}\\right|:6 , S_{short}:[3+,3-]$$\n",
    "\n",
    "$$Entropy\\left(S_{short}\\right) = -\\frac{3}{6} \\log_{2} \\frac{3}{6}-\\frac{3}{6} \\log_{2} \\frac{3}{6} = 1$$\n",
    "\n",
    "$$\\left|S_{medium}\\right|:3 , S_{medium}:[1+,2-]$$\n",
    "\n",
    "$$Entropy\\left(S_{medium}\\right) = -\\frac{1}{3} \\log_{2} \\frac{1}{3}-\\frac{2}{3} \\log_{2} \\frac{2}{3} \\approx 0.918$$\n",
    "\n",
    "$$\\left|S_{long}\\right|:3 , S_{long}:[0+,3-]$$\n",
    "\n",
    "$$Entropy\\left(S_{long}\\right) = -\\frac{0}{3} \\log_{2} \\frac{0}{3}-\\frac{3}{3} \\log_{2} \\frac{3}{3} = 0$$\n",
    "\n",
    "$$Gain\\left(S,distance\\right) \\approx 0.92 - \\left(\\frac{6}{12}\\cdot 1 + \\frac{3}{12}\\cdot 0.918 + \\frac{3}{12}\\cdot 0\\right) = 0.191$$\n",
    "\n",
    "The information gain is greatest for the **late** attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5825d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Perform the same calculation as in **a)** but use the gain ratio instead of the information gain. Does the result for the root node change?\n",
    "\n",
    "$$\\operatorname{GainRatio}(S,A) = \\frac{\\operatorname{Gain}(S,A)}{\\operatorname{SplitInformation}(S,A)}$$\n",
    "\n",
    "$$\\operatorname{SplitInformation}(S,A) = - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} \\log_{2} \\frac{|S_{v}|}{|S|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c27db",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "2b-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Attribute **raining**:\n",
    "\n",
    "$$SplitInformation(S,raining) = -\\left(\\frac{6}{12} \\log_{2} \\frac{6}{12} + \\frac{6}{12} \\log_{2} \\frac{6}{12}\\right) = 1$$\n",
    "$$GainRatio(S,raining) = \\frac{0.095}{1} = 0.095$$\n",
    "\n",
    "Attribute **tired**:\n",
    "\n",
    "$$SplitInformation(S,tired) = -\\left(\\frac{7}{12} \\log_{2} \\frac{7}{12} + \\frac{5}{12} \\log_{2} \\frac{5}{12}\\right) = 0.98$$\n",
    "$$GainRatio(S, tired) = \\frac{0.171}{0.98} \\approx 0.174$$\n",
    "\n",
    "Attribute **late**:\n",
    "\n",
    "$$SplitInformation(S,late) = -\\left(\\frac{4}{12} \\log_{2} \\frac{4}{12} + \\frac{8}{12} \\log_{2} \\frac{8}{12}\\right) = 0.918$$\n",
    "$$GainRatio(S,late) = \\frac{0.253}{0.918} \\approx 0.276$$\n",
    "\n",
    "Attribute **distance**:\n",
    "\n",
    "$$SplitInformation(S,distance) = -\\left(\\frac{6}{12} \\log_{2} \\frac{6}{12} + \\frac{3}{12} \\log_{2} \\frac{3}{12} + \\frac{3}{12} \\log_{2} \\frac{3}{12}\\right) = 1.5$$\n",
    "$$GainRatio(S,distance) = \\frac{0.191}{1.5} \\approx 0.127$$\n",
    "\n",
    "We should still use the **late** attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ef14f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## ID3 algorithm [4 Points]\n",
    "\n",
    "Implement the following two functions in Python. Take a look at the `assert`s to see how the function should behave. An assert is a condition that your function is required to pass. Most of the conditions here are taken from the lecture slides (ML-03, Slide 12 & 13). Don't worry if you do not get all asserts to pass, just comment the failing ones out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0824c7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a-info",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a) Entropy**\n",
    "\n",
    "$$\\operatorname{Entropy}(S) = - \\sum_{i=1...c} p_i \\log_2 p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b8504",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3a-code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "def entropy(s):\n",
    "    \"\"\"\n",
    "    Calculate the entropy for a given target value set.\n",
    "\n",
    "    Args:\n",
    "        s (list): Target classes for specific observations.\n",
    "\n",
    "    Returns:\n",
    "        The entropy of s.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    freq = {}\n",
    "    for item in s:\n",
    "        freq[item] = freq.get(item, 0) + 1\n",
    "    return -sum(f/len(s) * log2(f/len(s)) for f in freq.values())\n",
    "\n",
    "    # or alternatively:\n",
    "    return -sum((s.count(target) / len(s)) * log2(s.count(target) / len(s))\n",
    "                for target in set(s)) # Sets only contain unique values.\n",
    "    ### END SOLUTION\n",
    "\n",
    "# See ML-03, Slide 12 & 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon: Account for small computational and rounding erros\n",
    "epsilon = 1e-3\n",
    "assert abs(entropy([1,1,1,0,0,0]) - 1.0) < epsilon\n",
    "assert abs(entropy([1,1,1,1,0,0,0]) - 0.985) < epsilon\n",
    "assert abs(entropy([1,1,1,1,1,1,0]) - 0.592) < epsilon\n",
    "assert abs(entropy([1,1,1,1,1,1,0,0]) - 0.811) < epsilon\n",
    "assert abs(entropy([2,2,1,1,0,0]) - 1.585) < epsilon\n",
    "assert abs(entropy([2,2,2,1,0]) - 1.371) < epsilon\n",
    "assert abs(entropy([2,2,2,0,0]) - 0.971) < epsilon\n",
    "assert abs(entropy(['yes','yes','yes','no','no','no']) - 1.0) < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3064a3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Information Gain\n",
    "\n",
    "$$\\operatorname{Gain}(S,A) = \\operatorname{Entropy}(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} \\operatorname{Entropy}(S_v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39923f4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3b-code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gain(targets, attr_values):\n",
    "    \"\"\"\n",
    "    Calculates the expected reduction in entropy due to sorting on A.\n",
    "\n",
    "    Args:\n",
    "        targets (list): Target classes for observations in attr_values.\n",
    "        attr_values (list): Values of each instance for the respective attribute.\n",
    "\n",
    "    Returns:\n",
    "        The information gain of\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    sigma = 0\n",
    "    for v in set(attr_values): # Sets only contain unique values.\n",
    "        S_v = [targets[key] for (key, v_) in enumerate(attr_values) if v_ == v]\n",
    "        sigma += ((len(S_v) / len(targets)) * entropy(S_v))\n",
    "    return entropy(targets) - sigma\n",
    "    ### END SOLUTION\n",
    "\n",
    "# See ML-03, Slide 12 & 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lists here can each be seen as one column of a table such as the one in assignment 2.\n",
    "# Assert targets would be the last column, while the attribute values are the values of one attribute, here the\n",
    "# example rain and distance\n",
    "assert_targets = [\"no\",\"no\",\"yes\",\"yes\",\"yes\",\"no\",\"yes\",\"no\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"no\"]\n",
    "assert_attribute_values_1 = [\"yes\", \"yes\",\"yes\",\"yes\",\"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\",\"yes\", \"no\", \"yes\"]\n",
    "assert_attribute_values_2 = [\"high\",\"low\",\"medium\",\"high\",\"high\",\"medium\",\"low\",\"medium\",\"low\",\"high\",\"high\",\"medium\",\"low\",\"low\"]\n",
    "assert_attribute_values_3 = [0,1,0,0,0,1,1,0,0,0,1,1,0,1]\n",
    "\n",
    "epsilon = 1e-3\n",
    "assert abs(gain(assert_targets, assert_attribute_values_1) - 0.152) < epsilon\n",
    "assert abs(gain(assert_targets, assert_attribute_values_2) - 0.05) < epsilon\n",
    "assert abs(gain(assert_targets, assert_attribute_values_3) - 0.048) < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ea06c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** ID3\n",
    "\n",
    "In the next two cells we have implemented the ID3 algorithm. It relies on your two functions from above, `entropy` and `gain`. Try to understand what the code does and replace `# YOUR CODE HERE` with meaningful comments describing the respective parts of the code. Do not forget to write the docstring. Though its often annoying, being able to read other peoples code is one of the key skills (and obstacles) in software engineering. So give it a try! Otherwise you are of course welcome to write your own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f3372e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "\n",
    "\n",
    "class Node(namedtuple('Node', 'label children')):\n",
    "    \"\"\"\n",
    "    A small node representation with a pretty string representation.\n",
    "    \"\"\"\n",
    "    def __str__(self, level=0):\n",
    "        return_str ='{}{!s}\\n'.format(' ' * level * 4, self.label)\n",
    "        for child in self.children:\n",
    "            return_str += child.__str__(level + 1)\n",
    "        return return_str\n",
    "\n",
    "def id3(data, attributes, targets, target_names, attribute_names):\n",
    "    \"\"\"\n",
    "    Recursively calculate a tree of Nodes (fields: label [string], children [list])\n",
    "    using the ID3 algorithm.\n",
    "    ### BEGIN SOLUTION\n",
    "    Args:\n",
    "        data (list):            The (subset of) data points/examples. Each example is a list \n",
    "                                   with a value for each attribute.\n",
    "        attributes (list):      The integer representation of the attributes from which\n",
    "                                    the best attribute is computed.\n",
    "        targets (list):         The target values for each example. Same length as data.\n",
    "        target_names (list):    Names of the target values represented as strings.\n",
    "        attribute_names (list): Names of the attribute represented as strings. \n",
    "\n",
    "    Returns:\n",
    "        The root node\n",
    "    ### END SOLUTION\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    # If all data points have the same target value, directly return the single-node tree Root, \n",
    "    # with this target value as label\n",
    "    ### END SOLUTION\n",
    "    if all(target == targets[0] for target in targets):\n",
    "        return Node('Result: {!s}'.format(target_names[targets[0]]), [])\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    # If the list of attributes is empty, directly return the single node tree root,\n",
    "    # with the most common target value as label\n",
    "    ### END SOLUTION\n",
    "    if len(attributes) == 0:       \n",
    "        most_common_idx = Counter(targets).most_common(1)[0][0]\n",
    "        return Node('Result: {!s}'.format(target_names[most_common_idx]), [])\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    # Find the attribute with the maximum gain\n",
    "    ### END SOLUTION\n",
    "    gains = [gain(targets, [r[attribute] for r in data])\n",
    "             for attribute in attributes]\n",
    "    max_gain_attribute = attributes[gains.index(max(gains))]\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    # Create a root note with the maximum gain attribute\n",
    "    ### END SOLUTION\n",
    "    root = Node('Attribute: {!s} (gain {!s})'.format(attribute_names[max_gain_attribute],\n",
    "                                                     round(max(gains), 4)), [])\n",
    "    ### BEGIN SOLUTION\n",
    "    # For each possible value, vi, of the maximum gain attribute\n",
    "    ### END SOLUTION\n",
    "    for vi in set(data_sample[max_gain_attribute] for data_sample in data):\n",
    "        ### BEGIN SOLUTION\n",
    "        # Add a new tree branch below root for which the maximum gain attribute has value vi\n",
    "        ### END SOLUTION\n",
    "        child = Node('Value: {!s}'.format(vi), [])\n",
    "        root.children.append(child)\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        # Find the indices of the datapoints which have vi as value for the maxmium gain attribute\n",
    "        ### END SOLUTION\n",
    "        vi_indices = [idx for idx, data_sample in enumerate(data)\n",
    "                          if data_sample[max_gain_attribute] == vi]\n",
    " \n",
    "        ### BEGIN SOLUTION\n",
    "        # Create the data and target subsets with maximum gain attribute = vi\n",
    "        ### END SOLUTION\n",
    "        data_vi = [data[i] for i in vi_indices]\n",
    "        targets_vi = [targets[i] for i in vi_indices]\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        # exclude the maximum gain attribute from the subset of attributes which are used in the\n",
    "        # next iteration\n",
    "        ### END SOLUTION\n",
    "        attributes_vi = [attribute for attribute in attributes if not attribute == max_gain_attribute]\n",
    "       \n",
    "        if data_vi:\n",
    "            ### BEGIN SOLUTION\n",
    "            # If data_vi is not empty below this new branch\n",
    "            # add the subtree ID3 (data_vi, attributes - maximum gain attributes).\n",
    "            ### END SOLUTION\n",
    "            child.children.append(\n",
    "                id3(data_vi, attributes_vi, targets_vi, target_names, attribute_names)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            ### BEGIN SOLUTION\n",
    "            # If no data point has value vi for the max gain attribute add a new leaf node with \n",
    "            #the most common target value in the current dataset as label\n",
    "            ### END SOLUTION\n",
    "            most_common_idx = Counter(targets_vi).most_common(1)[0][0]\n",
    "            label = 'Result: {!s}'.format(target_names[most_common_idx])\n",
    "            child.children.append(Node(label, []))\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f77149",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3d-1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** The algorithm is applied to two data sets. Run those and discuss the differences. For which data set is the ID3 algorithm better suited and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2b0c8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-790ca323099344b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First look at the json file in which the party dataset is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeef5a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c651a26268c4fd31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('party.json', 'r') as party_file:\n",
    "    party = json.load(party_file)\n",
    "    \n",
    "print(json.dumps(party, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f53ff",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cf4012147f12460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We see that the dataset is parsed as a dictionary with four entries:\n",
    "\n",
    "* `attributes`: A list of the attribute names\n",
    "* `data`: A list of the x-data of our samples. Each sample is again a list\n",
    "* `target_names`: A list of the targets, i.e. labels\n",
    "* `targets`: A list of the labels of our samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af3171",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3d-2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This code runs the ID3 algorithm on the party data set which you already know from assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a70688",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3d-3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('party.json', 'r') as party_file:\n",
    "    \n",
    "    party = json.load(party_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "epsilon = 1e-3\n",
    "assert abs(gain(party['targets'], [r[2] for r in party['data']]) - 0.252) < epsilon\n",
    "\n",
    "\n",
    "data = party['data']\n",
    "attribute_names = party['attributes']\n",
    "attributes = list(range(len(attribute_names)))\n",
    "targets = party['targets']\n",
    "target_names = party['target_names']\n",
    "\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_party = id3(data, attributes, targets, target_names, attribute_names)\n",
    "\n",
    "print(tree_party)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2d55f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3d-4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This code runs the ID3 algorithm on the famous iris flowers data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445cd6d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3d-5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('iris.json', 'r') as iris_file:\n",
    "    iris = json.load(iris_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "epsilon = 1e-3\n",
    "assert abs(gain(iris['targets'], [r[2] for r in iris['data']]) - 1.446) < epsilon\n",
    "\n",
    "data = iris['data']\n",
    "attribute_names = iris['attributes']\n",
    "attributes = list(range(len(attribute_names)))\n",
    "targets = iris['targets']\n",
    "target_names = iris['target_names']\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_iris = id3(data, attributes, targets, target_names, attribute_names)\n",
    "\n",
    "print(tree_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384011f8",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "The problem with the iris data set is that since the ID3 algorithm works with the assumption of nominal variables it splits on every unique value for continuous data. Thus the tree grows very wide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26169934",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4-1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Decision Trees on Iris Flowers [4 Points]\n",
    "\n",
    "In this exercise we are going to examine and compare two decision trees that were generated from the iris flower data set to classify three variations of Iris flowers. The Iris data set is a classical example of a labeled dataset, i.e. every sample consists of two parts: features and labels. There are four features per sample in this data set (sepal length ($x_1$), sepal width ($x_2$), petal length ($x_3$) and petal width ($x_4$) in cm) and a corresponding label (Iris Setosa, Iris Versicolour, Iris Virginica). These samples are by nature **noisy**, no matter how carefully the measurement was taken - slight deviation from the actual length **cannot be avoided**. We want to learn how the features are related to the label so that we could (in the future) predict the label of a new sample automatically. One way to obtain such a `classifier` is to train a decision tree on the data.\n",
    "\n",
    "Here are two decisions tree generated by the data set. We will now take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84e890",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4-2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Tree 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8a48722",
   "metadata": {},
   "source": [
    "                      +  \n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                     x3 < 4.95 |   x3 >= 4.95      +\n",
    "                        +--------------+       virginica\n",
    "                        |              |\n",
    "                        |              |\n",
    "              x4 < 1.65 | x4 >= 1.65   +\n",
    "                 +------------+    virginica\n",
    "                 |            |\n",
    "                 |            |\n",
    "                 +            +\n",
    "            versicolor    virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b9694",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4-4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Tree 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db3625eb",
   "metadata": {},
   "source": [
    "                      +\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                               +                   +\n",
    "                          versicolor           virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da22eca",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4a-1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What does it mean that the features $x1$ and $x2$ do not appear in the decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bdc4c0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "4a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Sepal length and sepal width are not relevant for the classification. This might be either because they are redundant or because they are independent of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b6cc2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** With which method from the lecture might the second tree have been generated from the first one? Explain the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe57ac4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "4b-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Reduced error pruning. Greedily remove the node that reduces error on validation set the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d09c43",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** After training the tree we can calculate the accuracy, i.e. the percentage of the training set that is classified correctly. Although the first tree was trained on the data set until no improvement of the accuracy was possible, its accuracy is *only* 98%. Explain why it is not 100 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab150f3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "4c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The dataset is probably inconsistent, i.e. there are samples with the same features but different classes. Alternative: the thresholding has not produced the optimal partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f7789",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** Tree 2 only has a 96% accuracy on the training set. Why might this tree still be preferable over tree 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39aa7f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Tree 1 is probably overfitted to this specific dataset, i.e. it has not only captured the structure but also the noise in the data. It probably won't generalize as well as the second tree.\n",
    "Another advantage of tree 2 is that it is faster at classifiying new data since less computations have to be made. This difference is hardly noticeable however."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
