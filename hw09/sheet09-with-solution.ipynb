{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2b7a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73131126",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5581a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before end of **Sunday, June 16, 2024**. If you need help (and Google and other resources were not enough), feel free to ask in the StudIP forum, contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd818a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-recap-intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Independence) [0 Points]\n",
    "\n",
    "This exercise is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a9c96",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-independence-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the idea of stochastical independence. What is the intuition and how is it defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98f46a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-independence-a1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Two random variables $X$ and $Y$ are stochastical independent if knowing the value of one does not help to predict the value of the other. In other words, the distribution of $X$ is the same as the conditional distribution of $X$ given $Y$, in formula $P(X|Y) = P(X)$.  Spelling out the definition of $P(X|Y)$, this amounts to $P(X,Y)/P(Y) = P(X)$, or more general $P(X,Y) = P(X)P(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c475e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-independence-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is pairwaise independence and what is mutual independence? Can you provide an example of variables that are pairwise independent but not mutual independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355851e1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-independence-a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "A collection of random variables $X_1,X_2,\\ldots,X_n$ is *pairwise independent*, if each pair of variables $X_i$ and $X_j$, $i\\neq j$, is independent, that is $P(X_i,X_j)=P(X_i,X_j)$. Mutual independence means $P(X_1,\\ldots,P_n)=P(X_1)\\cdots P(X_n)$, meaning that knowing one variable does not allow to make any prediction with respect to the others.  Mutual independence is the stronger property, it implies pairwise independence, but not vice versa.\n",
    "\n",
    "Example: Assume there are two coins. $X_1$ encodes if the first coin shows head, $X_2$ encodes if the second coin shows head and $X_3$ indicates whethr both coins show the same side. Then knowing one of these values does not allow to predict any of the other two (they are pairwise independent), but knowing two of them allows to infer the third (they are not mutually independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ca12f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-independence-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What does it mean that variables are i.i.d.? When do we assume i.i.d. variables in machine learning and what are the consequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c8fbd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-independence-a3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "I.i.d. means independent and identically distributed. For a series of datapoints to be i.i.d. means that they have be sampled from the same distribution and are mutually independent.  Although the assumption of i.i.d. data may not always be true, it often dramatically simplifies the computation, as dependencies between datapoints can be neglected.  So instead of considering a complex joint distribution $P(x_1,\\ldots,x_n)$ one can simply treat the marginal distributions $P(x_1),\\ldots,P(x_n)$ independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eed349",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex4ab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Network Architecture and Decision Boundaries (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cbd6a",
   "metadata": {
    "lines_to_next_cell": 0,
    "nbgrader": {
     "grade": false,
     "grade_id": "ex4abc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** (ML-07 slide 58) visualizes the claim, that all (binary) classifiers can be realized by a network with at most two hidden layers. Explain the underlying idea by sketching for each of the three figures a network that would be able do represent the indicated decision boundaries. Each sketch should clarify:\n",
    "* how many layers are required\n",
    "* how many neurons to use in each layer\n",
    "* what bias term should be put into the neuron.\n",
    "\n",
    "Either use pen and paper and scan/photograph the result or employ your ASCII artist within the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bad23e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex4abc_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Start with the leftmost case: Assume the separatrix (red line) is given in [Hesse normal form](https://en.wikipedia.org/wiki/Hesse_normal_form), that is with a unit normal vector $\\vec{n}_0 = (w_1,w_2)$ orthogonal to the line, pointing in the direction of the positive side, i.e, where the '+' points are located), and the distance value $d$, specifying the closest distance of the separatrix to the origin. Then the network consists of a single perceptron, with two inputs $x_1$ and $x_2$, with input weights $w_1$ and $w_2$ being the components of $\\vec{n}_0$. The bias term of the perceptron, that is the threshold for the activation function) is set to $d$.\n",
    "\n",
    "For the second case, we use two layers: in the first layer, we need perceptron per separatrix (that is 3 in the given example), and an output layer with a single perceptron. That output neuron is supposed to fire if and only if all hidden neurons fire, that is, it should realize a logical conjunction (AND). This could be achieved by setting all weights of that neuron to $1/3$ and the bias value to $1$ (or all weights to $1$ and the bias to $3$).\n",
    "\n",
    "For the third case, we combined multiple copies of case two, that is, we use a first hidden layer consisting of nine perceptrons (three groups à three). Each of these groups is combined by a logical conjunction in the second layer, which now consists of three perceptrons. Finally, an output layer is added, that should fire if any of the neurons in the second layer is active (logical disjunction).  This can be realized by setting all incomming connections as well as the bias value to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f0bc7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(b)** With the online tool [TensorFlow playground](http://playground.tensorflow.org/) it is possible to interactively play with feed forward neural networks, compellingly visualize their behavior and share specific configurations. \n",
    "\n",
    "Follow [this link](http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=gauss&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=1&seed=0.56339&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true) to the TensorFlow playground. If you click it, many features are disabled and set to useful defaults, since they were either not discussed yet in the lecture or are not important for this exercise.\n",
    "\n",
    "You will see a simple configuration: Two activated inputs ($x_1$ and $x_2$), one hidden layer with one neuron (which can be understood as a simple perceptron) and the output shown as a nice picture. It initially shows a training loss of 0.527. Try and run it to see how the perceptron can learn to separate the two clusters. Note that for the rest of the exercise we assume at most about 1000 learning steps (usually many fewer will do it), so don't wait too long in front of your browser.\n",
    "\n",
    "The dataset gets fully classified after very few iterations. Next try the XOR dataset, either by clicking on it on the left (the top right data pattern) or by following [this link](http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=1&seed=0.56339&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true). You will notice that you won't achieve much better results than a loss of 0.4, which is just above chance. Try to improve the result by adding neurons and or layers (but don't change the inputs!) until you get a classification with a loss smaller than 0.01. You may also change the learning rate. Then copy the link from your browser address bar and paste it below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0db84",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex4a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "* [Four neurons in one hidden layer](http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=4&seed=0.33221&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true)\n",
    "\n",
    "* [Two hidden layers with two neurons each (may fall into the wrong local minimum)](http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=2,2&seed=0.33221&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true)\n",
    "\n",
    "* [To avoid the local minimum, it's possible to add one more neuron to the first hidden layer (so it's 3, 2)](http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=3,2&seed=0.33221&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true)\n",
    "\n",
    "By reducing the learning rate to 0.03 it is possible to achieve even better results or similar results with fewer neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9a6af",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex4b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How many neurons in hidden layers are already sufficient to get at least 99% classification (i.e. loss < 0.01) if they are a) in one hidden layer or b) in two hidden layers? You may consider configurations which just need above 1000 iterations to get there as well, but we don't expect you to run any configuration longer than 1000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf62a3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex4b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "In both scenarios, a) and b) four neurons are enough to achieve 99% classification.\n",
    "In fact, in b) they even achieve 100% after very few steps. We can not achieve 100% in 100 steps with scenario a), not even with the maximum number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62674f3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: MLP - Multilayer Perceptron (10 points)\n",
    "\n",
    "Last week you implemented a simple perceptron. We discussed that one can use multiple perceptrons to build a network. This week you will build your own MLP. Again the following code cells are just a guideline. If you feel like it, just follow the algorithm steps and implement the MLP yourself.\n",
    "\n",
    "\n",
    "**Useful python methods:**\n",
    "* [classes again](https://docs.python.org/3/tutorial/classes.html)\n",
    "* [@property of classes](https://docs.python.org/3/library/functions.html?highlight=property#property)\n",
    "* [*: An asterisk in front of an iterable denotes iterable unpacking. Its operand must be an iterable. The iterable is expanded into a sequence of items, which are included in the new tuple, list, or set, at the site of the unpacking.](https://docs.python.org/3/tutorial/controlflow.html#tut-unpacking-arguments)\n",
    "* [np.append()](https://numpy.org/doc/stable/reference/generated/numpy.append.html)\n",
    "* [reversed()](https://docs.python.org/3/library/functions.html#reversed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bed6a7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1a_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "In the following you will be guided through implementing an MLP step by step. Instead of sticking to this guide, you are free to take a complete custom approach instead if you wish.\n",
    "\n",
    "We will take a bottom-up approach: Starting from an individual **perceptron** (aka neuron), we will derive a **layer of perceptrons** and end up with a **multilayer perceptron** (aka neural network). Each step will be implemented as its own python *class*. Such a class defines a type of element which can be instantiated multiple times. You can think of the relation between such instances and their designated classes as individuals of a specific population (e.g. Bernard and Bianca are both individuals of the population mice). Class definitions contain methods, which can be used to manipulate instance of that class or to make it perform specific actions.\n",
    "\n",
    "To guide you along, all required classes and functions are outlined in valid python code with extensive comments. You just need to fill in the gaps. For each method the [docstring](https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring) (the big comment contained by triple quotes at the beginning of the method) describes the arguments that each specific method accepts (`Args`) and the values it is expected to return (`Returns`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2d452",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1a_intro2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Perceptron\n",
    "Similar to last week you here need to implement a perceptron. But instead of directly applying it, we will define a class which is reusable to instantiate a theoretically infinite amount of individual perceptrons. We will need the following three functionalities:\n",
    "\n",
    "#### Weight initialization\n",
    "\n",
    "The weights are initialized by sampling values from a standard normal distribution. There are as many weights as there are values in the input vector and an additional one for the perceptron's bias.\n",
    "\n",
    "#### Forward-Propagation / Activation\n",
    "\n",
    "Calculate the weighted sums of a neuron's inputs and apply it's activation function $\\sigma$. The output vector $o$ of perceptron $j$ of layer $k$ given an input $x$ (the output of the previous layer) in a neural network is given by the following formula. Note: $N$ gives the number of values of a given vector, $w_{j,0}(k)$ specifies the bias of perceptron $j$ in layer $k$ and $w_{j,1...N(x)}(k)$ the other weights of perceptron $j$ in layer $k$.\n",
    "\n",
    "$$o_{k,j}(x) = \\sigma\\left(w_{j,0}(k)+\\sum\\limits_{i=1}^{N(x)} x_i w_{j,i}(k)\\right)$$\n",
    "\n",
    "Think of the weights $w(k)$ as a matrix being located in-between layer $k$ and the layer located *to its left* in the network. So values flowing from layer $k-1$ to layer $k$ are weighted by the values of $w(k)$. As activation function we will use the sigmoid function because of its nice derivative (needed later):\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sigma(x) &= \\frac{1}{1 + \\exp{(-x)}}\\\\\n",
    "\\frac{d\\sigma}{dx}(x) &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align*}$$\n",
    "\n",
    "#### Back-Propagation / Adaptation\n",
    "In order to learn something the perceptron needs to slowly adjust its weights. Each weight $w_{j,i}$ in layer $k$ is adjusted by a value $\\Delta w_{j,i}$ given a learning rate $\\epsilon$, the previous layer's output (or, for the first hidden layer, the network's input) $o_{k-1,i}(x)$ and the layer's error signals $\\delta(k)$ (which will be calculated by the MultilayerPerceptron):\n",
    "\n",
    "$$\\Delta w_{j,i}(k) = \\epsilon\\, \\delta_j(k) o_{k-1,i}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65151f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function σ.\n",
    "# We use scipy's builtin because it fixes some NaN problems for us.\n",
    "# sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"Single neuron handling its own weights and bias.\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, act_func=sigmoid):\n",
    "        \"\"\"Initialize a new neuron with its weights and bias.\n",
    "\n",
    "        Args:\n",
    "            dim_in (int): Dimensionality of the data coming into this perceptron. \n",
    "                In a network of perceptrons this basically represents the \n",
    "                number of neurons in the layer before this neuron's layer. \n",
    "                Used for generating the perceptron's weights vector, which \n",
    "                not only includes one weight per input but also an additional \n",
    "                bias weight.\n",
    "            act_fun (function): Function to apply on activation.\n",
    "        \"\"\"\n",
    "        self.act_func = act_func\n",
    "        # Set self.weights\n",
    "        ### BEGIN SOLUTION \n",
    "        self.weights = np.random.normal(size=dim_in + 1)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate this neuron with a specific input.\n",
    "\n",
    "        Calculate the weighted sum of inputs and apply the activation function.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "\n",
    "        Returns:\n",
    "            float: A real number representing the perceptron's activation after \n",
    "            calculating the weighted sum of inputs and applying the \n",
    "            perceptron's activation function.\n",
    "        \"\"\"\n",
    "        # Return the activation value\n",
    "        ### BEGIN SOLUTION \n",
    "        return self.act_func(self.weights @ np.append(1, x))\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def adapt(self, x, delta, rate=0.03):\n",
    "        \"\"\"Adapt this neuron's weights by a specific delta.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "            delta (float): Weight adaptation delta value.\n",
    "            rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        # Adapt self.weights according to the update rule\n",
    "        ### BEGIN SOLUTION \n",
    "        self.weights += rate * delta * np.append(1, x)\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "p = Perceptron(2)\n",
    "assert p.weights.size == 3, \"Should have a weight per input and a bias.\"\n",
    "assert isinstance(p.activate([2, 1]), float), \"Should activate as scalar.\"\n",
    "assert -1 <= p.activate([100, 100]) <= 1, \"Should activate using sigmoid.\"\n",
    "p.weights = np.array([.5, .5, .5])\n",
    "p.adapt(np.array([2, 3]), np.array(.5))\n",
    "assert np.allclose(p.weights, [0.515, 0.53, 0.545]), \\\n",
    "        \"Should update weights correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925acaa",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1b_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### PerceptronLayer\n",
    "A `PerceptronLayer` is a combination of multiple `Perceptron` instances. It mainly is concerened with passing input and delta values to its individual neurons. There is no math to be done here!\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "When initializing a `PerceptronLayer` (like this: `layer = PerceptronLayer(5, 3)`), the `__init__` function is called. It creates a list of `Perceptron`s: For each output value there must be one perceptron. Each of those perceptrons receives the same inputs and the same activation function as the perceptron layer.\n",
    "\n",
    "#### Activation\n",
    "\n",
    "During the activation step, the perceptron layer activates each of its perceptrons. These values will not only be needed for forward propagation but will also be needed for implementing backpropagation in the `MultilayerPerceptron` (coming up next).\n",
    "\n",
    "#### Adaptation\n",
    "\n",
    "To update its perceptrons, the perceptron layer adapts each one with the corresponding delta. For this purpose, the MLP passes a list of input values and a list of deltas to the adaptation function. The inputs are passed to *all* perceptrons. The list of deltas is exactly as long as the list of perceptrons: The first delta is for the first perceptron, the second for the second, etc. The delta values themselves will be computed by the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5dcded",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PerceptronLayer:\n",
    "    \"\"\"Layer of multiple neurons.\n",
    "    \n",
    "    Attributes:\n",
    "        perceptrons (list): List of perceptron instances in the layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, act_func=sigmoid):\n",
    "        \"\"\"Initialize the layer as a list of individual neurons.\n",
    "\n",
    "        A layer contains as many neurons as it has outputs, each\n",
    "        neuron has as many input weights (+ bias) as the layer has inputs.\n",
    "\n",
    "        Args:\n",
    "            dim_in (int): Dimensionality of the expected input values,\n",
    "                also the size of the previous layer of a neural network.\n",
    "            dim_out (int): Dimensionality of the output, also the requested \n",
    "                amount of in this layer and the input dimension of the\n",
    "                next layer.\n",
    "            act_func (function): Activation function to use in each perceptron of\n",
    "                this layer.\n",
    "        \"\"\"\n",
    "        # Set self.perceptrons to a list of Perceptrons\n",
    "        ### BEGIN SOLUTION\n",
    "        self.perceptrons = [Perceptron(dim_in, act_func)\n",
    "                            for _ in range(dim_out)]\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate this layer by activating each individual neuron.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "\n",
    "        Retuns:\n",
    "            ndarray: Vector of output values which can be \n",
    "            used as input to another PerceptronLayer instance.\n",
    "        \"\"\"\n",
    "        # return the vector of activation values\n",
    "        ### BEGIN SOLUTION\n",
    "        return np.array([p.activate(x) for p in self.perceptrons])\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def adapt(self, x, deltas, rate=0.03):\n",
    "        \"\"\"Adapt this layer by adapting each individual neuron.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "            deltas (ndarray): Vector of delta values.\n",
    "            rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        # Update all the perceptrons in this layer\n",
    "        ### BEGIN SOLUTION\n",
    "        for perceptron, delta in zip(self.perceptrons, deltas):\n",
    "            perceptron.adapt(x, delta, rate)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    @property\n",
    "    def weight_matrix(self):\n",
    "        \"\"\"Helper property for getting this layer's weight matrix.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: All the weights for this perceptron layer.\n",
    "        \"\"\"\n",
    "        return np.asarray([p.weights for p in self.perceptrons]).T\n",
    "\n",
    "\n",
    "l = PerceptronLayer(3, 2)\n",
    "assert len(l.perceptrons) == 2, \"Should have as many perceptrons as outputs.\"\n",
    "assert len(l.activate([1,2,3])) == 2, \"Should provide correct output amount.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf249b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1c_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### MultilayerPerceptron\n",
    "\n",
    "#### Forward-Propagation / Activation\n",
    "Propagate the input value $x$ through each layer of the network, employing the output of the previous layer as input to the next layer.\n",
    "\n",
    "#### Back-Propagation / Adaptation\n",
    "This is the most complex step of the whole task. Split into three separate parts:\n",
    "\n",
    "1. ***Forward propagation***: Compute the outputs for each individual layer – similar to the forward-propagation step above, but we need to keep track of the intermediate results to compute each layer's errors. That means: Store the input as the first \"output\" and then activate each of the network's layers using the *previous* layer's output and store the layer's activation result.\n",
    "\n",
    "2. ***Backward propagation***: Calculate each layer's error signals $\\delta_i(k)$. The important part here is to do so from the last to the first array, because each layer's error depends on the error from its following layer:\n",
    "\n",
    "    $$\\delta_i(k) = \\varphi'_i(k) \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)$$\n",
    "\n",
    "    Since we use the sigmoid function $\\sigma(t)=\\frac{1}{1+e^{-t}}$ as activation function $\\varphi$ the derivative is $\\varphi'_i(k) = \\sigma'_i(k) = \\sigma_i(k)\\ (1 - \\sigma_i(k))$ and the formula becomes:\n",
    "\n",
    "    $$\\delta_i(k) = \\sigma_i(k)\\ (1 - \\sigma_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)$$\n",
    "\n",
    "    (*Hint*: For the last layer (i.e. the first you calculate the $\\delta$ for) the sum in the formula above is the total network error. For all preceding layers $k$ you need to recalculate `e` using the $\\delta$ and weights of layer $k+1$. We already implemented a helper function for you to access the weights of a specific layer. Check the `PerceptronLayer` if you did not find it yet.)\n",
    "\n",
    "3. ***Adaptation***: Call each layers adaptation function with its input, its designated error signals and the given learning rate.\n",
    "\n",
    "Hint: The last two steps can be performed in a single loop if you wish, but make sure to use the non-updated weights for the calculation of the next layer's error signals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b6f07",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "    \"\"\"Network of perceptrons, also a set of multiple perceptron layers.\n",
    "    \n",
    "    Attributes:\n",
    "        layers (list): List of perceptron layers in the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, *layers):\n",
    "        \"\"\"Initialize a new network, madeup of individual PerceptronLayers.\n",
    "\n",
    "        Args:\n",
    "            *layers: Arbitrarily many PerceptronLayer instances.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate network and return the last layer's output.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "\n",
    "        Returns:\n",
    "            (ndarray): Vector of output values from the last layer of the \n",
    "            network after propagating forward through the network.\n",
    "        \"\"\"\n",
    "        # Propagate activation through the network\n",
    "        # and return output for last layer\n",
    "        ### BEGIN SOLUTION \n",
    "        for layer in self.layers:\n",
    "            x = layer.activate(x)\n",
    "        return x        \n",
    "        ### END SOLUTION\n",
    "\n",
    "    def adapt(self, x, t, rate=0.03):\n",
    "        \"\"\"Adapt the whole network given an input and expected output.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "            t (ndarray): Vector of target values (expected outputs).\n",
    "            rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        # Activate each layer and collect intermediate outputs.\n",
    "        # * Iterate through all layers and activate them with the output\n",
    "        #       of the previous layer. \n",
    "        # * Create a list with all the outputs.\n",
    "        # * Call the first layer with the input x\n",
    "        ### BEGIN SOLUTION\n",
    "        outputs = [x]\n",
    "        for layer in self.layers:\n",
    "            outputs.append(layer.activate(outputs[-1]))        \n",
    "        ### END SOLUTION\n",
    "\n",
    "        # Calculate error 'e' between t and network output, i.e\n",
    "        # the last element of the list of outputs. Here the error\n",
    "        # is the difference between t and the output.\n",
    "        ### BEGIN SOLUTION \n",
    "        e = t - outputs[-1]\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # Backpropagate the error through the network computing\n",
    "        # the intermediate deltas and adapting each layer.\n",
    "        # * Reversly iterate through the layers\n",
    "        # * Get the input of the layer. I.e. the output of the previous layer\n",
    "        # * Get the output of the layer\n",
    "        # * Compute delta using the output of the layer and the error.\n",
    "        #      (Use the derivative of the sigmoid activation function)\n",
    "        #      (The error for the last layer is computed above)\n",
    "        # * Compute the error for the previous layer in the network\n",
    "        #      using the weight matrix and delta\n",
    "        # * Adapt the layer using the input of the layer, delta and the learning rate\n",
    "        ### BEGIN SOLUTION\n",
    "        for k, layer in reversed(list(enumerate(self.layers, 1))):\n",
    "            layer_input = outputs[k - 1]\n",
    "            layer_output = outputs[k]\n",
    "            delta = (layer_output * (1 - layer_output)) * e\n",
    "            e = (layer.weight_matrix @ delta)[1:]\n",
    "            layer.adapt(layer_input, delta, rate)\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ed02d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fe9b8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Problem Definition\n",
    "Before we start, we need a problem to solve. In the following cell we first generate some two dimensional data (= $\\text{input\\_dim}$) between 0 and 1 and label all data according to a binary classification: If the data is close to the center (radius < 2.5), it belongs to one class, if it is further away from the center it belongs to the other class.\n",
    "\n",
    "In the cell below we visualize the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caab9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(a, b, n=1):\n",
    "    \"\"\"Returns n floats uniformly distributed between a and b.\"\"\"\n",
    "    return (b - a) * np.random.random_sample(n) + a\n",
    "\n",
    "\n",
    "n = 1000\n",
    "radius = 5\n",
    "r = np.append(uniform(0, radius * .5, n // 2),\n",
    "              uniform(radius * .7, radius, n // 2))\n",
    "angle = uniform(0, 2 * np.pi, n)\n",
    "x = r * np.sin(angle) + uniform(-radius, radius, n)\n",
    "y = r * np.cos(angle) + uniform(-radius, radius, n)\n",
    "inputs = np.vstack((x, y)).T\n",
    "targets = np.less(np.linalg.norm(inputs, axis=1), radius * .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc551558",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_demo1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(num='Data')\n",
    "ax.set(title='Labeled Data')\n",
    "ax.scatter(*inputs.T, 2, c=targets, cmap='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9610d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_intro2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Model Design\n",
    "The following cell already contains a simple model with a single layer. Play around with some different configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c628f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1d_solution1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "MLP = MultilayerPerceptron(\n",
    "    PerceptronLayer(2, 1),\n",
    ")\n",
    "# Adapt this MLP\n",
    "### BEGIN SOLUTION\n",
    "MLP = MultilayerPerceptron(\n",
    "    PerceptronLayer(2, 4),\n",
    "    PerceptronLayer(4, 2),\n",
    "    PerceptronLayer(2, 1),\n",
    ")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadb532",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_intro3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Training\n",
    "Train the network on random samples from the data. Try adjusting the epochs and watch the training performance closely using different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4cc2c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_demo2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "from matplotlib import cm\n",
    "\n",
    "EPOCHS = 200000\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "fig, ax = plt.subplots(num='Training')\n",
    "scatter = ax.scatter(*inputs.T, 2)\n",
    "plt.show()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    sample_index = np.random.randint(0, len(targets))\n",
    "    MLP.adapt(inputs[sample_index], targets[sample_index])\n",
    "\n",
    "    if (epoch % 2500) == 0:\n",
    "        outputs = np.squeeze([MLP.activate(x) for x in inputs])\n",
    "        predictions = np.round(outputs)\n",
    "        accuracy = np.sum(predictions == targets) / len(targets) * 100\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "        scatter.set_color(cm.RdYlBu(outputs))\n",
    "        ax.set(title=f'Training {epoch / EPOCHS * 100:.0f}%: {accuracy:.2f}%. Best accuracy: {max_accuracy:.2f}%')\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9257918",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_intro4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c632c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_demo3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "ax[0, 0].scatter(*inputs.T, 2, c=outputs, cmap='RdYlBu')\n",
    "ax[0, 0].set_title('Continuous Classification')\n",
    "ax[0, 1].set_title('Discretized Classification')\n",
    "ax[0, 1].scatter(*inputs.T, 2, c=np.round(outputs), cmap='RdYlBu')\n",
    "ax[1, 0].set_title('Original Labels')\n",
    "ax[1, 0].scatter(*inputs.T, 2, c=targets, cmap='RdYlBu')\n",
    "ax[1, 1].set_title('Wrong Classifications')\n",
    "ax[1, 1].scatter(*inputs.T, 2, c=(targets != np.round(outputs)), cmap='OrRd')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf6e7e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d_intro5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Results\n",
    "Document your results in the following cell. We are interested in which network configurations you tried and what accuracies they resulted in. Did you run into problems during training? Was it steady or did it get stuck? Did you recognize anything about the training process? How could we get better results? Tell us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785ce9f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Answer:** 2 hidden and one output layer with a total of 7 neurons can already stably render results of 90%+ (with some data generation luck). \n",
    "\n",
    "During training the model sometimes gets stuck in saddle points for a long time. One way to tackle this would be to compute noisy gradients instead of the real gradients -- something that *stochastic gradient descent*, the main method most frameworks for working with neural networks use by default, makes use of as well. Some more information on that specific problem and solution [here](http://www.offconvex.org/2016/03/22/saddlepoints/). \n",
    "\n",
    "Another problem with our training approach is that we train on the complete dataset without a training/evaluation split! If we would split the data we could also make use of \"early stopping\": Instead of using the final state of the network for our evaluation, we could use the one which got the best max accuracy on the evaluation set during training by saving it whenever the max accuracy goes up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd748e58",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex-nn1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Nearest Neighbor Theory (5 points)\n",
    "\n",
    "In this exercise you will examine some aspects of the nearest neighbor algorithm in more detail. Not all questions have been answered in the lecture, so you may refer to literature for a deeper understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fb5bf",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex-nn1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a) The nearest neighbor algorithm\n",
    "\n",
    "Explain in your own words the nearest neighbor algorithm.  What prerequisites have to be fulfilled to apply the algorithm? To what kind of tasks can it be applied? What additional problem arises when considering multiple nearest neighbors instead of only one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0024b1f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex-nn1a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "During training, the nearest neighbor learner stores all examples presented (it acts like a rote learner). During inference, that is, when it has to predict properties of new, previously unseen data, it looks up the most similar memorized example and bases its prediction on that example.\n",
    "For this to work, the nearest neighbor approach needs to assess the similarity between data points, that is,  it requires the data to be from a metric space, allowing to obtain distances between data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509a2c2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex-nn1-bias",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Inductive bias\n",
    "\n",
    "What is the inductive bias of nearest neighbor learning? In what sense is it a local method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14023e84",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex-nn1-bias-solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The nearest neighbor approach is the assumption that similar observations behave similarly. More explicitly: for a simple nearest neighbor classifier ($1$-NN), the class of $x$ is the same as the class of nearby\n",
    "datapoints.\n",
    "\n",
    "The algorithm is local with respect to the input space: the result for a data point $x$ only depends on the $k$ closest neighbors. All changes to the data outside this local region, e.g., adding more training data or changing the properties of far away points, will not alter the estimate for $x$, even if the algorithm is retrained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce3d4c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex-nn1-dimensionality",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### c) Curse of dimensionality\n",
    "\n",
    "What problems do you anticipate when applying the nearest neighbor algorithm in high dimensional spaces? How would you mitigate those problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43301a0f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex-nn1-dimensionality_solution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In high dimensions, a curious effect arises: the ratio between the nearest and the farthest points approaches 1, i.e., the points essentially become uniformly distant from each other (see exercise 1 on sheet 06). This phenomenon can be observed for wide variety of distance metrics, but it is more pronounced for the Euclidean metric than, e.g., Manhattan metric.  Hence for such metrics the basic assumption of the nearest neighbor algorithm, that closer points are more relevant than points farther apart, becomes meaningless.\n",
    "\n",
    "See also:\n",
    "* Kevin Beyer et al. (1999): *When Is \"Nearest Neighbor\" Meaningful?*\n",
    "  doi: [10.1007/3-540-49257-7_15](https://doi.org/10.1007/3-540-49257-7_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fc328",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex-nn1-search",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### d) Nearest neighbor search\n",
    "\n",
    "The lecture slides claim that application of the nearest neighbor approach may be slow (for many stored examples). Explain this statement: what is the complexity?  What methods could be applied to improve the situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e18a4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex-nn1-search-solution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "To find the nearest neighbor, a naive approach is to compute the distance of the new datapoint to each datapoint stored in the database and then take the one with minimal distance. The complexity of this approach is linear in the size of the database $N$, that is $O(N)$.\n",
    "\n",
    "One can try to improve this situation by using storage strategies that allow for more efficient lookup of nearest neighbors.  For data from a Euclidean space, [k-d-trees](https://en.wikipedia.org/wiki/K-d_tree) could be used, which can reduce complexity for nearest neighbor search in average to $O(\\log N)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3192359",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex-nn1-noise",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### e) Sensitivity to noise\n",
    "\n",
    "Why is the nearest neighbor approach sensitive to noise? What is the essential difference to other learning approaches, that are less sensitive to noise? How could the nearest neighbor approach be made less noise sensitive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be740b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex-nn1-noise-solution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The nearest neighbor algorithm stores all training examples, including noisy data points, e.g., data points with wrong labels.  As a consequence, all nearby points will be treated incorrectly (if no other neighbors are taken into consideration).\n",
    "\n",
    "Other learning algorithms may explicitly or implicitly filter out noise during training by focusing more on the coarse structure of the data than on specific details.  This may be due to limited capacity, or by explicitly averaging over multiple datapoints. The nearest neighbor algorithm can achieve a similar effect, by taking multiple neighbors into account ($k$-nearest neighbor), thereby reducing the effect of individual noisy data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
