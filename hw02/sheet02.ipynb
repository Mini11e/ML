{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e28dcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44fca659c1e85aa18c1a44bcb17af98a",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17606b2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac582d5001c9669f006e7190d2ff2030",
     "grade": false,
     "grade_id": "cell-175c5b8f652e026d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise Sheet 02: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be770f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78ce1178407b0a2ae29dbb6cdce302dc",
     "grade": false,
     "grade_id": "cell-0779efa0bccb27ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "By now everyone should have found a group. If someone still has none but wants to participate in the course please contact one of the tutors.\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, April 28th, 2024**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whom ever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e5533",
   "metadata": {},
   "source": [
    "# install ipympl\n",
    "for the following interactive plots we use ipympl which is not installed in your environment, since it was not in the `ml.yml` file.\n",
    "We can extend our ml environment by installing the packages `ipympl` and `ipywidgets` in a code cell via `pip`.\n",
    "We could install it via `conda` as well but `conda` and `pip` use different sources for the packages (conda packages are usually older).\n",
    "We use `pip` since the current version from `conda` is not compatable with the rest of our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ipympl ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48519ffe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16b7327c168b66662f21312fefa412ff",
     "grade": false,
     "grade_id": "math-euclid",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Math recap (Euclidean Space) [0 Points]\n",
    "\n",
    "This exercise is supposed to be very easy and is voluntary. There will be a similar exercise on every sheet.\n",
    "It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them.\n",
    "Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session.\n",
    "Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3ebcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "899ac70855dbd336b6edef96c4c1a6f5",
     "grade": false,
     "grade_id": "math-euclid-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a *Euclidean space*? What is the *Cartesian plane*? How are they usually denoted? How to write points in these spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf33cfb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10da800d32483f0d4bfa9cf43624b3c0",
     "grade": true,
     "grade_id": "math-euclid-a1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbe8c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bf071a4c6d1ff977bcd336eb96d7daa",
     "grade": false,
     "grade_id": "math-euclid-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the *norm* of a vector in a Euclidean space? How to *add* and *substract* two vectors? How is the *Euclidean distance* defined? Are there other ways to measure distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4de05",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f95335b2228d0766f6bdbbba83f857d",
     "grade": true,
     "grade_id": "math-euclid-a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0175844",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7a7304a941a4864f1c0ad2cce2d86b3",
     "grade": false,
     "grade_id": "math-euclid-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What is the (standard) *scalar product* of two vectors? How is it related to the length and angle between these vectors? Name some use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131e418",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5c2f82430f4ba30219c7b0e9a43836d",
     "grade": true,
     "grade_id": "math-euclid-a3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b07ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eaf761564b24bec7944095ba22c7eeb",
     "grade": false,
     "grade_id": "1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Rosner test (4 points)\n",
    "\n",
    "The Rosner test is an iterative procedure to remove outliers of a data set via a z-test. In this exercise you will implement it and apply it to a sample data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9b3f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d82297f0a89eb3fc4696837945639779",
     "grade": false,
     "grade_id": "1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a) Outliers\n",
    "\n",
    "First of all, think about why we use procedures like this and answer the following questions: \n",
    "\n",
    "What are causes for outliers? And what are our options to deal with them? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e56b75",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1781fd2b6c19b6721f71e77b2c55156c",
     "grade": true,
     "grade_id": "1a_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac4dcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9bfc15074353da2770f3cbbac0461cb",
     "grade": false,
     "grade_id": "1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Rosner test\n",
    "\n",
    "In the following you find a stub for the implementation. The dataset is already generated. Now it is your turn to write the Rosner test and detect the outliers in the data. \n",
    "\n",
    "`data` is a `np.array` of `[x, y]` coordinates. `outliers` is a list of `[x, y]` coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84207ad0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eb285760d0dffc0d3c6cd386bf4dbdb",
     "grade": true,
     "grade_id": "1b_code",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate dataset\n",
    "data = list(zip(np.random.uniform(size=100), np.random.normal(size=100)))\n",
    "data += list(zip(np.random.uniform(size=10), np.random.normal(0, 10, size=10)))\n",
    "data = np.array(data)\n",
    "outliers = []\n",
    "\n",
    "# just to check if everything is pretty\n",
    "fig_rosner_data = plt.figure('The Dataset')\n",
    "plt.scatter(data[:,0], data[:,1], marker='x')\n",
    "plt.axis([0, 1, -20, 20])\n",
    "fig_rosner_data.canvas.draw()\n",
    "\n",
    "# Now find the outliers, add them to 'outliers', and remove them from 'data'.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# plot results\n",
    "outliers = np.array(outliers)\n",
    "fig_rosner = plt.figure('Rosner Result')\n",
    "plt.scatter(data[:,0], data[:,1], c='b', marker='x', label='cleared data')\n",
    "plt.scatter(outliers[:,0], outliers[:,1], c='r', marker='x', label='outliers')\n",
    "plt.axis([0, 1, -20, 20])\n",
    "plt.legend(loc='lower right');\n",
    "fig_rosner.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e287598",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1dfc4abf364b714a9638eab20d8d06f",
     "grade": false,
     "grade_id": "cell-d8241b1497d8735e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Expectation Maximization (2 points)\n",
    "\n",
    "### a) What applications of the EM algorithm did you get to know in the lecture? Describe how EM treats the missing value problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84c43e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcd4867f43f549e78c769a8661bc9a9d",
     "grade": true,
     "grade_id": "cell-6cdd837f733a953b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f732e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d724a3805e2ad713e3540f50d3611540",
     "grade": false,
     "grade_id": "cell-c508b74ff424a7b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### b) Explain how EM algorithm can be used to fit a mixture model to a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16818402",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b60108758e24c7bad5723dc999c0a45b",
     "grade": true,
     "grade_id": "cell-1eb075d9361e00bb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcbd339",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0145714c0565d4366a960b27dd9a4b6d",
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Implement Expectation Maximization for Soft Clustering (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bbc5f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4837f98db19d41d82644aa2d496be3b",
     "grade": false,
     "grade_id": "3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As some parts of this exercise would require some more knowledge of Python than what was already discussed in the practice sessions we built a small number of templates for you to use. However, if you prefer to do so you are also allowed to just go ahead and implement everything yourself!\n",
    "\n",
    "Use the next cell to implement your own solution or, if you want some more guidance, skip the next cell and continue the exercise at  [Step 1) Load the data](#Step-1%29-Load-the-data).\n",
    "\n",
    "Here is an overview of what you have to do:\n",
    "\n",
    "**1) Load the data:**\n",
    "\n",
    "Load the provided data set. It is stored in `em_normdistdata.txt`. We call the set $X$ and each individual data $x \\in X$.\n",
    "\n",
    "**2) Initialize EM:**\n",
    "\n",
    "Initialize three normal distributions whose parameters will be changed iteratively by the EM to converge close to the original distributions.\n",
    "\n",
    "Each normal distribution $j$ has three parameters: $\\mu_j$ (the mean), $\\sigma_j$ (the standard deviation), $\\alpha_j$ (the proportion of the normal distribution in the mixture, that means $\\sum\\limits_j\\alpha_j=1$).\n",
    "\n",
    "Initialize the three parameters using three random partitions $S_j$ of the data set. Calculate each $\\mu_j$ and $\\sigma_j$ and set $\\alpha_j = \\frac{|S_j|}{|X|}$.\n",
    "\n",
    "**3) Implement the expectation step:**\n",
    "\n",
    "Perform a soft classification of the data samples with the three normal distributions. That means: Calculate the likelihood that a data sample $x_i$ belongs to distribution $j$ given parameters $\\mu_j$ and $\\sigma_j$. Or in other words, what is the likelihood of $x_i$ to be drawn from $N_j(\\mu_j, \\sigma_j)$? When you got the likelihood, weight the result by $\\alpha_j$.\n",
    "\n",
    "As a last step normalize the results such that the likelihoods of a data sample $x_i$ sum up to $1$.\n",
    "\n",
    "**4) Implement the maximization step:**\n",
    "\n",
    "In the maximization step each $\\mu_j$, $\\sigma_j$ and $\\alpha_j$ is updated. First calculate the new means:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij}x_i$$\n",
    "\n",
    "That means $\\mu_j$ is the weighted mean of all samples, where the weight is their likelihood of belonging to distribution $j$.\n",
    "\n",
    "Then calculate the new $\\sigma_j$. Each new $\\sigma_j$ is the standard deviation of the normal distribution with the new $\\mu_j$, so for the calculation you already use the new $\\mu_j$:\n",
    "\n",
    "$$\\sigma_j = \\sqrt{ \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij} \\left(x_i - \\mu_j\\right)^2 }$$\n",
    "\n",
    "To calculate the new $\\alpha_j$ for each distribution, just take the mean of $p_j$ for each normal distribution $j$.\n",
    "\n",
    "**5) Perform the complete EM and plot your results:**\n",
    "\n",
    "Build a loop around the iterative procedure of expectation and maximization which stops when the changes in all $\\mu_j$ and $\\sigma_j$ are sufficiently small enough.\n",
    "\n",
    "Plot your results after each step and mark which data points belong to which normal distribution. If you don't get it to work, just plot your final solution of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cc0b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1151768e2ad255f18086da2569feb3b",
     "grade": true,
     "grade_id": "3b_self",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Free space to implement your own solution -- either use this OR use the following step by step guide. \n",
    "# You may use scipy.stats.norm.pdf for your own implementation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3cdd2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "155160ce65563c76a18df3dccfbba85a",
     "grade": false,
     "grade_id": "3b1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 1) Load the data\n",
    "\n",
    "\n",
    "Load the provided data set. It is stored in `em_normdistdata.txt`. We call the set $X$ and each individual data $x \\in X$. \n",
    "\n",
    "*Hint:* Figure out a way on how numpy can load text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b7c8d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6da9b1d7d87bebb4bba5784d9cff3e33",
     "grade": true,
     "grade_id": "3b1_code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Loads the data stored in file_name into a numpy array.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    # YOUR CODE HERE\n",
    "    return result\n",
    "\n",
    "\n",
    "assert load_data('em_normdistdata.txt').shape == (200,) , \"The data was not properly loaded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de877b99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c38879618eae0d156b639e62aa1bb4f7",
     "grade": false,
     "grade_id": "3b1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "*Optional:* The data consists of 200 data points drawn from three normal distributions. To get a feeling for the data set you can plot the data with the following cell. Change the number of bins to get a rough idea of how the three distributions might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c48b56",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac010f5d17d133648cf7e2899cdc71ce",
     "grade": true,
     "grade_id": "3b1_test",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_data('em_normdistdata.txt')\n",
    "fig = plt.figure()\n",
    "plt.title(f'Data overview ({len(data)} datapoints)')\n",
    "# YOUR CODE HERE\n",
    "# You may change the number of bins here\n",
    "plt.hist(data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d12af2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "950ceab7618d14e322349e05c53dcd40",
     "grade": false,
     "grade_id": "3b2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 2) Initialize EM\n",
    "\n",
    "Below is a class definition `NormPDF` which represents the probability density function (pdf) of the normal distribution with an additional parameter $\\alpha$. The class is explained in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425cf16",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce62bec9d2125f84217d809c162d07c4",
     "grade": false,
     "grade_id": "3b2a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NormPDF():\n",
    "    \"\"\"\n",
    "    A representation of the probability density function of the normal distribution\n",
    "    for the EM Algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu=0, sigma=1, alpha=1):\n",
    "        \"\"\"\n",
    "        Initializes the normal distribution with mean mu, standard deviation sigma \n",
    "        and proportion of the normal distribution in the mixture alpha.\n",
    "        The defaults are 0, 1, and 1 respectively.\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Returns the evaluation of this normal distribution at x.\n",
    "        Does not take alpha into account!\n",
    "        \"\"\"\n",
    "        return np.exp(-(x - self.mu) ** 2 / (2 * self.sigma ** 2)) / (np.sqrt(np.pi * 2) * self.sigma)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        A simple string representation of this instance.\n",
    "        \"\"\"\n",
    "        return 'NormPDF({self.mu:.2f},{self.sigma:.2f},{self.alpha:.2f})'.format(self=self)\n",
    "    \n",
    "    def plot(self, ax):\n",
    "        \"\"\"\n",
    "        Plot the density curve in the given Matplotlib axes object.\n",
    "        \"\"\"\n",
    "        x = np.linspace(self.mu-4*self.sigma, self.mu+4*self.sigma, 200)\n",
    "        ax.plot(x, self.alpha*self(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08875fd8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0fa2533c31258ae500e7f21f1217aca",
     "grade": false,
     "grade_id": "3b2b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The class `NormPDF` offers several class methods: `__init__`, `__call__`, `__repr__`. They are all special Python functions which are overloaded so they can be used in a nice way. Note that all methods take as the first parameter `self`: this is just the python way of passing the instance itself to the method so that it becomes possible to access its data. You can always ignore it for now and just assume that the methods only need the parameters which follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78884f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22b6956505aeba8d26b4dcaedd575db2",
     "grade": false,
     "grade_id": "3b2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "`__init__`: This is the constructor. When a new instance of the class is created this method is used. It takes the parameters `mu`, `sigma`, and `alpha`. Note that if you leave out parameters, they will be set to some default values.\n",
    "So you can create `NormPDF` instances like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd4e4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f70dede2f2a5f74227349f8fb8803a3",
     "grade": false,
     "grade_id": "3b2d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "a = NormPDF()             # No parameters: mu = 0, sigma = 1, alpha = 1\n",
    "b = NormPDF(1)            # mu = 1, sigma = 1, alpha = 1\n",
    "c = NormPDF(1, alpha=0.1) # skips sigma but sets alpha, thus: mu = 1, sigma = 1, alpha = 0.4\n",
    "d = NormPDF(0, 0.5)       # mu = 0, sigma = 0.5, alpha = 1\n",
    "e = NormPDF(0, 0.5, 0.9)  # mu = 0, sigma = 0.5, alpha = 0.9\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "for normpdf in a, b, c, d, e:\n",
    "    normpdf.plot(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee4d83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2d16d076056d8ffd484cd7f4b92a71f",
     "grade": false,
     "grade_id": "3b2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "`__call__`: This is a very cool feature of Python. By implementing this method one can make an instance *callable*. That basically means one can use it as if it was a function. The `NormPDF` instances can be called with an x value (or a numpy array of x values) to get the evaluation of the normal distribution at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47afae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e211098bfd1324043a83328237f76339",
     "grade": false,
     "grade_id": "3b2f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf = NormPDF()\n",
    "print(normpdf(0))\n",
    "print(normpdf(0.5))\n",
    "print(normpdf(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d7964",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cf9cecf48680271e4b9b3cd9140e024",
     "grade": false,
     "grade_id": "3b2g",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "`__repr__`: This method will be used in Python when one calls `repr(NormPDF())`. As long as `__str__` is not implemented (which you saw in assignment 3c of last week's sheet) `str(NormPDF())` will also use this method. This comes in handy for printing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03b2a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f0b3ff3894b4589c7029088ec88f610",
     "grade": false,
     "grade_id": "3b2h",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "normpdf2 = NormPDF(1, 0.5, 0.9)\n",
    "print(normpdf1)\n",
    "print([normpdf1, normpdf2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b3f76",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "274dded14a04c72ba9cf530657d5f362",
     "grade": false,
     "grade_id": "3b2i",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "It is also possible to change the values of an instance of the NormPDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cfa9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63cc57dd9458996f3a2cbe81e08f0618",
     "grade": false,
     "grade_id": "3b2j",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))\n",
    "\n",
    "normpdf1.mu = 1\n",
    "normpdf1.sigma = 2\n",
    "normpdf1.alpha = 0.9\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8501256",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28b0ade192f5f1e05ac41bc24101bc58",
     "grade": false,
     "grade_id": "3b2k",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that you know how the `NormPDF` class works, it is time for the implementation of the initialization function. Here is the task again:\n",
    "\n",
    "Write a function `gaussians = initialize_EM(data, num_distributions)` to initialize the EM.\n",
    "\n",
    "Each normal distribution $j$ has three parameters: $\\mu_j$ (the mean), $\\sigma_j$ (the standard deviation), $\\alpha_j$ (the proportion of the normal distribution in the mixture, that means $\\sum\\limits_j\\alpha_j=1$).\n",
    "Initialize the three parameters using three random partitions $S_j$ of the data set. Calculate each $\\mu_j$ and $\\sigma_j$ and set $\\alpha_j = \\frac{|S_j|}{|X|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7e7f1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "160b43d12147ceaeb4ed811c263d3571",
     "grade": true,
     "grade_id": "3b2_code",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_EM(data, num_distributions):\n",
    "    \"\"\"\n",
    "    Initializes the EM algorithm by calculating num_distributions NormPDFs\n",
    "    from a random partitioning of data. I.e., the data set is randomly\n",
    "    divided into num_distribution parts, and each part is used to initialize\n",
    "    mean, standard deviation and alpha parameter of a NormPDF object.\n",
    "    \n",
    "    Args:\n",
    "        data (array): A collection of data.\n",
    "        num_distributions (int): The number of distributions to return.\n",
    "        \n",
    "    Returns:\n",
    "        A list of num_distribution NormPDF objects, initialized from a\n",
    "        random partioning of the data.\n",
    "    \"\"\"\n",
    "    gaussians = None\n",
    "    # YOUR CODE HERE\n",
    "    return gaussians\n",
    "\n",
    "\n",
    "normpdfs_ = initialize_EM(np.linspace(-1, 1, 100), 2)\n",
    "assert len(normpdfs_) == 2, \"The number of initialized distributions is not correct.\"\n",
    "\n",
    "epsilon = 1e-8\n",
    "assert abs(1 - sum([normpdf.alpha for normpdf in normpdfs_])) < epsilon , \"Sum of all alphas is not 1.0!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e41440",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1dc7394ede5c8e447eca17053b6cd144",
     "grade": false,
     "grade_id": "3b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 3) Implement the expectation step\n",
    "\n",
    "Perform a soft classification of the data samples with the normal distributions. That means: Calculate the likelihood that a data sample $x_i$ belongs to distribution $j$ given parameters $\\mu_j$ and $\\sigma_j$. Or in other words, what is the likelihood of $x_i$ to be drawn from $N_j(\\mu_j, \\sigma_j)$? When you got the likelihood, weight the result by $\\alpha_j$.\n",
    "\n",
    "As a last step normalize the results such that the likelihoods of a data sample $x_i$ sum up to $1$.\n",
    "\n",
    "*Hint:* Store the data in a different array before you normalize it to not run into problems with partly normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b80ae7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "636f8d1e5d9b559a15400abcc2c3c81c",
     "grade": true,
     "grade_id": "3b3_code",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def expectation_step(gaussians, data):\n",
    "    \"\"\"\n",
    "    Performs the expectation step of the EM.\n",
    "    \n",
    "    Args:\n",
    "        gaussians (list): A list of NormPDF objects.\n",
    "        data (array): The data vector.\n",
    "        \n",
    "    Returns:\n",
    "        An array of shape (len(data), len(gaussians))\n",
    "        which contains normalized likelihoods for each sample\n",
    "        to denote to which of the normal distributions it \n",
    "        most likely belongs to.\n",
    "    \"\"\"\n",
    "    expectation = None\n",
    "    # YOUR CODE HERE\n",
    "    return expectation\n",
    "\n",
    "assert expectation_step([NormPDF(), NormPDF()], np.linspace(-2, 2, 100)).shape == (100, 2) , \"Shape is not correct!\"\n",
    "\n",
    "x = np.linspace(data.min(), data.max(), 2*len(data))\n",
    "normpdfs = initialize_EM(data, 2)\n",
    "expectations = expectation_step(normpdfs, x)\n",
    "\n",
    "import itertools\n",
    "plt.figure()\n",
    "colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y', 'k'])\n",
    "for pdf, expect, color in zip(normpdfs, expectations.T, colors):\n",
    "    plt.plot(x, pdf.alpha * pdf(x), color=color)\n",
    "    plt.plot(x, expect, color=color, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a68c2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b681f42617531523ddc94caef30b5b3a",
     "grade": false,
     "grade_id": "3b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 4) Implement the maximization step\n",
    "\n",
    "In the maximization step each $\\mu_j$, $\\sigma_j$ and $\\alpha_j$ is updated. First calculate the new means:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij}x_i$$\n",
    "\n",
    "That means $\\mu_j$ is the weighted mean of all samples, where the weight is their likelihood of belonging to distribution $j$.\n",
    "\n",
    "Then calculate the new $\\sigma_j$. Each new $\\sigma_j$ is the standard deviation of the normal distribution with the new $\\mu_j$, so for the calculation you already use the new $\\mu_j$:\n",
    "\n",
    "$$\\sigma_j = \\sqrt{ \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij} \\left(x_i - \\mu_j\\right)^2 }$$\n",
    "\n",
    "To calculate the new $\\alpha_j$ for each distribution, just take the mean of $p_j$ for each normal distribution $j$.\n",
    "\n",
    "**Caution:** For the next step it is necessary to know how much all $\\mu$ and $\\sigma$ changed. For that the function `maximization_step` should return a numpy array of those (absolute) changes. For example if $\\mu_0$ changed from 0.1 to 0.15, $\\sigma_0$ from 1 to 0.9, $\\mu_1$ from 0.5 to 0.6, and $\\sigma_1$, $\\mu_2$, and $\\sigma_2$ stayed the same, we expect the function to return `np.array([0.05, 0.1, 0.1, 0, 0, 0])` (however, the order is not important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e59be9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0c8f756e23c34671c3ab1ce2af7b8e9",
     "grade": true,
     "grade_id": "3b4_code",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def maximization_step(gaussians, data, expectation):\n",
    "    \"\"\"\n",
    "    Performs the maximization step of the EM.\n",
    "    Modifies the gaussians by updating their mus and sigmas.\n",
    "    \n",
    "    Args:\n",
    "        gaussians (list): A list of NormPDF objects.\n",
    "        data (array): The data vector.\n",
    "        expectation (array): The expectation values for data element\n",
    "            (as computed by expectation_step()).\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of absolute changes in any mu or sigma, \n",
    "        that means the returned array has twice as many elements as\n",
    "        the supplied list of gaussians.\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    # YOUR CODE HERE\n",
    "    return np.array(changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42e31f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ba65c711e6d7fbe7f85285a51fa9635",
     "grade": false,
     "grade_id": "3b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**5) Perform the complete EM and plot your results:**\n",
    "\n",
    "Initialize three normal distributions whose parameters will be changed iteratively by the EM to converge close to the original distributions.\n",
    "\n",
    "Build a loop around the iterative procedure of expectation and maximization which stops when the changes in all $\\mu_j$ and $\\sigma_j$ are sufficiently small enough.\n",
    "\n",
    "Plot your results after each step and mark which data points belong to which normal distribution. If you don't get it to work, just plot your final solution.\n",
    "\n",
    "*Hint:* Remember to load the data and initialize the EM before the loop.\n",
    "\n",
    "*Hint:* A function `plot_intermediate_result` to plot your result after each step is already defined in the next cell. Take a look at what arguments it takes and try to use it in your loop.\n",
    "\n",
    "*Hint:* To plot your final result the first three images and corresponding code examples on the tutorial of [`plt.plot(...)`](http://matplotlib.org/users/pyplot_tutorial.html) should help you.\n",
    "\n",
    "*Optional:* Run the code multiple times. If your results are changing, use `np.random.seed(2)` in the beginning of the cell to get consistent results (any other integer will work as well, but 2 has some good results for the example solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801827d1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eef199fde4a6067b1735b5d3f34257ce",
     "grade": true,
     "grade_id": "cell-4e7335a369316ed0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "\n",
    "# Sets the random seed to a fix value to make results consistent\n",
    "np.random.seed(2)\n",
    "\n",
    "figure, axis = plt.subplots(1)\n",
    "axis.set_xlim(-5, 5)\n",
    "axis.set_ylim(-0.2, 4)\n",
    "axis.set_title('EM Animation')\n",
    "\n",
    "# Perform the initialization.\n",
    "data = load_data('em_normdistdata.txt')\n",
    "gaussians = initialize_EM(data, 3)\n",
    "\n",
    "def update_plot(gaussians, data, mapping):\n",
    "    \"\"\"\n",
    "    Gets a list of gaussians and data input. The mapping\n",
    "    parameter is a list of indices of gaussians. Each value\n",
    "    corresponds to the data value at the same position and \n",
    "    maps this data value to the proper gaussian.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y', 'k'])\n",
    "    plots = []\n",
    "    #container = plt.plot(x, gaussians[0](x), 'r', x, gaussians[1](x), 'g', x, gaussians[2](x), 'b')\n",
    "    for j, (N, color) in enumerate(zip(gaussians, colors)):\n",
    "        plots.append(plt.plot(x, gaussians[j](x), color)[0])\n",
    "        plots.append(plt.plot(data[mapping == j], [0] * len(data[mapping == j]), color, marker='x', markersize=5)[0])\n",
    "    return plots\n",
    "\n",
    "# Loop until the changes are small enough.\n",
    "eps = 0.05\n",
    "changes = [float('inf')] * 2\n",
    "artists = []  # for animation\n",
    "while max(changes) > eps:\n",
    "    # Iteratively apply the expectation step, followed by the maximization step.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Optional: Calculate the parameters to update the plot.\n",
    "    artists.append(update_plot(gaussians, data, np.argmax(expectation, 1)))\n",
    "\n",
    "ani = animation.ArtistAnimation(fig=figure, artists=artists, interval=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d302760",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e2d3481604f1b81104ad88601a1bfc8",
     "grade": false,
     "grade_id": "4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 4: Soft Clustering with Gaussian Mixture (6 points)\n",
    "\n",
    "In this assignment you will calculate the update rules for a Gaussian Mixture model required for the M-step of the EM algorithm. The Gaussian Mixture model can be used for soft clustering since it allows us to express varying degrees of certainty about the membership of individual samples. It is one of the most widely used models since Gaussian distributions generally have the property of fitting all different kinds of data reasonably well.\n",
    "\n",
    "A mixture model with $K$ components is in general of the form:\n",
    "\n",
    "$$ p(\\mathbf{x}|\\mathbf{\\theta}) = \\sum_{k=1}^K\\alpha_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)$$\n",
    "where $\\sum_{k=1}^K\\alpha_k = 1$.\n",
    "\n",
    "This means that the probability of observing a dataset $\\mathbf{x}$ given the parameter vector $\\mathbf{\\theta}$ can be expressed as the sum of $K$ individual distributions $p_k$ with parameters $\\mathbf{\\theta}_k \\subseteq {\\theta}$ which are weighted by respective class probabilities $\\alpha_k$. The probability for individual data $x_i \\in \\textbf{x}$ is calculated correspondingly (note however, that of course the values for individual data differs from the overall probability). \n",
    "\n",
    "We can now choose distributions for $p_k$ and $\\alpha_k$ and we get a whole collection full of different possible models, each of which has its own advantages and disadvantages (you can check <a href='https://en.wikipedia.org/wiki/Mixture_model'>Wikipedia</a> if you want an overview). The easiest case is where our mixing distributions are normally distributed, $p_k \\sim \\mathcal{N}(\\mu_k,\\sigma_k)$, and our latent class probabilities have a discrete distribution where we only have $\\alpha_k \\in [0,1]$ and the constraint $\\sum_k\\alpha_k=1$.\n",
    "\n",
    "If we were to randomly pick values for the parameter vector $\\theta$ then we would now have a generative model that can produce naturally clustered data for us, we would just have to sample $\\hat{x} \\sim p(\\mathbf{x}|\\mathbf{\\theta})$. However, we want to go into the opposite direction and figure out what the distribution of the labels given the data is. This can be calculated easily by Bayes' Theorem for each model $k$, where the (latent) probability for choosing model $k$ is $p(k|\\mathbf{\\theta}_k) = \\alpha_k$:\n",
    "\n",
    "$$p(k|\\mathbf{x},\\mathbf{\\theta})=\\frac{p(k|\\mathbf{\\theta}_k)p(\\mathbf{x}|k,\\mathbf{\\theta}_k)}{\\sum_{k'=1}^Kp(k'|\\mathbf{\\theta}_{k'})p(\\mathbf{x}|k',\\mathbf{\\theta}_{k'})} = \\frac{\\alpha_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)}{\\sum_{k'=1}^K\\alpha_{k'}p_{k'}(\\mathbf{x}|\\mathbf{\\theta}_{k'})}$$\n",
    "\n",
    "That sounds good enough, but where do we actually start now? We have a mathematical framework pinned down, but it contains many variables and it is not *a priori* obvious how we can figure out the best values for them. We *have* some data which we want to use to determine the parameters so the usual approach would be to simply calculate a Maximum Likelihood Estimator (MLE) or an Maximum A Posteriori Estimator (MAP) by maximizing the above formulas over the possible parameters with a method like Gradient Descent. It turns out however that this is very, very hard to do (optimal MLE for a GMM is NP-hard (Aloise et al. 2009; Drineas et al. 2004)) since the $\\alpha_k$ and the $\\theta_k$ are strongly interdependent and neither is known. It *can* still be done with some work-arounds, but there is also an alternative path that we can go down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c3655",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7631ec1c7fb0d6169a7538ea7d1a66e7",
     "grade": false,
     "grade_id": "4_background",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "*(The following exhibition is only for those who are interested in the mathematical background of the EM-algorithm, those who only want to solve the exercise can skip ahead to the function that you have to maximize.)*\n",
    "\n",
    "We want to maximize the log likelihood given as\n",
    "$$\\mathcal{\\ell}(\\mathbf{\\theta})=\\sum_{i=1}^N\\log p(x_i|\\mathbf{\\theta}) = \\sum_{i=1}^N\\log\\left[\\sum_{k=1}^Kp_k(x_i|\\mathbf{\\theta}_k)\\right].$$\n",
    "All the problems occur because we have a sum inside the logarithm and so we can't pull the logarithm further in towards the densitiy and that is what makes the problem so hard. If we just *ignore* the inner sum we get an expression\n",
    "$$\\mathcal{\\ell}_c(\\mathbf{\\theta}) = \\sum_{i=1}^N\\log p_k(x_i|\\mathbf{\\theta}_k)$$\n",
    "which would be much nicer to compute. But now we have a free floating $k$ in the subscript of our density! Which one of the mixing distributions are we talking about here? Kind of all of them at once. But we need one quantity to represent all the distributions. So to get rid of the $k$ we take the expected value with respect to the latent variable $k$ and receive a function that only depends on $\\mathbf{\\theta}$:\n",
    "$$Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) = \\mathbb{E}\\left[\\mathcal{\\ell}_c\\left(\\mathbf{\\theta}\\middle|\\mathcal{\\theta}^{t-1}\\right)\\right]$$\n",
    "\n",
    "Calculating this $Q$ function can be difficult - but at least we only have to do it once instead of solving an NP-hard optimization problem every time we have a new dataset. We will only provide you the final formula, you will have to trust us on this one:\n",
    "\n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log\\alpha_k + \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "This still looks nasty but it really isn't that bad! Since $\\theta^{t-1}$ is known at time $t$ we can calculate $p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)$ with Bayes' Theorem as stated above and replace these expressions with constants $r_{i,k}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3cf1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6496e996f85803588963a147c5013458",
     "grade": false,
     "grade_id": "4_explanation",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**This is where your work begins:**\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$\n",
    "In the lecture you saw a proof that if we choose\n",
    "$$\\mathbf{\\theta}^t = \\argmax_{\\mathbf{\\theta}} Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$$\n",
    "that the likelihood of the parameter is non-decreasing then. So we want to maximize $Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$ for the parameters $\\left(\\alpha_1\\dots,\\alpha_K\\right)$ and $\\theta = \\left(\\mu_1,\\dots,\\mu_K,\\sigma_1,\\dots,\\sigma_K\\right)$. So your job is to take the derivative of \n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k r_{i,k}\\log\\alpha_k + \\sum_i\\sum_k r_{i,k}\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "with respect to these variables, to set it equal to 0 and to solve for the value that you are currently maximizing for. You only have to do this for the one dimensional case, i.e. \n",
    "$$p_k(x_i|\\mathbf{\\theta}_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2d87c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1c3b420867c7a2a536fcdc60a371fb7",
     "grade": false,
     "grade_id": "4_b_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## a) Calculate the maximizer for the $\\mu_k$:\n",
    "\n",
    "*Hint:* derive $Q(\\theta,\\theta^{t-1})$ with respect $\\mu_{k}$ and set the derivative to $0$ to get an optimal value for $\\mu_k$.  You do not need to form the second derivative to check for that being a maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c9657",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "817e01dae7690d6c949625eaa4834c0b",
     "grade": true,
     "grade_id": "4_b_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a53d127",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dc1125c98923b5cd52966528a14e8d5",
     "grade": false,
     "grade_id": "4_c_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## b) Calculate the maximizer for the $\\sigma_k^2$:\n",
    "\n",
    "*Hint:* now derive $Q(\\theta,\\theta^{t-1})$ with respect $\\sigma_{k}^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef0760",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb98c9da31ad0d50d1d69ae76140e52c",
     "grade": true,
     "grade_id": "4_c_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b37273",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bd4beea1a3da4cf70fe2e54b8a3e7f3",
     "grade": false,
     "grade_id": "4_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## c) Calculate the maximizer for the $\\alpha_k$ \n",
    "\n",
    "You need the ensure $\\sum_k\\alpha_k =1$. You can either use the formula to express one of the $\\alpha_i$ in terms of all the others or use a Lagrangian Multiplier for this.\n",
    "\n",
    "Hint: using a Lagrangian Multiplier, you would start with an equation like this:\n",
    "$$\\frac{\\partial}{\\partial \\alpha_k}\\left(Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) - \\lambda \\left(\\sum_k \\alpha_k - 1\\right)\\right) = 0$$\n",
    "Some brief introduction to Langrangian multipliers can be found in Bishop (2006): *Pattern Recognition and Machine Learning*, Appendix E."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50963bfd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b723b244c95e4273b2ee50db1dc0fd2",
     "grade": true,
     "grade_id": "4_a_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
