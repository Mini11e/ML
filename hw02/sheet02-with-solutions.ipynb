{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2b7a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23050bad",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-175c5b8f652e026d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise Sheet 02: Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf3796",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0779efa0bccb27ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "By now everyone should have found a group. If someone still has none but wants to participate in the course please contact one of the tutors.\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, April 28th, 2024**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whom ever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287e608",
   "metadata": {},
   "source": [
    "# install ipympl\n",
    "for the following interactive plots we use ipympl which is not installed in your environment, since it was not in the `ml.yml` file.\n",
    "We can extend our ml environment by installing the packages `ipympl` and `ipywidgets` in a code cell via `pip`.\n",
    "We could install it via `conda` as well but `conda` and `pip` use different sources for the packages (conda packages are usually older).\n",
    "We use `pip` since the current version from `conda` is not compatable with the rest of our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3716c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ipympl ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8684c0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-euclid",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Math recap (Euclidean Space) [0 Points]\n",
    "\n",
    "This exercise is supposed to be very easy and is voluntary. There will be a similar exercise on every sheet.\n",
    "It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them.\n",
    "Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session.\n",
    "Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e551f8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-euclid-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a *Euclidean space*? What is the *Cartesian plane*? How are they usually denoted? How to write points in these spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd34e6a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-euclid-a1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "A *Euclidean space* is a space in which the axioms of Euclidean geometry hold. There is one such space for each dimension $n$. One usually uses Cartesian coordinates, i.e. tuples of real numbers, to refer to points in these spaces, so the *Cartesian plane* is the two-dimensional Eucliden space modeled by pairs of real coordinates. The $n$-dimensional Euclidean space is usually denoted by\n",
    "$$\\mathbb{R}^n = \\underbrace{\\mathbb{R}\\times\\ldots\\times\\mathbb{R}}_{n\\text{-times}}$$\n",
    "(sometimes also $\\mathbf{R}^n$). Points in that space are hence denoted by $n$-tuples of real numbers,\n",
    "$$\\vec{x}=\\mathbf{x} = (x_1,\\ldots,x_n)\\in\\mathbb{R}^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a6ef2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-euclid-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the *norm* of a vector in a Euclidean space? How to *add* and *substract* two vectors? How is the *Euclidean distance* defined? Are there other ways to measure distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37ef34",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-euclid-a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The *(Euclidean) norm* of a vector\n",
    "\n",
    "$$\\|\\vec{x}\\| = \\sqrt{x_1^2+\\ldots+x_n^2}$$\n",
    "\n",
    "it can be interpreted as the *length* of the vector. The sum of two vectors is defined as\n",
    "$$\\vec{x}+\\vec{y} = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix}+\n",
    "\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n",
    "\\begin{pmatrix} x_1+y_1 \\\\ \\vdots \\\\ x_n+y_n\\end{pmatrix}$$\n",
    "and analogously the difference is\n",
    "$$\\vec{x}-\\vec{y} = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix}-\n",
    "\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n",
    "\\begin{pmatrix} x_1-y_1 \\\\ \\vdots \\\\ x_n-y_n\\end{pmatrix}$$\n",
    "The euclidean distance is defined as the norm of the difference of two vectors\n",
    "\n",
    "$$d(\\vec{x},\\vec{y}) = \\|\\vec{x}-\\vec{y}\\| = \\sqrt{(x_1-y_1)^2+\\ldots+(x_n-y_n)^2} = \\left(\\sum_{i=1}^n (x_i-y_i)^{2}\\right)^{\\frac{1}{2}}$$\n",
    "\n",
    "(remember the Pythagorean theorem). Be replacing 2 by $p$ one gets the $p$-norm instead of the Euclidean norm, from which one then gets the $p$-metric to measure distances. There are even more distance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2485f7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-euclid-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What is the (standard) *scalar product* of two vectors? How is it related to the length and angle between these vectors? Name some use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a71485",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-euclid-a3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The scalar product of two vectors $\\vec{x}$ and $\\vec{y}$ is defined as\n",
    "\n",
    "$$ \\vec{x}\\cdot\\vec{y} = \\langle\\vec{x},\\vec{y}\\rangle = x_1y_1+\\ldots+x_ny_n = \\sum_{i=1}^{n}x_iy_i$$\n",
    "\n",
    "On a geometric level the scalar product can be computed as\n",
    "\n",
    "$$\\vec{x}\\cdot\\vec{y} = \\|\\vec{x}\\| \\|\\vec{y}\\| \\cos(\\sphericalangle(\\vec{x},\\vec{y}))$$\n",
    "\n",
    "where $\\sphericalangle(\\vec{x},\\vec{y})$ denotes the angle between vectors $\\vec{x}$ and $\\vec{y}$. The scalar product is sometimes used as a similarity measure, which will be $0$ for orthogonal (i.e. unrelated) vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e245a5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Rosner test (4 points)\n",
    "\n",
    "The Rosner test is an iterative procedure to remove outliers of a data set via a z-test. In this exercise you will implement it and apply it to a sample data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277dbaae",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a) Outliers\n",
    "\n",
    "First of all, think about why we use procedures like this and answer the following questions: \n",
    "\n",
    "What are causes for outliers? And what are our options to deal with them? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a3c55",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1a_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "There are different types of outliers which can have different causes. They could arise through measurement or technical errors when collecting data. This may be connected to having a sharp cut-off in regard to the range of measurements, which could lead to a high concentration of values at the artificial boundaries of an experiment. However they may also show us a true underlying effect in our data that we didn't expect or account for. This might be the case when we are treating the measurements as a single distribution, when in reality there are actually two underlying distributions. Lastly, our distribution might actually naturally have a high variance, which makes outliers or extreme values a natural part of the distribution.\n",
    "\n",
    "First, we need to detect probable outliers. In order to decide which data points we want to declare as an outlier we have to find a model for regular, meaning \"not outlying\", data points. What we do most of the time is to assume a normal distribution underlying the data (or a multivariate distribution where each cluster is normally distributed).\n",
    "\n",
    "One option is to calculate the z-value for each data point (a measure of the distance from the mean in terms of the standard deviation) -- data points with a high z-value would be regarded outliers, a common threshold would be a z value bigger than 3. This can be improved by using the median instead of the mean and tweaking the threshold. The Rosner test takes it one step further by iteratively calculating z-values and removing found outliers until none can be found anymore. This can be done one outlier at a time or k outliers at a time for more efficiency. \n",
    "\n",
    "A different approach would be to not remove the outliers completely, but to weight them according to the z-values. And lastly an alternative for complete removal would be to fill up the emerging gaps with values that fit the distribution better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29bed6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Rosner test\n",
    "\n",
    "In the following you find a stub for the implementation. The dataset is already generated. Now it is your turn to write the Rosner test and detect the outliers in the data. \n",
    "\n",
    "`data` is a `np.array` of `[x, y]` coordinates. `outliers` is a list of `[x, y]` coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81ad3f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1b_code",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate dataset\n",
    "data = list(zip(np.random.uniform(size=100), np.random.normal(size=100)))\n",
    "data += list(zip(np.random.uniform(size=10), np.random.normal(0, 10, size=10)))\n",
    "data = np.array(data)\n",
    "outliers = []\n",
    "\n",
    "# just to check if everything is pretty\n",
    "fig_rosner_data = plt.figure('The Dataset')\n",
    "plt.scatter(data[:,0], data[:,1], marker='x')\n",
    "plt.axis([0, 1, -20, 20])\n",
    "fig_rosner_data.canvas.draw()\n",
    "\n",
    "# Now find the outliers, add them to 'outliers', and remove them from 'data'.\n",
    "### BEGIN SOLUTION\n",
    "while True:\n",
    "    # Find mean and standard deviation to calculate z values.\n",
    "    stdev = np.std(data[:,1])\n",
    "    m = np.mean(data[:,1])\n",
    "    zs = [abs(value - m) / stdev for value in data[:,1]]\n",
    "\n",
    "    # Find maximum z value and its index.\n",
    "    z = max(zs)\n",
    "    z_index = zs.index(z)\n",
    "    \n",
    "    # Check if we have to remove the value, and if so, remove it\n",
    "    # and append it to the outlier list.\n",
    "    # Otherwise stop (break).\n",
    "    if z > 3:\n",
    "        outliers.append(data[z_index, :])\n",
    "        data = np.delete(data, z_index, 0)\n",
    "    else:\n",
    "        break\n",
    "### END SOLUTION\n",
    "\n",
    "# plot results\n",
    "outliers = np.array(outliers)\n",
    "fig_rosner = plt.figure('Rosner Result')\n",
    "plt.scatter(data[:,0], data[:,1], c='b', marker='x', label='cleared data')\n",
    "plt.scatter(outliers[:,0], outliers[:,1], c='r', marker='x', label='outliers')\n",
    "plt.axis([0, 1, -20, 20])\n",
    "plt.legend(loc='lower right');\n",
    "fig_rosner.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d2bbb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8241b1497d8735e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Expectation Maximization (2 points)\n",
    "\n",
    "### a) What applications of the EM algorithm did you get to know in the lecture? Describe how EM treats the missing value problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d8c0c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6cdd837f733a953b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In the EM-Algorithm all known values are considered via their likelihood depending on the distribution. In the same way hidden (i.e. missing) values are considered as depending on the probability distribution and additionally on the known values. So the complete distribution can be seen as the product of two probability distributions (known and missing values).\n",
    "\n",
    "The algorithm searches for the parameters that maximize the log-likelihood. As they depend on the missing values, those are averaged out. In an iterative procedure the estimated parameter is improved (M-step) followed by averaging over the missing values using the obtained parameter (E-step). This will lead the estimation of the parameter to converge to a local maximum which hopefully is close to the real parameter value. The principle in handling missing values here is to not try to regain them somehow, but to invent values from a model optained through the probability distribution. In the best case this does not lead to information loss, although it generally does. However, this at least makes the existing values technically usable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2f393",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c508b74ff424a7b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### b) Explain how the EM algorithm can be used to fit a mixture model to a given dataset.\n",
    "<!--\n",
    "### b) Explain how the EM algorithm can be used to fit a (gaussian) mixture model to a given (1 dimensional) dataset.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578a198",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1eb075d9361e00bb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "A mixture model describes a complex probability distribution $P(x)$ by a (weighted) mixture of $n$ simple distributions $P(x;\\theta_i)$ for $i=1,\\ldots,n$.  Here $\\theta_i$ denotes the parameter(s) of the $i$-th component (for example, in case of a Gaussian Mixture Model the distributions $P(X;\\theta_i)$ would be normal distributions $\\mathcal{N}(X;\\mu_i,\\sigma_i^2)$ and hence $\\theta_i=(\\mu_i\\sigma_i^2)$ would be the parameters of such a distribution, that is mean and variance).  The weighing is done by scalar factors $\\alpha_i$, which can be considered the probability of that component, i.e., the $\\alpha_i$ should add up to one.  So in summary, a mixture model can generally be described as $$\\sum_{i=1}^{n}\\alpha_i P(x;\\theta_i)$$ \n",
    "\n",
    "Model fitting in general refers to the tasks of choosing the model parameter(s) $\\Theta$, in our case $\\Theta=(\\alpha_1,\\theta_1,\\ldots,\\alpha_n,\\theta_n)$, in a way that their likelihood, with respect to the given (training) data $\\mathcal{D}$, is maximized. That is, formally our task is to solve\n",
    "\n",
    "$$\\Theta^{\\ast}=\\operatorname{argmax}_{\\Theta} \\prod_{x\\in\\mathcal{D}}\\sum_{i=1}^{n}\\alpha_i P(x;\\theta_i)$$\n",
    "\n",
    "This is a hard problem, as this likehood depends on the probability distribution of each component (i.e., on the $\\theta_i$) as well as on the importance the model assigns to each component (i.e., $\\alpha_i$). Changing one requires reevaluating the other.\n",
    "\n",
    "The EM algorithm now takes an iterative approach to approximate a solution: It starts with some initially choosen values for $\\Theta$ (i.e., $\\alpha_i$ and $\\theta_i$). It then repeats the eponymous steps:\n",
    "\n",
    "E-step: as we do not know to which component a datapoint $x$ \"belongs\", we do the likelihood analysis for every component, and then we weight this contribution by the strength of the component, i.e. we form a weighted average, or in stochastical terms we compute an \"expectation\". The resulting likelihood function depends on the distributions parameters $\\theta_i$ as well as the mixture coefficients $\\alpha_i$.\n",
    "\n",
    "M-step: we choose the parameters $\\theta_i$ and $\\alpha_i$ in a way that they maximize the likelihood function obtained in the E-step. As proven in the lecture, this in fact is guaranteed to improve the likelihood of the model.\n",
    "\n",
    "However, as changing the $\\alpha_i$ (in the M-step) affects the likelihood function of the model (computed in the E-step), the obtained parameters are not optimal.\n",
    "\n",
    "Remark: The mixture model can be seen as a special case of a more general class of hidden variable model $P(X,Z)$, consisting of two classes of variables: the observable variable(s) $X$ that is supposed do describe the data distribution, and some hidden (latent/unobservable) variable(s) $Z$ that is introduced for modeling purposes (in our case of a mixture model that would be a discrete variable, selecting the mixture component for a datapoint). Such models are very powerful and they are ubiquitous in machine learning\n",
    "(neural networks, generative AI, temporal modeling, etc.).  The underlying learning task always involves understanding the distribution of the observable $X$ as well as the hidden $Z$.  The EM algorithm is one way to address such problems, but it is not suitable\n",
    "for all types of models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66d061",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Implement Expectation Maximization for Soft Clustering (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe74ac7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As some parts of this exercise would require some more knowledge of Python than what was already discussed in the practice sessions we built a small number of templates for you to use. However, if you prefer to do so you are also allowed to just go ahead and implement everything yourself!\n",
    "\n",
    "Use the next cell to implement your own solution or, if you want some more guidance, skip the next cell and continue the exercise at  [Step 1) Load the data](#Step-1%29-Load-the-data).\n",
    "\n",
    "Here is an overview of what you have to do:\n",
    "\n",
    "**1) Load the data:**\n",
    "\n",
    "Load the provided data set. It is stored in `em_normdistdata.txt`. We call the set $X$ and each individual data $x \\in X$.\n",
    "\n",
    "**2) Initialize EM:**\n",
    "\n",
    "Initialize three normal distributions whose parameters will be changed iteratively by the EM to converge close to the original distributions.\n",
    "\n",
    "Each normal distribution $j$ has three parameters: $\\mu_j$ (the mean), $\\sigma_j$ (the standard deviation), $\\alpha_j$ (the proportion of the normal distribution in the mixture, that means $\\sum\\limits_j\\alpha_j=1$).\n",
    "\n",
    "Initialize the three parameters using three random partitions $S_j$ of the data set. Calculate each $\\mu_j$ and $\\sigma_j$ and set $\\alpha_j = \\frac{|S_j|}{|X|}$.\n",
    "\n",
    "**3) Implement the expectation step:**\n",
    "\n",
    "Perform a soft classification of the data samples with the three normal distributions. That means: Calculate the likelihood that a data sample $x_i$ belongs to distribution $j$ given parameters $\\mu_j$ and $\\sigma_j$. Or in other words, what is the likelihood of $x_i$ to be drawn from $N_j(\\mu_j, \\sigma_j)$? When you got the likelihood, weight the result by $\\alpha_j$.\n",
    "\n",
    "As a last step normalize the results such that the likelihoods of a data sample $x_i$ sum up to $1$.\n",
    "\n",
    "**4) Implement the maximization step:**\n",
    "\n",
    "In the maximization step each $\\mu_j$, $\\sigma_j$ and $\\alpha_j$ is updated. First calculate the new means:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij}x_i$$\n",
    "\n",
    "That means $\\mu_j$ is the weighted mean of all samples, where the weight is their likelihood of belonging to distribution $j$.\n",
    "\n",
    "Then calculate the new $\\sigma_j$. Each new $\\sigma_j$ is the standard deviation of the normal distribution with the new $\\mu_j$, so for the calculation you already use the new $\\mu_j$:\n",
    "\n",
    "$$\\sigma_j = \\sqrt{ \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij} \\left(x_i - \\mu_j\\right)^2 }$$\n",
    "\n",
    "To calculate the new $\\alpha_j$ for each distribution, just take the mean of $p_j$ for each normal distribution $j$.\n",
    "\n",
    "**5) Perform the complete EM and plot your results:**\n",
    "\n",
    "Build a loop around the iterative procedure of expectation and maximization which stops when the changes in all $\\mu_j$ and $\\sigma_j$ are sufficiently small enough.\n",
    "\n",
    "Plot your results after each step and mark which data points belong to which normal distribution. If you don't get it to work, just plot your final solution of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70225b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3b_self",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Free space to implement your own solution -- either use this OR use the following step by step guide. \n",
    "# You may use scipy.stats.norm.pdf for your own implementation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f0a47",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 1) Load the data\n",
    "\n",
    "\n",
    "Load the provided data set. It is stored in `em_normdistdata.txt`. We call the set $X$ and each individual data $x \\in X$. \n",
    "\n",
    "*Hint:* Figure out a way on how numpy can load text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd0d33",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3b1_code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Loads the data stored in file_name into a numpy array.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    ### BEGIN SOLUTION\n",
    "    result = np.loadtxt(file_name)\n",
    "    ### END SOLUTION\n",
    "    return result\n",
    "\n",
    "\n",
    "assert load_data('em_normdistdata.txt').shape == (200,) , \"The data was not properly loaded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6f301",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "*Optional:* The data consists of 200 data points drawn from three normal distributions. To get a feeling for the data set you can plot the data with the following cell. Change the number of bins to get a rough idea of how the three distributions might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3653501",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3b1_test",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_data('em_normdistdata.txt')\n",
    "fig = plt.figure()\n",
    "plt.title(f'Data overview ({len(data)} datapoints)')\n",
    "### BEGIN SOLUTION\n",
    "### END SOLUTION\n",
    "# You may change the number of bins here\n",
    "plt.hist(data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb00079",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 2) Initialize EM\n",
    "\n",
    "Below is a class definition `NormPDF` which represents the probability density function (pdf) of the normal distribution with an additional parameter $\\alpha$. The class is explained in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad3b19",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NormPDF():\n",
    "    \"\"\"\n",
    "    A representation of the probability density function of the normal distribution\n",
    "    for the EM Algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu=0, sigma=1, alpha=1):\n",
    "        \"\"\"\n",
    "        Initializes the normal distribution with mean mu, standard deviation sigma \n",
    "        and proportion of the normal distribution in the mixture alpha.\n",
    "        The defaults are 0, 1, and 1 respectively.\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Returns the evaluation of this normal distribution at x.\n",
    "        Does not take alpha into account!\n",
    "        \"\"\"\n",
    "        return np.exp(-(x - self.mu) ** 2 / (2 * self.sigma ** 2)) / (np.sqrt(np.pi * 2) * self.sigma)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        A simple string representation of this instance.\n",
    "        \"\"\"\n",
    "        return 'NormPDF({self.mu:.2f},{self.sigma:.2f},{self.alpha:.2f})'.format(self=self)\n",
    "    \n",
    "    def plot(self, ax):\n",
    "        \"\"\"\n",
    "        Plot the density curve in the given Matplotlib axes object.\n",
    "        \"\"\"\n",
    "        x = np.linspace(self.mu-4*self.sigma, self.mu+4*self.sigma, 200)\n",
    "        ax.plot(x, self.alpha*self(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f90fb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The class `NormPDF` offers several class methods: `__init__`, `__call__`, `__repr__`. They are all special Python functions which are overloaded so they can be used in a nice way. Note that all methods take as the first parameter `self`: this is just the python way of passing the instance itself to the method so that it becomes possible to access its data. You can always ignore it for now and just assume that the methods only need the parameters which follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b17260",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "`__init__`: This is the constructor. When a new instance of the class is created this method is used. It takes the parameters `mu`, `sigma`, and `alpha`. Note that if you leave out parameters, they will be set to some default values.\n",
    "So you can create `NormPDF` instances like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8121d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "a = NormPDF()             # No parameters: mu = 0, sigma = 1, alpha = 1\n",
    "b = NormPDF(1)            # mu = 1, sigma = 1, alpha = 1\n",
    "c = NormPDF(1, alpha=0.1) # skips sigma but sets alpha, thus: mu = 1, sigma = 1, alpha = 0.4\n",
    "d = NormPDF(0, 0.5)       # mu = 0, sigma = 0.5, alpha = 1\n",
    "e = NormPDF(0, 0.5, 0.9)  # mu = 0, sigma = 0.5, alpha = 0.9\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "for normpdf in a, b, c, d, e:\n",
    "    normpdf.plot(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc253d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "`__call__`: This is a very cool feature of Python. By implementing this method one can make an instance *callable*. That basically means one can use it as if it was a function. The `NormPDF` instances can be called with an x value (or a numpy array of x values) to get the evaluation of the normal distribution at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58c946",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf = NormPDF()\n",
    "print(normpdf(0))\n",
    "print(normpdf(0.5))\n",
    "print(normpdf(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0954f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2g",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "`__repr__`: This method will be used in Python when one calls `repr(NormPDF())`. As long as `__str__` is not implemented (which you saw in assignment 3c of last week's sheet) `str(NormPDF())` will also use this method. This comes in handy for printing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758db6ff",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2h",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "normpdf2 = NormPDF(1, 0.5, 0.9)\n",
    "print(normpdf1)\n",
    "print([normpdf1, normpdf2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65caac36",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2i",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "It is also possible to change the values of an instance of the NormPDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4f21f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2j",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))\n",
    "\n",
    "normpdf1.mu = 1\n",
    "normpdf1.sigma = 2\n",
    "normpdf1.alpha = 0.9\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d450d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b2k",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that you know how the `NormPDF` class works, it is time for the implementation of the initialization function. Here is the task again:\n",
    "\n",
    "Write a function `gaussians = initialize_EM(data, num_distributions)` to initialize the EM.\n",
    "\n",
    "Each normal distribution $j$ has three parameters: $\\mu_j$ (the mean), $\\sigma_j$ (the standard deviation), $\\alpha_j$ (the proportion of the normal distribution in the mixture, that means $\\sum\\limits_j\\alpha_j=1$).\n",
    "Initialize the three parameters using three random partitions $S_j$ of the data set. Calculate each $\\mu_j$ and $\\sigma_j$ and set $\\alpha_j = \\frac{|S_j|}{|X|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41d34c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3b2_code",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_EM(data, num_distributions):\n",
    "    \"\"\"\n",
    "    Initializes the EM algorithm by calculating num_distributions NormPDFs\n",
    "    from a random partitioning of data. I.e., the data set is randomly\n",
    "    divided into num_distribution parts, and each part is used to initialize\n",
    "    mean, standard deviation and alpha parameter of a NormPDF object.\n",
    "    \n",
    "    Args:\n",
    "        data (array): A collection of data.\n",
    "        num_distributions (int): The number of distributions to return.\n",
    "        \n",
    "    Returns:\n",
    "        A list of num_distribution NormPDF objects, initialized from a\n",
    "        random partioning of the data.\n",
    "    \"\"\"\n",
    "    gaussians = None\n",
    "    ### BEGIN SOLUTION\n",
    "    # generate len(data) many random integers between 0 and num_distributions\n",
    "    partition_mapping = np.random.randint(0, num_distributions, len(data))\n",
    "    # initialize num_distributions many NormPDFs\n",
    "    gaussians = [NormPDF() for i in range(num_distributions)]\n",
    "    \n",
    "    # calculate the mean, standard deviation and alpha depending on the partition mapping\n",
    "    for index, gaussian in enumerate(gaussians):\n",
    "        gaussian.mu = np.mean(data[partition_mapping == index])\n",
    "        gaussian.sigma = np.std(data[partition_mapping == index])\n",
    "        gaussian.alpha = len(data[partition_mapping == index]) / len(data)\n",
    "    ### END SOLUTION\n",
    "    return gaussians\n",
    "\n",
    "\n",
    "normpdfs_ = initialize_EM(np.linspace(-1, 1, 100), 2)\n",
    "assert len(normpdfs_) == 2, \"The number of initialized distributions is not correct.\"\n",
    "\n",
    "epsilon = 1e-8\n",
    "assert abs(1 - sum([normpdf.alpha for normpdf in normpdfs_])) < epsilon , \"Sum of all alphas is not 1.0!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af2ef6b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 3) Implement the expectation step\n",
    "\n",
    "Perform a soft classification of the data samples with the normal distributions. That means: Calculate the likelihood that a data sample $x_i$ belongs to distribution $j$ given parameters $\\mu_j$ and $\\sigma_j$. Or in other words, what is the likelihood of $x_i$ to be drawn from $N_j(\\mu_j, \\sigma_j)$? When you got the likelihood, weight the result by $\\alpha_j$.\n",
    "\n",
    "As a last step normalize the results such that the likelihoods of a data sample $x_i$ sum up to $1$.\n",
    "\n",
    "*Hint:* Store the data in a different array before you normalize it to not run into problems with partly normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b4844",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3b3_code",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def expectation_step(gaussians, data):\n",
    "    \"\"\"\n",
    "    Performs the expectation step of the EM.\n",
    "    \n",
    "    Args:\n",
    "        gaussians (list): A list of NormPDF objects.\n",
    "        data (array): The data vector.\n",
    "        \n",
    "    Returns:\n",
    "        An array of shape (len(data), len(gaussians))\n",
    "        which contains normalized likelihoods for each sample\n",
    "        to denote to which of the normal distributions it \n",
    "        most likely belongs to.\n",
    "    \"\"\"\n",
    "    expectation = None\n",
    "    ### BEGIN SOLUTION\n",
    "    # Calculates the likelihoods of the samples per \n",
    "    # distribution and weights the results by alpha.\n",
    "    tmp = np.empty((len(data), len(gaussians)))\n",
    "    for j, gaussian in enumerate(gaussians):\n",
    "        tmp[:,j] = gaussian.alpha * gaussian(data)\n",
    "        \n",
    "    # Normalize the results.\n",
    "    expectation = np.zeros_like(tmp)\n",
    "    for j, _ in enumerate(gaussians):\n",
    "        expectation[:,j] = tmp[:,j] / np.sum(tmp, 1)\n",
    "    tmp2 = tmp.T.sum(axis=1)\n",
    "    ### END SOLUTION\n",
    "    return expectation\n",
    "\n",
    "assert expectation_step([NormPDF(), NormPDF()], np.linspace(-2, 2, 100)).shape == (100, 2) , \"Shape is not correct!\"\n",
    "\n",
    "x = np.linspace(data.min(), data.max(), 2*len(data))\n",
    "normpdfs = initialize_EM(data, 2)\n",
    "expectations = expectation_step(normpdfs, x)\n",
    "\n",
    "import itertools\n",
    "plt.figure()\n",
    "colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y', 'k'])\n",
    "for pdf, expect, color in zip(normpdfs, expectations.T, colors):\n",
    "    plt.plot(x, pdf.alpha * pdf(x), color=color)\n",
    "    plt.plot(x, expect, color=color, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46436806",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Step 4) Implement the maximization step\n",
    "\n",
    "In the maximization step each $\\mu_j$, $\\sigma_j$ and $\\alpha_j$ is updated. First calculate the new means:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij}x_i$$\n",
    "\n",
    "That means $\\mu_j$ is the weighted mean of all samples, where the weight is their likelihood of belonging to distribution $j$.\n",
    "\n",
    "Then calculate the new $\\sigma_j$. Each new $\\sigma_j$ is the standard deviation of the normal distribution with the new $\\mu_j$, so for the calculation you already use the new $\\mu_j$:\n",
    "\n",
    "$$\\sigma_j = \\sqrt{ \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij} \\left(x_i - \\mu_j\\right)^2 }$$\n",
    "\n",
    "To calculate the new $\\alpha_j$ for each distribution, just take the mean of $p_j$ for each normal distribution $j$.\n",
    "\n",
    "**Caution:** For the next step it is necessary to know how much all $\\mu$ and $\\sigma$ changed. For that the function `maximization_step` should return a numpy array of those (absolute) changes. For example if $\\mu_0$ changed from 0.1 to 0.15, $\\sigma_0$ from 1 to 0.9, $\\mu_1$ from 0.5 to 0.6, and $\\sigma_1$, $\\mu_2$, and $\\sigma_2$ stayed the same, we expect the function to return `np.array([0.05, 0.1, 0.1, 0, 0, 0])` (however, the order is not important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1495d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3b4_code",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def maximization_step(gaussians, data, expectation):\n",
    "    \"\"\"\n",
    "    Performs the maximization step of the EM.\n",
    "    Modifies the gaussians by updating their mus and sigmas.\n",
    "    \n",
    "    Args:\n",
    "        gaussians (list): A list of NormPDF objects.\n",
    "        data (array): The data vector.\n",
    "        expectation (array): The expectation values for data element\n",
    "            (as computed by expectation_step()).\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of absolute changes in any mu or sigma, \n",
    "        that means the returned array has twice as many elements as\n",
    "        the supplied list of gaussians.\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    ### BEGIN SOLUTION\n",
    "    for j, N in enumerate(gaussians):\n",
    "        # calculate new parameters\n",
    "        # @ is the matrix multiplication (behaves like numpy.matlib.matmul)\n",
    "        mu = expectation[:,j] @ data / np.sum(expectation[:,j])\n",
    "        sigma = np.sqrt((expectation[:,j] @ (data - mu) ** 2) / np.sum(expectation[:,j]))\n",
    "        alpha = np.mean(expectation[:,j])\n",
    "        \n",
    "        # append relevant changes\n",
    "        changes += [np.abs(N.mu - mu)]\n",
    "        changes += [np.abs(N.sigma - sigma)]\n",
    "        \n",
    "        # update gaussian\n",
    "        N.mu = mu\n",
    "        N.sigma = sigma\n",
    "        N.alpha = alpha\n",
    "    ### END SOLUTION\n",
    "    return np.array(changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e04be8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**5) Perform the complete EM and plot your results:**\n",
    "\n",
    "Initialize three normal distributions whose parameters will be changed iteratively by the EM to converge close to the original distributions.\n",
    "\n",
    "Build a loop around the iterative procedure of expectation and maximization which stops when the changes in all $\\mu_j$ and $\\sigma_j$ are sufficiently small enough.\n",
    "\n",
    "Plot your results after each step and mark which data points belong to which normal distribution. If you don't get it to work, just plot your final solution.\n",
    "\n",
    "*Hint:* Remember to load the data and initialize the EM before the loop.\n",
    "\n",
    "*Hint:* A function `plot_intermediate_result` to plot your result after each step is already defined in the next cell. Take a look at what arguments it takes and try to use it in your loop.\n",
    "\n",
    "*Hint:* To plot your final result the first three images and corresponding code examples on the tutorial of [`plt.plot(...)`](http://matplotlib.org/users/pyplot_tutorial.html) should help you.\n",
    "\n",
    "*Optional:* Run the code multiple times. If your results are changing, use `np.random.seed(2)` in the beginning of the cell to get consistent results (any other integer will work as well, but 2 has some good results for the example solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568400b0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4e7335a369316ed0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "\n",
    "# Sets the random seed to a fix value to make results consistent\n",
    "np.random.seed(2)\n",
    "\n",
    "figure, axis = plt.subplots(1)\n",
    "axis.set_xlim(-5, 5)\n",
    "axis.set_ylim(-0.2, 4)\n",
    "axis.set_title('EM Animation')\n",
    "\n",
    "# Perform the initialization.\n",
    "data = load_data('em_normdistdata.txt')\n",
    "gaussians = initialize_EM(data, 3)\n",
    "\n",
    "def update_plot(gaussians, data, mapping):\n",
    "    \"\"\"\n",
    "    Gets a list of gaussians and data input. The mapping\n",
    "    parameter is a list of indices of gaussians. Each value\n",
    "    corresponds to the data value at the same position and \n",
    "    maps this data value to the proper gaussian.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y', 'k'])\n",
    "    plots = []\n",
    "    #container = plt.plot(x, gaussians[0](x), 'r', x, gaussians[1](x), 'g', x, gaussians[2](x), 'b')\n",
    "    for j, (N, color) in enumerate(zip(gaussians, colors)):\n",
    "        plots.append(plt.plot(x, gaussians[j](x), color)[0])\n",
    "        plots.append(plt.plot(data[mapping == j], [0] * len(data[mapping == j]), color, marker='x', markersize=5)[0])\n",
    "    return plots\n",
    "\n",
    "# Loop until the changes are small enough.\n",
    "eps = 0.05\n",
    "changes = [float('inf')] * 2\n",
    "artists = []  # for animation\n",
    "while max(changes) > eps:\n",
    "    # Iteratively apply the expectation step, followed by the maximization step.\n",
    "    ### BEGIN SOLUTION\n",
    "    expectation = expectation_step(gaussians, data)\n",
    "    changes = maximization_step(gaussians, data, expectation)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # Optional: Calculate the parameters to update the plot.\n",
    "    artists.append(update_plot(gaussians, data, np.argmax(expectation, 1)))\n",
    "\n",
    "ani = animation.ArtistAnimation(fig=figure, artists=artists, interval=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16ae36",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 4: Soft Clustering with Gaussian Mixture (6 points)\n",
    "\n",
    "In this assignment you will calculate the update rules for a Gaussian Mixture model required for the M-step of the EM algorithm. The Gaussian Mixture model can be used for soft clustering since it allows us to express varying degrees of certainty about the membership of individual samples. It is one of the most widely used models since Gaussian distributions generally have the property of fitting all different kinds of data reasonably well.\n",
    "\n",
    "A mixture model with $K$ components is in general of the form:\n",
    "\n",
    "$$ p(\\mathbf{x}|\\mathbf{\\theta}) = \\sum_{k=1}^K\\alpha_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)$$\n",
    "where $\\sum_{k=1}^K\\alpha_k = 1$.\n",
    "\n",
    "This means that the probability of observing a dataset $\\mathbf{x}$ given the parameter vector $\\mathbf{\\theta}$ can be expressed as the sum of $K$ individual distributions $p_k$ with parameters $\\mathbf{\\theta}_k \\subseteq {\\theta}$ which are weighted by respective class probabilities $\\alpha_k$. The probability for individual data $x_i \\in \\textbf{x}$ is calculated correspondingly (note however, that of course the values for individual data differs from the overall probability). \n",
    "\n",
    "We can now choose distributions for $p_k$ and $\\alpha_k$ and we get a whole collection full of different possible models, each of which has its own advantages and disadvantages (you can check <a href='https://en.wikipedia.org/wiki/Mixture_model'>Wikipedia</a> if you want an overview). The easiest case is where our mixing distributions are normally distributed, $p_k \\sim \\mathcal{N}(\\mu_k,\\sigma_k)$, and our latent class probabilities have a discrete distribution where we only have $\\alpha_k \\in [0,1]$ and the constraint $\\sum_k\\alpha_k=1$.\n",
    "\n",
    "If we were to randomly pick values for the parameter vector $\\theta$ then we would now have a generative model that can produce naturally clustered data for us, we would just have to sample $\\hat{x} \\sim p(\\mathbf{x}|\\mathbf{\\theta})$. However, we want to go into the opposite direction and figure out what the distribution of the labels given the data is. This can be calculated easily by Bayes' Theorem for each model $k$, where the (latent) probability for choosing model $k$ is $p(k|\\mathbf{\\theta}_k) = \\alpha_k$:\n",
    "\n",
    "$$p(k|\\mathbf{x},\\mathbf{\\theta})=\\frac{p(k|\\mathbf{\\theta}_k)p(\\mathbf{x}|k,\\mathbf{\\theta}_k)}{\\sum_{k'=1}^Kp(k'|\\mathbf{\\theta}_{k'})p(\\mathbf{x}|k',\\mathbf{\\theta}_{k'})} = \\frac{\\alpha_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)}{\\sum_{k'=1}^K\\alpha_{k'}p_{k'}(\\mathbf{x}|\\mathbf{\\theta}_{k'})}$$\n",
    "\n",
    "That sounds good enough, but where do we actually start now? We have a mathematical framework pinned down, but it contains many variables and it is not *a priori* obvious how we can figure out the best values for them. We *have* some data which we want to use to determine the parameters so the usual approach would be to simply calculate a Maximum Likelihood Estimator (MLE) or an Maximum A Posteriori Estimator (MAP) by maximizing the above formulas over the possible parameters with a method like Gradient Descent. It turns out however that this is very, very hard to do (optimal MLE for a GMM is NP-hard (Aloise et al. 2009; Drineas et al. 2004)) since the $\\alpha_k$ and the $\\theta_k$ are strongly interdependent and neither is known. It *can* still be done with some work-arounds, but there is also an alternative path that we can go down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dee4be",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4_background",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "*(The following exhibition is only for those who are interested in the mathematical background of the EM-algorithm, those who only want to solve the exercise can skip ahead to the function that you have to maximize.)*\n",
    "\n",
    "We want to maximize the log likelihood given as\n",
    "$$\\mathcal{\\ell}(\\mathbf{\\theta})=\\sum_{i=1}^N\\log p(x_i|\\mathbf{\\theta}) = \\sum_{i=1}^N\\log\\left[\\sum_{k=1}^Kp_k(x_i|\\mathbf{\\theta}_k)\\right].$$\n",
    "All the problems occur because we have a sum inside the logarithm and so we can't pull the logarithm further in towards the densitiy and that is what makes the problem so hard. If we just *ignore* the inner sum we get an expression\n",
    "$$\\mathcal{\\ell}_c(\\mathbf{\\theta}) = \\sum_{i=1}^N\\log p_k(x_i|\\mathbf{\\theta}_k)$$\n",
    "which would be much nicer to compute. But now we have a free floating $k$ in the subscript of our density! Which one of the mixing distributions are we talking about here? Kind of all of them at once. But we need one quantity to represent all the distributions. So to get rid of the $k$ we take the expected value with respect to the latent variable $k$ and receive a function that only depends on $\\mathbf{\\theta}$:\n",
    "$$Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) = \\mathbb{E}\\left[\\mathcal{\\ell}_c\\left(\\mathbf{\\theta}\\middle|\\mathcal{\\theta}^{t-1}\\right)\\right]$$\n",
    "\n",
    "Calculating this $Q$ function can be difficult - but at least we only have to do it once instead of solving an NP-hard optimization problem every time we have a new dataset. We will only provide you the final formula, you will have to trust us on this one:\n",
    "\n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log\\alpha_k + \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "This still looks nasty but it really isn't that bad! Since $\\theta^{t-1}$ is known at time $t$ we can calculate $p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)$ with Bayes' Theorem as stated above and replace these expressions with constants $r_{i,k}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96583afc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4_explanation",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**This is where your work begins:**\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$\n",
    "In the lecture you saw a proof that if we choose\n",
    "$$\\mathbf{\\theta}^t = \\argmax_{\\mathbf{\\theta}} Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$$\n",
    "that the likelihood of the parameter is non-decreasing then. So we want to maximize $Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$ for the parameters $\\left(\\alpha_1\\dots,\\alpha_K\\right)$ and $\\theta = \\left(\\mu_1,\\dots,\\mu_K,\\sigma_1,\\dots,\\sigma_K\\right)$. So your job is to take the derivative of \n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k r_{i,k}\\log\\alpha_k + \\sum_i\\sum_k r_{i,k}\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "with respect to these variables, to set it equal to 0 and to solve for the value that you are currently maximizing for. You only have to do this for the one dimensional case, i.e. \n",
    "$$p_k(x_i|\\mathbf{\\theta}_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566b08b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4_b_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## a) Calculate the maximizer for the $\\mu_k$:\n",
    "\n",
    "*Hint:* derive $Q(\\theta,\\theta^{t-1})$ with respect $\\mu_{k}$ and set the derivative to $0$ to get an optimal value for $\\mu_k$.  You do not need to form the second derivative to check for that being a maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5ebfe",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "4_b_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$\\begin{align} \n",
    "\\frac{\\partial}{\\partial \\mu_k}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\frac{\\partial}{\\partial \\mu_k}\\sum_i\\sum_k r_{i,k}\\log p_k(\\mathbf{x}_i|\\mathbf{\\theta}) \\\\\n",
    "&= \\sum_i r_{i,k} \\frac{\\partial}{\\partial \\mu_k} \\left(-\\frac{1}{2}\\log\\left(2\\pi\\sigma_k^2\\right) - \\frac{1}{2\\sigma_k^2}\\left(x_i-\\mu_k\\right)^2\\right)\n",
    "\\\\\n",
    "&= \\frac{1}{\\sigma_k^2}\\sum_i r_{i,k}(x_i-\\mu_k) \\stackrel{!}{=} 0 \\\\\n",
    "\\Leftrightarrow \\sum_i r_{i,k}x_i &= \\mu_k\\sum_ir_{i,k} \\\\\n",
    "\\Leftrightarrow \\mu_k &= \\frac{\\sum_i r_{i,k}x_i}{\\sum_ir_{i,k}} \n",
    "\\end{align} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7656ef5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4_c_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## b) Calculate the maximizer for the $\\sigma_k^2$:\n",
    "\n",
    "*Hint:* now derive $Q(\\theta,\\theta^{t-1})$ with respect $\\sigma_{k}^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8bd56",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "4_c_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\sigma_k^2}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i r_{i,k} \\frac{\\partial}{\\partial \\sigma_k^2} \\left(-\\frac{1}{2}\\log\\left(2\\pi\\sigma_k^2\\right) - \\frac{1}{2\\sigma_k^2}\\left(x_i-\\mu_k\\right)^2\\right) \\\\\n",
    "&= \\sum_i r_{i,k}\\left( -\\frac{1}{\\sigma_k^2} + \\frac{1}{\\sigma_k^4}(x_i-\\mu_k)^2\\right) \\stackrel{!}{=} 0\\\\\n",
    "\\Leftrightarrow \\sum_i r_{i,k}(x_i-\\mu_k)^2 &= \\sigma_k^2\\sum_ir_{i,k} \\\\\n",
    "\\Leftrightarrow \\sigma_k^2 &= \\frac{\\sum_i r_{i,k}(x_i-\\mu_k)^2}{\\sum_ir_{i,k}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914f9ec",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## c) Calculate the maximizer for the $\\alpha_k$ \n",
    "\n",
    "You need the ensure $\\sum_k\\alpha_k =1$. You can either use the formula to express one of the $\\alpha_i$ in terms of all the others or use a Lagrangian Multiplier for this.\n",
    "\n",
    "Hint: using a Lagrangian Multiplier, you would start with an equation like this:\n",
    "$$\\frac{\\partial}{\\partial \\alpha_k}\\left(Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) - \\lambda \\left(\\sum_k \\alpha_k - 1\\right)\\right) = 0$$\n",
    "Some brief introduction to Langrangian multipliers can be found in Bishop (2006): *Pattern Recognition and Machine Learning*, Appendix E."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f0f5d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "4_a_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$\\begin{align} \n",
    "\\frac{\\partial}{\\partial \\alpha_k}\\left(Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) - \\lambda \\left(\\sum_k \\alpha_k - 1\\right)\\right) &= \\sum_i \\frac{r_{i,k}}{\\alpha_k} - \\lambda = 0 \\Leftrightarrow \\sum_i \\frac{r_{i,k}}{\\lambda} = \\alpha_k \\\\\n",
    "\\frac{\\partial}{\\partial \\lambda}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) + \\lambda \\left(\\sum_k \\alpha_k - 1\\right) &= \\sum_k \\alpha_k - 1 \\stackrel{!}{=} 0 \\Leftrightarrow \\sum_k \\alpha_k = 1 \\\\\n",
    "\\Rightarrow \\frac{1}{\\lambda}\\sum_k\\sum_i r_{i,k} &= 1 \\\\\n",
    "\\Rightarrow \\alpha_k &= \\frac{1}{N}\\sum_i r_{i,k}\n",
    "\\end{align}$$\n",
    "\n",
    "Because of popular request: If we don't want to use the Lagrangian the notation becomes a bit more cumbersome but the overall strategy remains the same.\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\sum_{k=1}^K\\alpha_k = 1 &\\Leftrightarrow \\alpha_K = 1 - \\sum_{k=1}^{K-1}\\alpha_k \\\\\n",
    "\\frac{\\partial}{\\partial \\alpha_k}Q &= \\frac{\\partial}{\\partial \\alpha_k}\\sum_{i=1}^N \\left(\\sum_{k=1}^{K-1}r_{ik}\\log\\alpha_k + r_{ik}\\log(1-\\sum_{k=1}^{K-1}\\alpha_k)\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(\\frac{r_{ik}}{\\alpha_k} - \\frac{r_{ik}}{1-\\sum_{k=1}^{K-1}\\alpha_k}\\right) \\stackrel{!}{=} 0 \\\\\n",
    "\\Leftrightarrow \\alpha_k &= \\left(\\sum_{i=1}^{N}r_{ik}\\right)\\frac{\\left(1 - \\sum_{k=1}^{K-1}\\alpha_k\\right)}{\\sum_{i-1}^Nr_{iK}} \\\\\n",
    "\\stackrel{\\text{sum over k}}{\\Rightarrow} \\frac{\\left(1 - \\sum_{k=1}^{K-1}\\alpha_k\\right)}{\\sum_{i-1}^Nr_{iK}} \\sum_{k=1}^K\\sum_{i=1}^Nr_{ik} &= 1 \\\\\n",
    "\\Leftrightarrow \\frac{\\left(1 - \\sum_{k=1}^{K-1}\\alpha_k\\right)}{\\sum_{i-1}^Nr_{iK}} &= \\frac{1}{N}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
