{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2b7a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e3fef",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9802371e834eb85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Sheet 03: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efa6fe",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f790e30691d4d590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 5th, 2022**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f5a81",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ceaa7378e4a713d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 0: Math recap (Eigenvectors and Eigenvalues) [0 Points]\n",
    "\n",
    "This exercise does not give any points, and is voluntary. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Eigenvectors and eigenvalues may be less familiar, so this may be a good time to look them up again (you will only need the basic concepts, you do not have to know how to actually compute them for this class). You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259ff49",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-106b918b6f9c6fea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## a) Eigenvectors and eigenvalues\n",
    "\n",
    "What is an eigenvector of a matrix/a linear mapping? What are eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c111970",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f80e2cfbc5dae96a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Mapping (i.e. multiplying with the matrix) an eigenvector will result a scaled version of that vector, i.e. it may change length and orientation, but not its direction in space. Formally, an eigenvector $v$ fulfills\n",
    "$$A\\cdot v=\\lambda v\\qquad\\text{for some scalar $\\lambda\\in\\mathbb{R}$}$$\n",
    "The scalar $\\lambda$ is called the eigenvalue belonging to the eigenvector $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569c688",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10c6f038150609e1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## b) Characteristic polynomial\n",
    "\n",
    "What is the characteristic polynomial of a matrix? How is it related to eigenvalues? What are algebraic and geometric multiplicity of an eigenvalue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c10e78",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c45db6ae30a5507a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The characteristic polynomial of a $n\\times n$-matrix $A$ is defined as\n",
    "$$p_A(X) = \\det(X\\cdot\\mathbf{I}_n-A)$$\n",
    "where $\\mathbf{I}_n$ is the identity matrix and $\\det$ the determinant. It is a polynomial of degree $n$, that is invariant under matrix similarity. The roots of the characteristic polynomial are the eigenvalues of the matrix.\n",
    "The algebraic multiplicity of an eigenvalue $\\lambda$ is its multiplicity as a root of the characteristic polynomial.\n",
    "\n",
    "For every eigenvalue, there may be multiple eigenvectors, that span a subspace called the eigenspace for that eigenvector. The geometric multiplicity of an eigenvalue is the dimension of the corresponding eigenspace. The geometric multiplicity cannot exceed the algebraic multiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf673a8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7822385798587c45",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## c) Spectrum\n",
    "\n",
    "What is the spectrum of a matrix? What does the spectral theorem state?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff481b9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1a5e17baf68e02e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The spectrum of a matrix is the set of its eigenvalues. The spectral theorem states under which conditions there is diagonalization and provides a cannonical decomposition, referred to es eigendecomposition. For example every real symmetric square matrix is diagonalizable. The diagonalization $A=VDV^T$ consists of a diagonal matrix having the eigenvalues in the diagonal and the matrix $V$ has the corresponding eigenvectors as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abe3d6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-eigen-q4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## d) Numpy/Scipy [bonus task]\n",
    "\n",
    "Numpy/Scipy provide functions to compute eigenvalues. Lookup these functions and apply them to an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39f246",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2400c166c6ecc90d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7273560d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "pnorm",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: p-norm (6 points)\n",
    "\n",
    "A very well known norm is the euclidean norm. However, it is not the only norm: It is in fact just one of many p-norms where $p = 2$. In this assignment you will take a look at other p-norms and see how they behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a146bba",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(a)** Implement a function `pnorm` which expects a vector $x \\in \\mathcal{R}^n$ and a scalar $p \\geq 1, p \\in \\mathcal{R}$ and returns the p-norm of $x$ which is defined as:\n",
    "\n",
    "$$||x||_p = \\left(\\sum\\limits_{i=1}^n |x_i|^p \\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "*Note:* Even though the norm is only defined for $p \\geq 1$, values $0 < p < 1$ are still interesting. In that case we can not talk about a norm anymore, as the triangle inequality ($||a|| + ||b|| \\geq ||a + b||$) does not hold. We will still take a look at some of these values, so your function should handle them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233945e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2a_code",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pnorm(x, p):\n",
    "    \"\"\"\n",
    "    Calculates the p-norm of x.\n",
    "    \n",
    "    Args:\n",
    "        x (array): the vector for which the norm is to be computed.\n",
    "        p (float): the p-value (a positive real number).\n",
    "        \n",
    "    Returns:\n",
    "        The p-norm of x.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    ### BEGIN SOLUTION\n",
    "    # If p is not valid, raise an error:\n",
    "    if p <= 0:\n",
    "        raise ValueError('p has to be > 0!')\n",
    "    result = np.sum(np.abs(x) ** p) ** (1 / p)\n",
    "    ### END SOLUTION\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34bcf1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "2a_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Epsilon: Account for rounding erros\n",
    "epsilon = 1e-8\n",
    "assert abs(pnorm(1, 2)      - 1          ) < epsilon, \"pnorm is incorrect for x = 1, p = 2\"\n",
    "assert abs(pnorm(2, 2)      - 2          ) < epsilon, \"pnorm is incorrect for x = 2, p = 2\"\n",
    "assert abs(pnorm([2, 1], 2) - np.sqrt(5) ) < epsilon, \"pnorm is incorrect for x = [2, 1], p = 2\" \n",
    "assert abs(pnorm(2, 0.5)    - 2          ) < epsilon, \"pnorm is incorrect for x = 2, p = 0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b87d1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(b)** Implement another function `pdist` which expects two vectors $x_0 \\in \\mathcal{R}^n, x_1 \\in \\mathcal{R}^n$ and a scalar $p \\geq 1, p \\in \\mathcal{R}$ and returns the distance between $x_0$ and $x_1$ on the p-norm defined by $p$. Again handle $0 < p < 1$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9f3f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2b_code",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pdist(x0, x1, p):\n",
    "    \"\"\"\n",
    "    Calculates the distance between x0 and x1\n",
    "    using the p-norm.\n",
    "    \n",
    "    Arguments:\n",
    "        x0 (array): the first vector.\n",
    "        x1 (array): the second vector.\n",
    "        p (float): the p-value (a positive real number).\n",
    "        \n",
    "    Returns:\n",
    "        The p-distance between x0 and x1.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    ### BEGIN SOLUTION\n",
    "    result = pnorm(np.array(x0) - np.array(x1), p)\n",
    "    ### END SOLUTION\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec6e80",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "2b_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Epsilon: Account for rounding erros\n",
    "epsilon = 1e-8\n",
    "assert abs(pdist(1, 2, 2)           - 1          ) < epsilon , \"pdist is incorrect for x0 = 1, x1 = 2, p = 2\"\n",
    "assert abs(pdist(2, 5, 2)           - 3          ) < epsilon , \"pdist is incorrect for x0 = 2, x1 = 5, p = 2\"\n",
    "assert abs(pdist([2, 1], [1, 2], 2) - np.sqrt(2) ) < epsilon , \"pdist is incorrect for x0 = [2, 1], x1 = [1, 2], p = 2\" \n",
    "assert abs(pdist([2, 1], [0, 0], 2) - np.sqrt(5) ) < epsilon , \"pdist is incorrect for x0 = [2, 1], x1 = [0, 0], p = 2\" \n",
    "assert abs(pdist(2, 0, 0.5)         - 2          ) < epsilon , \"pdist is incorrect for x0 = 2, x1 = 0, p = 0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54298e5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(c)** Now we will compare some different p-norms. Below is part of a code to plot data in nice scatter plots. \n",
    "\n",
    "Your task is to calculate the data to plot. The variable `data` is currently simply filled with zeros. Instead, fill it as follows:\n",
    "\n",
    "- Use the function `np.linspace()` to create a vector of `50` evenly distributed values between `-100` and `100` (inclusively).\n",
    "- Fill `data`: Data is basically the cartesian product of the vector you created before with itself filled up with each value's norm. It should have 2500 rows. Each of the 2500 rows should contain `[x, y, d]`, where `x` is the x coordinate and `y` the y coordinate of a point, and `d` the p-norm of `(x, y)`. Use either `pnorm` or `pdist` to calculate `d`.\n",
    "- Normalize the data in `data[:,2]` (i.e. all d-values) so that they are between 0 and 1.\n",
    "\n",
    "Run your code and take a look at your results. Darker colors mean that a value is further away from the center (0, 0) according to the p-norm used.\n",
    "\n",
    "*Hint:* To give you an idea of how `data` should look like, here is an example for three evenly distributed values between `-1` and `1` and a p-norm with `p = 2`.\n",
    "\n",
    "Before normalization of the d-column:\n",
    "\n",
    "```python\n",
    "data = np.array([[-1.         -1.          1.41421356]\n",
    "                 [-1.          0.          1.        ]\n",
    "                 [-1.          1.          1.41421356]\n",
    "                 [ 0.         -1.          1.        ]\n",
    "                 [ 0.          0.          0.        ]\n",
    "                 [ 0.          1.          1.        ]\n",
    "                 [ 1.         -1.          1.41421356]\n",
    "                 [ 1.          0.          1.        ]\n",
    "                 [ 1.          1.          1.41421356]])\n",
    "```\n",
    "\n",
    "After normalization of the d-column:\n",
    "\n",
    "```python\n",
    "data = np.array([[-1.         -1.          1.        ]\n",
    "                 [-1.          0.          0.70710678]\n",
    "                 [-1.          1.          1.        ]\n",
    "                 [ 0.         -1.          0.70710678]\n",
    "                 [ 0.          0.          0.        ]\n",
    "                 [ 0.          1.          0.70710678]\n",
    "                 [ 1.         -1.          1.        ]\n",
    "                 [ 1.          0.          0.70710678]\n",
    "                 [ 1.          1.          1.        ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d232be",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3c_code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "color = ColorConverter()\n",
    "figure_norms = plt.figure('p-norm comparison')\n",
    "\n",
    "# create the linspace vector\n",
    "### BEGIN SOLUTION\n",
    "ls = np.linspace(-100, 100)\n",
    "### END SOLUTION\n",
    "\n",
    "assert len(ls) == 50 , 'ls should be of length 50.'\n",
    "assert (min(ls), max(ls)) == (-100, 100) , 'ls should range from -100 to 100, inclusively.'\n",
    "\n",
    "for i, p in enumerate([1/8, 1/4, 1/2, 1, 1.5, 2, 4, 8, 128]):\n",
    "    # Create a numpy array containing useful values instead of zeros.\n",
    "    data = np.zeros((2500, 3))\n",
    "    ### BEGIN SOLUTION\n",
    "    data = np.array([[x, y, pnorm((x, y), p)] for x in ls for y in ls])\n",
    "    data[:,2] = data[:,2] / np.max(data[:,2])\n",
    "    ### END SOLUTION\n",
    "\n",
    "    assert data[100,2]>0.9 and data[100,2]<1, \"Wrong result for p norm, make sure you use NORM and not pdist!\"\n",
    "    assert all(data[:,2] <= 1), 'The third column should be normalized.'\n",
    "\n",
    "    # Plot the data.\n",
    "    colors = [color.to_rgb((1, 1-a, 1-a)) for a in data[:,2]]\n",
    "    a = plt.subplot(3, 3, i + 1)\n",
    "    plt.scatter(data[:,0], data[:,1], marker='.', color=colors)\n",
    "    a.set_ylim([-100, 100])\n",
    "    a.set_xlim([-100, 100])\n",
    "    a.set_title('{:.3g}-norm'.format(p))\n",
    "    a.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    figure_norms.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac797ab1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "pnorm-d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(d)** In the first parts of this exercise we have used the fact that every $p$-norm induces an associated metric by setting $d_p(x,y):=\\|x-y\\|_p$ for $x,y\\in\\mathcal{R}^{n}$.  Show that for $p\\ge 1$ this function $d_p$ indeed fulfills the conditions listed on ML-04 slide 39. What problems occur for $p<1$?\n",
    "\n",
    "Hint: start with a specific case, e.g. the Euclidean metric $p=2$ for a low dimension $n$ and then generalize your arguments to other values of $n$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f356058",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "p-norm-d-answer",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "First show symmetry: this is straight forward, just spelling out the definition:\n",
    "$$d(x,y) = \\|x-y\\| \n",
    "= \\left(\\sum\\limits_{i=1}^n |x_i-y_i|^p \\right)^{\\frac{1}{p}}\n",
    "= \\left(\\sum\\limits_{i=1}^n |y_i-x_i|^p \\right)^{\\frac{1}{p}}\n",
    "= \\|y-x\\| = d(y,x)$$\n",
    "Here we have used the fact that $|a-b|=|-(b-a)| = |b-a|$ for\n",
    "all real numbers $a,b$.\n",
    "\n",
    "The coincidence axiom can be seen as follows: Assume that\n",
    "$$0 = d(x,y) = \\|x-y\\| = \\left(\\sum\\limits_{i=1}^n |x_i-y_i|^p \\right)^{\\frac{1}{p}}$$\n",
    "We use the fact that $a^{\\frac{1}{p}}$ can only be $0$ if $a=0$. This implies that the sum $\\sum\\limits_{i=1}^n |x_i-y_i|^p$ has to be $0$.\n",
    "We know that $|b|\\ge 0$ for all real numbers $b$ and hence $|x_i-y_i|\\geq 0$ for all $i=1,\\ldots,n$ and further $|x_i-y_i|^p\\geq 0$, i.e., none of the summands can be negative.  Hence all have to be $0$, that is $x_i=y_i$ for all $i=1,\\ldots,n$. This means $x=y$.\n",
    "\n",
    "Finally, consider the triangle inequality: Take three points $x,y,z\\in\\mathcal{R}^{n}$:\n",
    "Assuming that we know that $\\|\\cdot\\|_p$ is a norm, i.e., that it fulfills the triangle inequality for a norm $\\|a+b\\|_p\\leq \\|a\\|_p+\\|b\\|_p$, then we can derive the triangle inequality for the derived metric as\n",
    "$$d_p(x,y) = \\|(x-y)\\|_p = \\|(x-z) + (z-y)\\|_p\\leq = \\|x-z\\|_p + \\|z-y\\|_p = d_p(x,z) + d_p(z,y)$$\n",
    "Showing that the norm $\\|\\cdot\\|_p$ in fact obeys the triangle inequality is a bit more challenging (this is known as [Minkowski inequality](https://en.wikipedia.org/wiki/Minkowski_inequality)): for the Euclidean norm ($p=2$) one can for example used the [Cauchy-Schwartz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality) ($\\langle x,y\\rangle\\leq \\|x\\|_2\\|y\\|_2$) and then derive:\n",
    "\\begin{align}\n",
    "  \\|x+y\\|_2^2 &= \\sum_{i=1}^n(x_i+y_i)^2 = \\sum_{i=1}^n(x_i^2+2x_iy_i+y_i^2) = \n",
    "\\sum_{i=1}^n x_i^2+2\\sum_{i=1}^n x_iy_i+\\sum_{i=1}^n y_i^2 \\\\\n",
    "& = \n",
    "\\|x\\|_2^2+2\\langle x,y\\rangle + \\|y\\|_2^2 \\leq\n",
    "\\|x\\|_2^2+2\\|x\\|_2\\|y_i\\|_2 + \\|y\\|_2^2 = (\\|x\\|_2 + \\|y\\|_2)^2\n",
    "\\end{align}\n",
    "For other $p\\geq 1$ we will assume that $\\|x\\|_p+\\|y\\|_p=1$ and show\n",
    "$\\|x+y\\|_p\\leq 1$ (the general case follows then by the homogeneity of the norm). Set $\\lambda:=\\|y\\|_p$ and hence $\\|x\\|=1$, and define $u=x/(1-\\lambda)$ and $v=y/\\lambda$ (this implies $\\|u\\|_p=1=\\|v\\|_p$. We will show\n",
    "$\\|(1-\\lambda)u+\\lambda v\\|_p\\leq 1$. For this we use the [convexity](https://en.wikipedia.org/wiki/Convex_function) of the map $z\\mapsto |z|^p$ for $p\\ge1$, that is the fact that $|tz_1+(1-t)z_2|^p\\leq t|z_1|^p+(1-t)|z_2|^p$ for all $z_1,z_2\\in\\mathbb{R}$ and $t\\in[0,1]$. For each component $i=1,\\ldots,n$ it hence holds\n",
    "$$|(1-\\lambda) u_i+\\lambda v_i|^p \\leq (1-\\lambda)|u_i|^p + \\lambda|v_i|^p$$\n",
    "Summing up all components we get\n",
    "\\begin{align}\n",
    "  \\left(\\|x+y\\|_p\\right)^p & = \\sum_{i=1}^n |x_i+y_i|^p = \\sum_{i=1}^n |(1-\\lambda)u_i +\\lambda+v_i|^p \\\\\n",
    "  & \\leq \\sum_{i=1}^n(1-\\lambda)|u_i|^p + \\lambda|v_i|^p\n",
    "  = (1-\\lambda)\\sum_{i=1}^n|u_i|^p + \\lambda\\sum_{i=1}^n|y_i|^p \\\\\n",
    "  &= (1-\\lambda)(\\|u\\|_p)^p + \\lambda(\\|v\\|_p)^p\n",
    "  = (1-\\lambda)\\cdot 1^p + \\lambda \\cdot 1^p = (1-\\lambda) + \\lambda \\\\\n",
    "  &= 1\n",
    "\\end{align}\n",
    "To show the general case of the triangle inequaltity (for arbitrary vectors $x$ and $y$, without the norm restriction from above) we use the homogeneity of the $p$-norm, that is the fact that for each real number $\\alpha$ it holds that $\\|\\alpha\\cdot x\\|_p=|\\alpha|\\cdot\\|x\\|_p$ (this follows directly as\n",
    "$\\sum |\\alpha x_i|^p = \\sum |\\alpha|^p\\cdot|x_i|^p) = |\\alpha|^p(\\sum |x_i|^p)$). \n",
    "We set  $\\kappa=\\|x\\|_p+\\|y\\|_p$, so that $x/\\kappa$ and $y/\\kappa$ are of the desired form (that is $\\|x/\\kappa\\|_p + \\|y/\\kappa\\|_p=1$). Then it follows that\n",
    "$$\\left\\|x+y\\right\\|_p \n",
    "= \\kappa\\left\\|\\frac{x}{\\kappa}+\\frac{y}{\\kappa}\\right\\|\n",
    "\\leq \\kappa\\left(\\left\\|\\frac{x}{\\kappa}\\right\\|_p+\\left\\|\\frac{y}{\\kappa}\\right\\|_p\\right)\n",
    "=\\|x\\|_p+\\|y\\|_p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1b26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec2f4c56",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: Distance Measures for Clusters (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892fb84",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1_ax",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## a) Point and cluster distances\n",
    "\n",
    "Explain the difference of point and cluster distances and their relation to each other. Give examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6fccf",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-64b9b3273be847ac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Clusters are collections of points. The distance of two clusters can be derived from the distance of the points they contain. This requires that distances between points can be measured, i.e., the points are from some metric space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5211b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efa27e859e56e6dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b) Mean and centroid distance\n",
    "\n",
    "* Describe how the cluster metrics *mean distance* and *centroid distance* work.\n",
    "* What formal requirements do they have?\n",
    "* What is their computational complexity (use the [Big O notation](https://en.wikipedia.org/wiki/Big_O_notation))? \n",
    "* Give a numerical example of clusters (with cluster size at least 2), where they lead to (a) the same result and (b) different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8feb8e0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d8619b5b4e63d573",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The $D_{\\text{centroid}}$ first computes the center of each cluster and then computes the distance of these centers. Hence it can only be applied to clusters that allow to compute such a center (e.g. clusters of points in a euclidean space), while the $D_{\\text{mean}}$ metric can be applied to clusters of points in a general metric space (where it is possible to measure the distance of datapoints, but not necessarily to obtain a cluster center). For clusters $X$ and $Y$ the following holds:\n",
    "\n",
    "* $D_{\\text{mean}}$ has complexity $O(|X|\\cdot|Y|)$: Compute the sum of the distances of all pairs from $(x,y)$ There exist $|X|\\cdot|Y|$ such pairs (actually you only need half of it, as the distance is symmetric, but this does not change the order of complexity). So you have to compute $O(|X|\\cdot|Y|)$ distances (the complexity of this operation depends on the distance measure applied) and add up the $O(|X|\\cdot|Y|)$ values.\n",
    "* $D_{\\text{centroid}}$ has complexity $O(|X|+|Y|)$: compute the mean of each cluster, which amounts to summing up $|X|$ vectors for cluster $X$ and $|Y|$ vectors for cluster $Y$ - again the complexity of summing vectors depends on the type of vectorspace you consider, it will usually depend directly on its dimensionality.\n",
    "\n",
    "\n",
    "\n",
    "Numerical example (in the euclidean space $\\mathbb{R}^2$): Cluster $X=\\{\\begin{pmatrix}1\\\\1\\end{pmatrix}, \\begin{pmatrix}1\\\\4\\end{pmatrix}\\}$, and cluster $Y=\\{\\begin{pmatrix}5\\\\1\\end{pmatrix},\\begin{pmatrix}5\\\\4\\end{pmatrix}\\}$.\n",
    "\n",
    "* (a) Euclidean norm:\n",
    "    * Centroid distance\n",
    "    \n",
    "        The centroid of $X$ is \n",
    "        $$\\bar{X} = \\frac{1}{2}\\sum_{x\\in X}x = \\frac{1}{2}\\left[\\begin{pmatrix}1\\\\1\\end{pmatrix} + \\begin{pmatrix}1\\\\4\\end{pmatrix}\\right] = \\frac{1}{2}\\begin{pmatrix}2\\\\5\\end{pmatrix} = \\begin{pmatrix}1\\\\2.5\\end{pmatrix}$$\n",
    "        and the centroid of $Y$ is \n",
    "        $$\\bar{Y}=\\frac{1}{2}\\sum_{y\\in Y}y= \\frac{1}{2}\\left[\\begin{pmatrix}5\\\\1\\end{pmatrix} + \\begin{pmatrix}5\\\\4\\end{pmatrix}\\right] = \\frac{1}{2}\\begin{pmatrix}10\\\\5\\end{pmatrix} = \\begin{pmatrix}5\\\\2.5\\end{pmatrix}$$\n",
    "\n",
    "        Hence $$D_{\\text{centroid}}(X,Y) = \\left\\|\\begin{pmatrix}1\\\\2.5\\end{pmatrix}-\\begin{pmatrix}5\\\\2.5\\end{pmatrix}\\right\\| = \\left\\|\\begin{pmatrix}-4\\\\ 0\\end{pmatrix}\\right\\| = 4$$\n",
    "\n",
    "    * Mean distance\n",
    "\n",
    "        $$D_{\\text{mean}}(X,Y) = \\frac{1}{2\\cdot 2}\\left[d\\left(\\begin{pmatrix}1\\\\1\\end{pmatrix},\\begin{pmatrix}5\\\\1\\end{pmatrix}\\right) + d\\left(\\begin{pmatrix}1\\\\1\\end{pmatrix},\\begin{pmatrix}5\\\\4\\end{pmatrix}\\right) + d\\left(\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}5\\\\4\\end{pmatrix}\\right) + d\\left(\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}5\\\\4\\end{pmatrix}\\right)\\right]=\\frac{1}{4}\\left[4 + 5 + 4 + 5\\right] = 4.5$$\n",
    "\n",
    "    * So here $D_{\\text{centroid}}(X,Y) \\neq D_{\\text{mean}}(X,Y)$\n",
    "\n",
    "\n",
    "* (b) Take the same points but now use the maximum norm instead of the Euclidean norm (alternatively, you could also use a new dataset here).\n",
    "\n",
    "    * Centroid distance\n",
    "\n",
    "        The centroids stay the same (they are independent of the underlying norm) and the centroid distance is\n",
    "        $$D_{\\text{centroid}}(X,Y) = d_{\\infty}\\left(\\begin{pmatrix}5\\\\2.5\\end{pmatrix}, \\begin{pmatrix}1\\\\2.5\\end{pmatrix}\\right) = \\max\\{|5-1|, |2.5-2.5|\\} = \\max\\{4,0\\} = 4$$\n",
    "\n",
    "    * Mean distance\n",
    "\n",
    "        $$D_{\\text{mean}}(X,Y) = \\frac{1}{2\\cdot 2}\\left[d_{\\infty}\\left(\\begin{pmatrix}1\\\\1\\end{pmatrix},\\begin{pmatrix}5\\\\1\\end{pmatrix}\\right) + d_{\\infty}\\left(\\begin{pmatrix}1\\\\1\\end{pmatrix},\\begin{pmatrix}5\\\\4\\end{pmatrix}\\right) + d_{\\infty}\\left(\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}5\\\\4\\end{pmatrix}\\right) + d_{\\infty}\\left(\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}5\\\\4\\end{pmatrix}\\right)\\right]=\\frac{1}{4}\\left[4 + 4 + 4 + 4\\right] = 4$$\n",
    "\n",
    "    * So now $D_{\\text{centroid}}(X,Y) = D_{\\text{mean}}(X,Y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed8b42",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## c) Implemention of  mean and centroid distance\n",
    "\n",
    "Now implement the $d_{mean}$ and $d_{centroid}$ distance from the lecture. Each function expects two clusters each represented by a 2-dimensional numpy array, where the number of columns $n$ reflects the dimensionality of the data space and has to agree for both clusters, while the number of rows $mx$ and $my$ can vary from cluster to cluster. The return value is the respective distance.  Use the Euclidean distance as underlying metric.\n",
    "\n",
    "Hint: you may consider using the function `scipy.spatial.distance.cdist`. Consult the documentation to find out how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6c080",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1_b_code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "def d_mean(cluster1, cluster2):\n",
    "    \"\"\"\n",
    "    Mean distance between points of two clusters.\n",
    "   \n",
    "    Args:\n",
    "        cluster1 (ndarray): Points belonging to cluster 1 of shape (num_points, num_dimensions).\n",
    "        cluster2 (ndarray): Points belonging to cluster 1 of shape (num_points, num_dimensions).\n",
    "    \n",
    "    Returns:\n",
    "        float: The mean distance between the points in the two clusters.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return cdist(cluster1, cluster2).mean()\n",
    "    ### END SOLUTION\n",
    "\n",
    "x = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "y = np.array([[13,14,15], [16,17,18], [19,20,21], [5,45,1], [1,12,7]])\n",
    "\n",
    "epsilon = 1e-3\n",
    "assert abs(d_mean(x, y) - 22.297) < epsilon, \"Result is not correct: {}\".format(d_mean(x, y))\n",
    "assert d_mean(x, y) == d_mean(y, x), \"X,Y is not equal to Y,X: {} != {}\".format(d_mean(x, y), d_mean(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee717e2",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "1_c_code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def d_centroid(cluster1, cluster2):\n",
    "    \"\"\"\n",
    "    Calculate the distance between the centroids of two clusters.\n",
    "    \n",
    "    Args:\n",
    "        cluster1 (ndarray): Points belonging to cluster 1 of shape (num_points, num_dimensions).\n",
    "        cluster2 (ndarray): Points belonging to cluster 1 of shape (num_points, num_dimensions).\n",
    "    \n",
    "    Returns:\n",
    "        float: The distance between the centroids of two clusters.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    centroid1 = cluster1.mean(axis=0)\n",
    "    centroid2 = cluster2.mean(axis=0)\n",
    "    return np.linalg.norm(centroid1 - centroid2)\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "x = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "y = np.array([[13,14,15], [16,17,18], [19,20,21]])\n",
    "z = np.array([[-2,0], [-1,100]])\n",
    "w = np.array([[2,0], [1,100], [1,-100], [1,-20]])\n",
    "\n",
    "epsilon = 1e-3\n",
    "assert abs(d_centroid(x, y) - 20.785) < epsilon, \"Result is not correct: {}\".format(d_centroid(x, y))\n",
    "assert abs(d_centroid(z, w) - 55.069) < epsilon, \"Result is not correct: {}\".format(d_centroid(z, w))\n",
    "assert d_centroid(x, y) == d_centroid(y, x), \"X,Y is not equal to Y,X: {} != {}\".format(d_centroid(x, y), d_centroid(y, x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8695d1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa3a155692767cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " # Assignment 3: Hierarchical Clustering (5 points)\n",
    " \n",
    " Consider the following matrix of distances\n",
    " \n",
    "|       |  a  |  b  |  c  |  d  |  e  |\n",
    "|-------|-----|-----|-----|-----|-----|\n",
    "| **a** |  0  |  2  |  6  |  10 |  9  |\n",
    "| **b** |  2  |  0  |  5  |  9  |  8  |\n",
    "| **c** |  6  |  5  |  0  |  4  |  5  |\n",
    "| **d** |  10 |  9  |  4  |  0  |  3  |\n",
    "| **e** |  9  |  8  |  5  |  3  |  0  |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607465b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-178fda94686d02f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## a) Perform agglomerative clustering\n",
    "\n",
    "Do *agglomerative* average linkage clustering by hand (i.e. employing the *mean* cluster distance). Analyze how many alternatives you have to consider at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb31266",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1b175c45344ce687",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Given a set $C$ with cardinality $|n|$ we start with $n$ singleton sets, our initiaial clusters, and then have $\\tfrac12 n(n-1)$ options to join two of theses cluster to form a new one: in our example we could pair $a$ with any of the 4 other elements $b,c,d,e$, or $b$ with ony of the elements $c,d,e$, or $c$ with either $d$ or $e$, or $d$ with $e$, that is $\\tfrac12\\cdot5\\cdot(5-1)=10$ possible pairs. \n",
    "\n",
    "A simple heuristic is to always merge clusters with the smallest inter-cluster distance (the most similar clusters) according to the cluster distance $d_{mean}$. In our case, in the first step this will be the clusters $\\{a\\}$ and $\\{b\\}$ with $d_{mean}(\\{a\\},\\{b\\})=2$ (as for singleton sets, the average distance is just the object distance, given in the matrix). \n",
    "\n",
    "To proceed further, we have to update our distance matrix to contain the new cluster:\n",
    " \n",
    "|             |  $\\{a,b\\}$ |  $\\{c\\}$  |  $\\{d\\}$  |  $\\{e\\}$ |\n",
    "|-------------|------------|-----------|-----------|----------|\n",
    "| **{a,b}**   |  $0.0$     |  $5.5$    |  $9.5$    |  $8.5$   |\n",
    "| **c**       |  $5.5$     |  $0.0$    |  $4.0$    |  $5.0$   |\n",
    "| **d**       |  $9.5$     |  $4.0$    |  $0.0$    |  $3.0$   |\n",
    "| **e**       |  $8.5$     |  $5.0$    |  $3.0$    |  $0.0$      |\n",
    "\n",
    "Now use this matrix to continue the process: the smallest distance is between $\\{d\\}$ and $\\{e\\}$, so these to clusters are merged to a new cluster $\\{d,e\\}$ and the cluster distance matrix has to be updated accordingly:\n",
    "\n",
    "|               |  $\\{a,b\\}$ |  $\\{c\\}$  |  $\\{d,e\\}$  |\n",
    "|---------------|------------|-----------|-------------|\n",
    "| **{a,b}**     |  $0.0$     |  $5.5$    |  $9.0$      |\n",
    "| **{c}**       |  $5.5$     |  $0.0$    |  $4.5$      |\n",
    "| **{d,e}**     |  $9.0$     |  $4.5$    |  $0.0$      |\n",
    "\n",
    "Now the smallest distance is between clusters $\\{c\\}$ and $\\{d,e\\}$, and joining these cluster and updating the distance matrix yields:\n",
    "\n",
    "|             |  $\\{a,b\\}$   |  $\\{c,d,e\\}$  |\n",
    "|-------------|--------------|---------------|\n",
    "| **{a,b}**   |  $0.0$       |  $7.83$       |\n",
    "| **{c,d,e}** |  $7.83$      |  $0.0$        |\n",
    "\n",
    "So in total wie get the following dendrogram\n",
    "\n",
    "\n",
    "                        {a,b,c,d,e}\n",
    "                            | 7.83\n",
    "               +------------+-------------+\n",
    "               |                          |\n",
    "               |                       {c,d,e}\n",
    "               |                          | 4.5\n",
    "               |                 +--------+---------+\n",
    "               |                 |                  |\n",
    "               |                 |                {d,e}\n",
    "               |                 |                  | 3.0\n",
    "             {a,b}               |             +----+----+\n",
    "               | 2.0             |             |         |\n",
    "        +------+------+          |             |         |\n",
    "        |             |          |             |         |\n",
    "       {a}           {b}        {c}           {d}       {e}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aba7a4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0e512db6f3b50fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b) Perform divisive clustering\n",
    "\n",
    "Now try to do divisive average linkage clustering. Again, analyze how many splits are possible in the first step? Think of a strategy that allows to reduce this number and use this in your computation. Then apply the strategy to obtain a hierarchical clustering, that is iteratively split clusters until all clusters are singletons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cb55d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-273971298318f089",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Having a set $C$ of cardinality $|C|=n$, there are in total $2^{n-1}-1$ possibilities to split it into two non-empty sets $A$ and $B$: each element of $C$ can either be assigned to $A$ or to $B$ giving $2^n$ possible ways of assigning elements of $C$ to $A$ and $B$. This number includes symmetric cases (e.g. \"AABAB\" is equivalent to \"BBABA\") and the empty set (that is \"AAAAA\"), so removing these leads to $\\frac{2^n}{2}-1$. That is, in our case we would have to analyze $2^{5-1}-1=15$ splits.\n",
    "\n",
    "This number grows exponentially large with the cardinality of $C$, making considering all possible splits not an options even for medium-sized sets $C$. Hence one has to apply some heuristics to find a good solution. There are of course several valid ideas here.\n",
    "\n",
    "One such heuristic, based on ideas from Macnaughton-Smith et al (1964) is called DIANA (from DIvisive\n",
    "ANAlysis) and it works as follows: First single out the single element to form a cluster of its own, such that the cluster distance to the other elements is maximized. Then elementwise grow this cluster. This will give the first split into two clusters $A$ and $B$. Then recursively apply this procedure to the resulting clusters.\n",
    "\n",
    "In our example, this strategy works as follows: First we find one element $\\{a,b,c,d,e\\}$ such that the cluster distance (according to the $d_{mean}$ metric) to the remaining elements is maximized:\n",
    " \n",
    "|  $S_1$  |  $C_1$        |  $d_{mean}(S_1,C_1)$                   |\n",
    "|---------|---------------|----------------------------------------|\n",
    "| $\\{a\\}$ | $\\{b,c,d,e\\}$ |  $(2  + 6 + 10 + 9)/4 = \\mathbf{6.75}$ |\n",
    "| $\\{b\\}$ | $\\{a,c,d,e\\}$ |  $(2  + 5 +  9 + 8)/4 = 6.00$          |\n",
    "| $\\{c\\}$ | $\\{a,b,d,e\\}$ |  $(6  + 5 +  4 + 5)/4 = 5.00$          |\n",
    "| $\\{d\\}$ | $\\{a,b,c,e\\}$ |  $(10 + 9 +  4 + 3)/4 = 6.50$          |\n",
    "| $\\{e\\}$ | $\\{a,b,c,e\\}$ |  $(9  + 8 +  5 + 3)/4 = 6.24$          |\n",
    "\n",
    "So the singleton $S_1=\\{a\\}$ has the largest distance to the other elements. Now repeat this, finding the next single element from $C_1$ that maximizes the distance to the remaining elements:\n",
    "\n",
    "|  $S_2$  |  $C_2$      | $d_{mean}(S_2,C_2)$      | $d_{mean}(S_2,S_1)$ | $d_{mean}(S_2,C_2)-d_{mean}(S_2,S_1)$ | \n",
    "|---------|-------------|--------------------------|---------------------|-----------------------------|\n",
    "| $\\{b\\}$ | $\\{c,d,e\\}$ | $(5+9+8)/3 \\approx 7.33$ | $2.00$              | $\\mathbf{5.33}$             |\n",
    "| $\\{c\\}$ | $\\{b,d,e\\}$ | $(5+4+5)/3 \\approx 4.67$ | $6.00$              | $-1.33$                     |\n",
    "| $\\{d\\}$ | $\\{b,c,e\\}$ | $(9+4+3)/3 \\approx 5.33$ | $10.00$             | $-4.67$                     |\n",
    "| $\\{e\\}$ | $\\{b,c,e\\}$ | $(8+5+3)/3 \\approx 5.33$ | $9.00$              | $-3.67$                     |\n",
    "\n",
    "So element $b$ should be clustered together with element $a$. Now repeat this procedure for the remaining three element $\\{c,d,e\\}$:\n",
    "\n",
    "|  $S_3$  |  $C_3$    | $d_{mean}(S_3,C_3)$ | $d_{mean}(S_3,S_2)$ | $d_{mean}(S_3,C_3)-d_{mean}(S_2,S_2)$ | \n",
    "|---------|-----------|---------------------|---------------------|---------------------------------------|\n",
    "| $\\{c\\}$ | $\\{d,e\\}$ | $(4+5)/2 = 4.50$    | $(6+5)/2=5.50$      | $-1.00$                               |\n",
    "| $\\{d\\}$ | $\\{c,e\\}$ | $(4+3)/2 = 3.50$    | $(10+9)/2=9.50$     | $-6.00$                               |\n",
    "| $\\{e\\}$ | $\\{c,e\\}$ | $(5+3)/2 = 4.00$    | $(9+8)/2=8.50$      | $-4.5$                                |\n",
    "\n",
    "Now all differences are negative, meaning that moving a further element from $\\{c,d,e\\}$ to $\\{a,b\\}$ will not give an improvement. Hence in the first split we split the set $C=\\{a,b,c,d,e\\}$ into the two clusters\n",
    "$A=\\{a,b\\}$ and $B=\\{c,d,e\\}$. Now recursively apply this procedure to $A$ and $B$, starting with the cluster with the largest diameter (highest maximal distance between its objects). The diameter of $A$ is $2$ and for $B$ it is $5$. Applying the procedure to $B$ will result in the clusters $\\{c\\}$ and $\\{d,e\\}$, the diameter of $\\{d,e\\}$ being $3$. Hence one first splits the set into $\\{d\\}$ and $\\{e\\}$ and then $A$ into $\\{a\\}$ and $\\{b\\}$.\n",
    "\n",
    "```\n",
    "\n",
    "              {a,b,c,d,e}\n",
    "                   |\n",
    "        +----------+------------+\n",
    "        |                       |\n",
    "        |                    {c,d,e} D=5\n",
    "        |                       |\n",
    "        |                +------+------+\n",
    "        |                |             |\n",
    "        |                |           {d,e} D=3\n",
    "        |                |             |\n",
    "      {a,b} D=2          |        +----+----+\n",
    "        |                |        |         |\n",
    "    +-------+            |        |         |\n",
    "    |       |            |        |         |\n",
    "   {a}     {b}          {c}      {d}       {e}\n",
    "```\n",
    "\n",
    "* Macnaughton-Smith, P., Williams, W. T., Dale, M. B. , and Mockett, L. G. (1964), *Dissimilarity analysis: A new technique of hierarchical sub-division*, Nature, 202, 1034-1035."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d3d0d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## c) Linkage criteria\n",
    "\n",
    "In the following you find implementations for single- and complete-linkage clustering. Take a look at the code  and answer the question posted below. You may of course change parameters and try it out on different datasets (`points.txt` & `clusterData.txt` are provided).\n",
    "\n",
    "Note that for performance reasons the code differs from the lecture's pseudocode (ML-05 Slide 8), but in general it does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3271c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bece3476fa1da0d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def linkage(data, k=5, complete=False):\n",
    "    \"\"\"\n",
    "    Runs single or complete linkage clustering.\n",
    "    \n",
    "    Args:\n",
    "        data (ndarray): Data points to be clustered in an array with shape (num_points, 2).\n",
    "        k (int): Number of clusters.\n",
    "        complete (bool): Whether to run complete linkage clustering.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: The cluster labels for each data point. Shape is (num_points).\n",
    "    \"\"\"\n",
    "    # Initially all points are their own cluster.\n",
    "    labels = np.arange(len(data))\n",
    "\n",
    "    # Calculate distance between all points.\n",
    "    # Also removing half of the matrix because \n",
    "    # its symmetrical along the diagonal.\n",
    "    dst = np.tril(cdist(data, data))\n",
    "\n",
    "    while len(set(labels)) > k:\n",
    "        # Get the lowest distance of two points which\n",
    "        # do not have the same label.\n",
    "        r, c = np.where(dst == np.min(dst[dst > 0]))\n",
    "        \n",
    "        # Ignore the case when there are multiple with\n",
    "        # equally smallest distance.\n",
    "        r = r[0]\n",
    "        c = c[0]\n",
    "\n",
    "        # The two points are now in the same cluster,\n",
    "        # so they have a distance of 0 now.\n",
    "        dst[r, c] = 0\n",
    "\n",
    "        # Make the two clusters have the same label.\n",
    "        labels[labels == labels[r]] = labels[c]\n",
    "\n",
    "        # Check if we want to do complete linkage clustering.\n",
    "        if complete:\n",
    "            # Update the distances of the points which are not in the same cluster.\n",
    "            for i in np.nonzero(dst[r, :] > 0)[0]:\n",
    "                dst[r, i] = np.max(cdist(data[i, None], data[labels == labels[r], :]))\n",
    "\n",
    "            # The distances to c are now the same as to r, so we can just\n",
    "            # set them to zero - would be duplicates otherwise.\n",
    "            dst[:, c] = 0\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467546a8",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-88337d69614d2e22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data.\n",
    "data = np.loadtxt('points.txt')\n",
    "\n",
    "# Show unprocessed data set.\n",
    "fig_cluster = plt.figure()\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "plt.title('Unprocessed Cluster Data')\n",
    "fig_cluster.canvas.draw()\n",
    "\n",
    "# Apply Single Linkage Clustering\n",
    "labels = linkage(data, k=5, complete=False)\n",
    "unique, inverse, counts = np.unique(labels, return_inverse=True, return_counts=True)\n",
    "print(\"Single Linkage Clustering:\")\n",
    "# Print the unqiue labels and their occurence\n",
    "for u, c in zip(unique, counts):\n",
    "    print(\"Label: {:4},  Occurence: {:4}\".format(u, c))    \n",
    "# Replace labels by continuous values starting from 1 for discernible colors in plot\n",
    "labels = np.arange(1,unique.size+1)[inverse]\n",
    "fig_single = plt.figure()\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels)\n",
    "plt.title('Single-linkage Clustering with k=5')\n",
    "fig_single.canvas.draw()\n",
    "\n",
    "\n",
    "# Apply Complete Linkage Clustering\n",
    "labels = linkage(data, k=5, complete=True)\n",
    "unique, inverse, counts = np.unique(labels, return_inverse=True, return_counts=True)\n",
    "print(\"Complete Linkage Clustering:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(\"Label: {:4},  Occurence: {:4}\".format(u, c))    \n",
    "labels = np.arange(1,unique.size+1)[inverse]\n",
    "fig_complete = plt.figure()\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels)\n",
    "plt.title('Complete-linkage Clustering with k=5')\n",
    "fig_complete.canvas.draw()\n",
    "\n",
    "# Test different parameters above\n",
    "### BEGIN SOLUTION\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc87ee",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "2_question",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What is the difference between single- and complete-linkage clustering and which is the better solution given the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068e631",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "2_answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Single-linkage tends to chain clusters along the data. That is why it combines the points in the center area with those in the bottom right corner.\n",
    "\n",
    "Complete-linkage prefers compact clusters and thus combines each of the point heavy areas individually without merging them.\n",
    "\n",
    "For this dataset, complete-linkage is superior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
