{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2b7a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637eec62",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba653d2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 9, 2024**. If you need help (and Google and other resources were not enough), ask in the forum, contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd8d16",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78e418c8c7c6b9cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Conditional Probability) [0 Points]\n",
    "\n",
    "This exercise is supposed to be easy and is voluntary. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e041956",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-cprob-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the idea of conditional probability. How is it defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df620959",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-cprob-a1",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Conditional probability is the probability that an event A happens, given that another event B happened.\n",
    "For example:\n",
    "The probability of rain is $$P(weather=\"rain\") = 0.3$$ But if you observe, if the street is wet you would get the conditional probability $$P(weather= \"rain\" |~ street=\"wet\") = 0.95$$\n",
    "The definition is:\n",
    "$$ P(A|B) = \\frac{P(A,B)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6727f78",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-cprob-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is Bayes' theorem? What are its applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28e7a9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-cprob-a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Bayes Theorem states:\n",
    "$$ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} $$\n",
    "\n",
    "The most important application is in reasoning backwards from event to cause (from data to parameters of your distribution):\n",
    "\n",
    "$$ P(\\Theta|Data) = \\frac{P(Data|\\Theta)P(\\Theta)}{P(Data)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec8de8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "math-cprob-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What does the law of total probability state? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d0da4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "math-cprob-a3",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The law of total probability states, that the probabilty of an event occuring is the same as the sum of the probabilities of this event occuring together with all possible states of an other event:\n",
    "$$P(A) = \\sum_b P(A,B=b) = \\sum_b P(A|B=b) P(B=b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2e263",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ex-perceptron-xor1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: The Logic Perceptron: XOR (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3735cb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ex-perceptron-xor2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Explain in your own words, why the XOR, in contrast to AND, OR, and NAND, can not be implemented by a single perceptron. What other logical operators face the same problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2c767",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-313b53617562fdb6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The percptron realizes a linear binary classifier, that is, it discriminates between positive and negative points using a hyperplane. Hence it can only implement logical functions where positive and negative outputs are linerly separable.  This is the case for AND, OR, and NAND, but not for XOR.\n",
    "\n",
    "The equivalence (X1 <=> X2) is another example of a logical operation not implementable by a single perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a3c72",
   "metadata": {},
   "source": [
    "**b)** Create two multi-layer perceptrons that encode the XOR function, applying the solutions sketched on (ML-7 slide 37): the first should distort the input space, while the second should add another axis. Explain the operation of your MLP on a geometric level. What is the minimal number of units to be placed in the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d37c51",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-035f1ff2152088cc",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "```\n",
    "         +1    >0.5\n",
    "    x1 ------ h1\n",
    "       \\    /   \\\n",
    "      -1\\  /     \\+1\n",
    "         \\/       \\ out >1.5\n",
    "         /\\       /\n",
    "      +1/  \\     /+1\n",
    "       /    \\   /\n",
    "    x2 ------ h2\n",
    "        -1     >-1.5\n",
    "```\n",
    "This MLP computes h1 as (x1 OR x2) and h2 as (x1 NAND x2), that is h2 = NOT(x1 AND x2), and out is (h1 AND h2).\n",
    "Hence out = (x1 OR x2) AND NOT (x1 AND x2) = x1 XOR x2.\n",
    "\n",
    "Geometrically, the first layer MLP maps the two dimensional input space (x1,x2) into another two-dimensional space (h1,h2) applying a nonlinear transformation $f$. \n",
    "\n",
    "  | (x1,x2)  | (h1,h2) | out |\n",
    "  |----------|---------|-----|\n",
    "  | (0,0)    | (0,1)   |  1  |\n",
    "  | (0,1)    | (1,1)   |  0  |\n",
    "  | (1,0)    | (1,1)   |  0  |\n",
    "  | (1,1)    | (1,0)   |  1  |\n",
    "\n",
    "The internal points (0,1) and (1,0) are linearly separable from the point (1,1), e.g. by the line h1+h2=1.5.\n",
    "\n",
    "```\n",
    "       +1   >0.5\n",
    "    x1 --- h1\n",
    "       \\       \\+1\n",
    "      +1\\       \\\n",
    "         \\    -2 \\\n",
    "           h3 --- out >.5\n",
    "         /  >1.5 /\n",
    "      +1/       /+1\n",
    "       /       /\n",
    "    x2 --- h2\n",
    "       +1   >0.5\n",
    "```\n",
    "\n",
    "  | (x1,x2)  | (h1,h2,h3) | out |\n",
    "  |----------|------------|-----|\n",
    "  | (0,0)    | (0,0,0)    |  1  |\n",
    "  | (0,1)    | (0,1,0)    |  0  |\n",
    "  | (1,0)    | (1,0,0)    |  0  |\n",
    "  | (1,1)    | (1,1,1)    |  1  |\n",
    "\n",
    "The internal points (0,1,0) and (1,0,0) are linearly separable from the points (0,0,0) and (1,1,1) by the hyperplane h1+h2-2$*$h3=0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21377369",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Perceptron (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40053f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this exercise you will implement a simple perceptron as described in the lecture [ML-07 Slide 31]. As with  previous exercises it is possible to not use our premade code blocks but write the single Perceptron completely from scratch (an empty cell to do so can be found [below](#Own-Implementation)). \n",
    "\n",
    "Use the following output function:\n",
    "$$y = \\begin{cases}1 \\quad \\text{if} \\ s \\ge 0\\\\0 \\quad \\text{else}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da8575",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron_x3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The `TODO`'s in the following code segments guide you through what has to be done.\n",
    "\n",
    "*Hint*: If you have problems with `np.arrays` (which usually have shapes like `(13,)`, thus with one degenerate dimension, either set the shapes manually (`my_np_array.shape = (13, 1)`) or use [np.atleast_2d](https://numpy.org/doc/stable/reference/generated/numpy.atleast_2d.html). Other useful functions might be\n",
    "* [lambda functions](https://docs.python.org/3/reference/expressions.html#lambda)\n",
    "* [np.hstack](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html?)\n",
    "* [np.append](https://numpy.org/doc/stable/reference/generated/numpy.append.html?highlight=append#numpy.append)\n",
    "* [np.apply_along_axis](https://numpy.org/doc/stable/reference/generated/numpy.apply_along_axis.html)\n",
    "* [try except](https://docs.python.org/3/tutorial/errors.html?highlight=try%20except#handling-exceptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d9096",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_perceptron_a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# TODO: Write the input activation (called net_input) and the output function (called out_fun).\n",
    "### BEGIN SOLUTION\n",
    "# The net input function (weighted input signals)\n",
    "net_input = lambda d, w: d @ w.T\n",
    "\n",
    "# The output function determines the output of the neuron (1 if x > 0 else 0).\n",
    "out_fun = lambda x: float(x >= 0)\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "# TODO: Write a function generate_weights that generates N (= number of dimensions) + 1 (w_0) random weights.\n",
    "### BEGIN SOLUTION\n",
    "generate_weights = lambda dims: rnd.rand(dims + 1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50d19a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron_a_assert",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Testing the perceptron with a concrete example ##\n",
    "####################################################\n",
    "\n",
    "# Dimensions for our test.\n",
    "dims = 12\n",
    "\n",
    "# Input is a row vector. (Shape is (1, 13).)\n",
    "D = np.hstack((1, rnd.rand(dims) - 0.5))\n",
    "\n",
    "# Weights are stored in a vector.\n",
    "W = generate_weights(dims)\n",
    "\n",
    "out = out_fun(net_input(D, W))\n",
    "\n",
    "assert out == 1 or out == 0, \"The output has to be either 1 or 0, but was {}\".format(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c3182",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron_b_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following `eval_network(t, D, W)` function is used to measure the performance of your perceptron for the upcoming task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c00fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_network(t, D, W):\n",
    "    \"\"\"\n",
    "    This function takes the trained weights of a perceptron\n",
    "    and the input data (D) as well as the correct target values (t)\n",
    "    and computes the overall error rate of the perceptron.\n",
    "    \"\"\"\n",
    "    error = 0.0\n",
    "    size = max(D.shape)\n",
    "    for i in range(size):\n",
    "        out = out_fun(net_input(D[i], W))\n",
    "        error = error + abs(t[i] - out)\n",
    "    # Normalize the error.\n",
    "    try:\n",
    "        return error.item(0) / size\n",
    "    except AttributeError:\n",
    "        return error / size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c696e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we will use the above defined functions to train the perceptron to one of the following logical functions: OR, NAND or NOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ffcc0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron_b_plotting",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def function_to_learn(selector, function):\n",
    "    \"\"\"\n",
    "    Functional definitions for the perceptron to learn\n",
    "    Instantiates plots for visualization of the decision boundary\n",
    "    :param selector: selects which function to activate\n",
    "    :return function:\n",
    "    \"\"\"\n",
    "    plot_points = [[0,0],[0,1],[1,0],[1,1]]\n",
    "    plot_colors = []\n",
    "\n",
    "    for point in plot_points:\n",
    "        plot_colors.append(function(point[0], point[1]))\n",
    "    for color, point in enumerate(plot_points):\n",
    "        plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "    print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577e894",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_perceptron_b_solution",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change this line to choose other operators:\n",
    "op = 'and'\n",
    "\n",
    "\n",
    "###################################################\n",
    "## Now we train our perceptron! [ML-07 Slide 33] ##\n",
    "###################################################\n",
    "\n",
    "# TODO: Write the update function (name it 'delta_fun')\n",
    "#       for the weights dependent on epsilon, the target,\n",
    "#       the output and the input vector.\n",
    "### BEGIN SOLUTION\n",
    "delta_fun = lambda ϵ, t, y, x: ϵ * (t - y) * x\n",
    "### END SOLUTION\n",
    "\n",
    "# TODO: Define suitable parameters for your problem.\n",
    "# Use the following names:\n",
    "#   ϵ: learning rate\n",
    "#   dims: dimensions\n",
    "#   training_size: the number of training samples\n",
    "### BEGIN SOLUTION\n",
    "ϵ = 0.1\n",
    "dims = 2\n",
    "training_size = 1000\n",
    "### END SOLUTION\n",
    "\n",
    "# TODO: Generate the weights (in a variable called W).\n",
    "### BEGIN SOLUTION\n",
    "W = generate_weights(dims)\n",
    "### END SOLUTION\n",
    "\n",
    "# TODO: Generate a matrix D of truthvalue pairs.\n",
    "# The shape should be (training_size, dims).\n",
    "### BEGIN SOLUTION\n",
    "D = rnd.rand(training_size, dims) > 0.5\n",
    "### END SOLUTION\n",
    "\n",
    "# TODO: Pad the input D with ones for the bias. The bias should always be\n",
    "# w_0, i. e. the first column of the data should be ones.\n",
    "### BEGIN SOLUTION\n",
    "D = np.hstack((np.ones((training_size, 1)), D))\n",
    "### END SOLUTION\n",
    "\n",
    "# Learn one of the logical functions OR, NAND, NOR\n",
    "# (the lambda keyword is just a short way to define functions).\n",
    "log_operators = {\n",
    "    'and': lambda x1, x2: x1 and x2,\n",
    "    'or': lambda x1, x2: x1 or x2,\n",
    "    'nand': lambda x1, x2: not (x1 and x2),\n",
    "    'nor': lambda x1, x2: not (x1 or x2),\n",
    "    'xor': lambda x1, x2: (x1 and not x2) or (not x1 and x2)\n",
    "}\n",
    "\n",
    "log_operator = log_operators[op]\n",
    "function_to_learn(op, log_operator)\n",
    "\n",
    "row_operator = lambda row: log_operator(row[0], row[1])\n",
    "labels = np.apply_along_axis(row_operator, 1, D[:, 1:])\n",
    "\n",
    "epochs = 200    # Extra question: What effects do changes in the epochs \n",
    "samp_size = 5  #                 and sample sizes have on our training?\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Sample random from the training data.\n",
    "    for idx in rnd.choice(range(training_size), samp_size, replace=False):\n",
    "        y = out_fun(net_input(D[idx], W))\n",
    "        W += delta_fun(ϵ, labels[idx], y, D[idx])\n",
    "    # Plotting code    \n",
    "    y_point = (0, (-W[0] / W[2]))\n",
    "    x_point = ((-W[0] / W[1]), 0)\n",
    "    try:\n",
    "        slope = (y_point[1] - x_point[1]) / (y_point[0] - x_point[0]) # will not work if x and y intercepts are 0\n",
    "    except ZeroDivisionError:\n",
    "        print(\"X and Y intercepts are both zero.  Due to the way slope is calculated, this causes a division by zero.  Sorry.\")\n",
    "    y_out = lambda points: slope * points\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    plt.plot(x, y_out(x) + y_point[1], 'g--', linewidth=3, alpha=i/epochs +.2 if i/epochs +.2 < 1 else 1)\n",
    "    \n",
    "plt.ylim([-.2, 1.2])\n",
    "plt.xlim([-.2, 1.2])\n",
    "plt.title(\"Logic Perceptron (Blue=True)\")\n",
    "plt.xlabel(\"True(1) or False(0)\")\n",
    "plt.ylabel(\"True(1) or False(0)\")\n",
    "plt.show()\n",
    "\n",
    "# Print the overall performance of the Perceptron.\n",
    "print(\"Overall error of the Perceptron: {:.2%}\".format(eval_network(labels, D, W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c2eebc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_perceptron_x4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Own Implementation\n",
    "\n",
    "Skip this if you already implemented the perceptron above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6572dd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_perceptron_x4_solution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Space for complete own implementation\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "###################################################\n",
    "## A more object oriented approad.               ##\n",
    "###################################################\n",
    "class Perceptron():\n",
    "    \n",
    "    def __init__(self, num_inputs, learning_rate=0.1):\n",
    "        \n",
    "        self._weights = rnd.normal(size=num_inputs + 1)\n",
    "        self._learning_rate = learning_rate\n",
    "    \n",
    "    def train(self, train_data, train_labels, epochs=1, sample_size=5):\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            # Sample random from the training data.\n",
    "            for idx in rnd.choice(range(train_data.shape[0]), sample_size, replace=False):\n",
    "                self._train_step(train_data[idx], train_labels[idx])\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self._output(self._activation(data))\n",
    "    \n",
    "    def evaluate(self, test_data, test_labels):\n",
    "        prediction = self.predict(test_data)\n",
    "        error = (np.abs(prediction - test_labels)).mean()\n",
    "\n",
    "        return error\n",
    "        \n",
    "    def _activation(self, inputs):\n",
    "        return self._weights @ inputs.T\n",
    "    \n",
    "    def _output(self, activation):\n",
    "        return (activation >= 0).astype('int32')\n",
    "    \n",
    "    def _train_step(self, data, label):\n",
    "        self._weights += self._learning_rate * (label - self.predict(data)) * data\n",
    "        \n",
    "        \n",
    "###################################################\n",
    "## Create training data.                         ##\n",
    "###################################################\n",
    "dims = 2\n",
    "training_size = 2000\n",
    "\n",
    "data = rnd.randint(low=0, high=2, dtype='int32', size=(training_size, dims))\n",
    "# Pad the input D with ones for the bias.\n",
    "data = np.column_stack((np.ones(training_size, dtype='int32'), data))\n",
    "\n",
    "logical_operator = lambda x1, x2: x1 and x2\n",
    "# logical_operator = lambda x, y: not (x and y)\n",
    "# logical_operator = lambda x, y: not (x or y)\n",
    "\n",
    "row_operator = lambda row: logical_operator(row[0], row[1])\n",
    "labels = np.apply_along_axis(row_operator, 1, data[:, 1:])\n",
    "\n",
    "# Split into test and training sets.\n",
    "train_data = data[:-5]\n",
    "train_labels = labels[:-5]\n",
    "test_data = data[-5:]\n",
    "test_labels = labels[-5:]\n",
    "\n",
    "###################################################\n",
    "## Train and evaluate.                           ##\n",
    "###################################################\n",
    "perceptron = Perceptron(num_inputs=2)\n",
    "perceptron.train(train_data, train_labels, epochs=20, sample_size=50)\n",
    "perceptron.evaluate(test_data, test_labels)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479ac78",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_sigmoid",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Sigmoid Activation & Backpropagation Delta Functions (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fa6f3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_sigmoid_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this exercise we are first going to take the derivative of a famous activation function - the sigmoid function:\n",
    "\n",
    "$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "This function is commonly used because of its nice analytical properties: Its image is the interval $(0,1)$, it is non-linear, strictly monotonous, continuous, differentiable and the derivative can be expressed in terms of the original function at the given point. This allows us to avoid redundant calculations. The sigmoid function is a special case of the more general *Logistic function* which can be found in many different fields: Biology, chemistry, economics, demography and recently most prominently: artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfba127",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_sigmoid_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(a)** Computing the derivative of the sigmoid activation function:\n",
    "\n",
    "Proof that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial t} = \\frac{1}{\\left({1 + e^{-t}}\\right)} \\cdot \\frac{e^{-t}}{\\left({1 + e^{-t}}\\right)}\n",
    "$$\n",
    "\n",
    "and that it can be rewritten as an expression in terms of $\\sigma(t)$ resulting in:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial t} = \\sigma(t) \\left(1 - \\sigma(t) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84922b6b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_sigmoid_a_solution",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$\\newcommand{\\e}{\\mathrm{e}}\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\sigma}{\\partial t} &= \\frac{\\partial}{\\partial t} \\frac{1}{1 + \\e^{-t}}\\\\\n",
    "&= \\frac{\\partial}{\\partial t} \\left({1 + \\e^{-t}}\\right)^{-1}\\\\\n",
    "&= \\left.- \\left({1 + \\e^{-t}}\\right)^{-2} \\cdot (- \\e^{-t}) ~\\right\\vert~ \\text{by chain rule}\\\\\n",
    "&= \\frac{\\e^{-t}}{\\left({1 + \\e^{-t}}\\right)^{2}}\\\\\n",
    "&= \\frac{1}{\\left({1 + \\e^{-t}}\\right)} \\cdot \\frac{\\e^{-t}}{\\left({1 + \\e^{-t}}\\right)}\\\\\n",
    "&= \\sigma(t) \\cdot \\frac{\\e^{-t}}{\\left({1 + \\e^{-t}}\\right)}\\\\\n",
    "&= \\left.\\sigma(t) \\cdot \\frac{(1+\\e^{-t}) - 1}{\\left({1 + \\e^{-t}}\\right)}~\\right\\vert~ 1-1=0\\\\\n",
    "&= \\sigma(t) \\cdot \\left( \\frac{(1+\\e^{-t})}{\\left({1 + \\e^{-t}}\\right)} - \\frac{1}{\\left({1 + \\e^{-t}}\\right)} \\right)\\\\\n",
    "&= \\sigma(t) \\left(1 - \\sigma(t) \\right)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50dbcec",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_sigmoid_b_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(b)** Multilayer perceptrons (MLPs) can be regarded as a simple concatenation (and parallelization) of several perceptrons, each having a specified activation function $\\sigma$ and a set of weights $\\mathbf{w}_{ij}$. The idea that this can be done was discovered early after the invention of the perceptron, but people didn't really use it in practice because nobody really knew how to figure out the appropriate $\\mathbf{w}_{ij}$. The solution to this problem was the discovery of the backpropagation algorithm which consists of two steps: first propagating the input forward through the layers of the MLP and storing the intermediate results and then propagating the error backwards and adjusting the weights of the units accordingly.\n",
    "\n",
    "An updating rule for the output layer can be derived straightforward. The rules for the intermediate layers can be derived very similarly and only require a slight shift in perspective - the mathematics for that are however not in the standard toolkit so we are going to omit the calculations and refer you to the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e404969",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex_sigmoid_b_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We take the (halvedleast) least-squares approach to derive the updating rule, i.e. we want to minimize the Loss function\n",
    "$$L = \\frac{1}{2}(y-t)^2$$\n",
    "where t is the given (true) label from the dataset and y is the (single) output produced by the MLP. To find the weights that minimize this expression we want to take the derivative of $L$ w.r.t. $\\mathbf{w}_{i}$ where we are now going to assume that the $\\mathbf{w}_{i}$ are the ones directly before the output layer:\n",
    "$$y = \\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)$$\n",
    "Calculate $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}}$.\n",
    "\n",
    "*Hint*: Start here if you don't know what to do: $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial \\mathbf{w}_{i}}$\n",
    "\n",
    "*Hint*: Remember that the derivative of the sigmoid activation function in its general form (from part **a**) is defined as: $\\frac{\\partial \\sigma}{\\partial t} = \\sigma(t) \\left(1 - \\sigma(t) \\right)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59474c73",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_sigmoid_b_solution",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_{i}} &= \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial \\mathbf{w}_{i}}\\\\\n",
    "&=\\frac{\\partial}{\\partial y} \\frac{1}{2} (y - t)^2 \\cdot \\frac{\\partial}{\\partial \\mathbf{w}_{i}} \\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)\\\\\n",
    "&=\\left.\\left((y-t) \\cdot 1\\right) \\cdot \\left(\\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)\\left(1-\\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)\\right) \\cdot o_i \\right) ~\\right\\vert~ \\text{by chain rule}\\\\\n",
    "&=\\left.(y-t) \\cdot y \\cdot (1 -y) \\cdot o_i ~\\right\\vert~ y = \\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e9e87",
   "metadata": {},
   "source": [
    "## Assignment 4: Training a MLP by hand (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9747a45",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9124cf5d238c3fe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Consider the following multilayer perceptron (notation from ML-7 slides 46ff), consisting of an input layer (layer $k=0$, with two neurons 1 & 2), a hidden layer ($k=1$ with two neurons 3 & 4) and an output layer ($k=2$ with two neurons 5 & 6).  The connection weights are given by the following image and connectivity matrix:\n",
    "\n",
    "![mlp-large.png](mlp-large.png)\n",
    "\n",
    "to\\from|1  |2  |3  |4  |5  |66\n",
    "-------|---|---|---|---|---|--\n",
    "1      |-  |-  |-  |-  |-  |-\n",
    "2      |-  |-  |-  |-  |-  |-\n",
    "3      |-3 |2  |-  |-  |-  |-\n",
    "4      |2  |1  |-  |-  |-  |-\n",
    "5      |-  |-  |4  |-1 |-  |-\n",
    "6      |-  |-  |-2 |0.5|-  |-\n",
    "\n",
    "The hidden layer (neurons 3 & 4) applies the [rectifier](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as activation function.\n",
    "$$\n",
    "    \\varphi_{R}(x)=\\max(0,x)\n",
    "$$\n",
    "\n",
    "The output layer (neurons 5 & 6) uses the sigmoid ([standard logistic function](https://en.wikipedia.org/wiki/Logistic_function), [Fermi function](https://en.wikipedia.org/wiki/Fermi%E2%80%93Dirac_statistics)) as activation function.\n",
    "$$\n",
    "    \\varphi_{S}(x)={\\frac {1}{1+e^{-x}}}\n",
    "$$\n",
    "\n",
    "To measure the error, following the lecture (ML-7 slide 48), the (halved) [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) is used, that is\n",
    "$$E[\\{w\\}](\\vec{t},\\vec{y}, ) = \n",
    "\\tfrac{1}{2}\\left\\|\\vec{t}-\\vec{y}\\right\\|_2^2 =\n",
    "\\frac{1}{2}\\sum_{i=1}^{d}(t_i-y_i)^2$$\n",
    "with $\\vec{y}$ being the values predicted by the network, $\\vec{t}$ the target value (\"ground truth\"), and $d=2$ the dimensionality of the output space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4452d32",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3e8b72921e20aa9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85783e35",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-55cef3a66de71874",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Computing the weighted input for layer 1:\n",
    "\\begin{align*}\n",
    "  s_3(1) &= \\sum_{j=1}^{2} w_{3j}(1,0)o_j(0) && = -3\\cdot 1 + 2\\cdot 2 = -3 + 4 && = 1 \\\\\n",
    "  s_4(1) &= \\sum_{j=1}^{2} w_{4j}(1,0)o_j(0) && = 2\\cdot 1 + 1\\cdot 2 = 2+2 && = 4\n",
    "\\end{align*}\n",
    "The outputs of layer 1 are hence:\n",
    "\\begin{align*}\n",
    "  o_3(1) &= \\varphi_{R}(s_3(1)) &&= \\max(0,1) && = 1 \\\\\n",
    "  o_4(1) &= \\varphi_{R}(s_4(1)) &&= \\max(0,4) && = 4 \n",
    "\\end{align*}\n",
    "\n",
    "For layer 2 we then get the following weighted input:\n",
    "\\begin{align*}\n",
    "  s_5(2) &= \\sum_{j=3}^{4} w_{5j}(2,1)o_j(1) &&= 4\\cdot 1 - 1\\cdot 4 &&= 0 \\\\\n",
    "  s_6(2) &= \\sum_{j=3}^{4} w_{6j}(2,1)o_j(1) &&= -2\\cdot 1 + 0.5\\cdot 4 && = 0 \n",
    "\\end{align*}\n",
    "The outputs of layer 2 (which also are the network output) ar\n",
    "\\begin{align*}\n",
    "  o_5(2) &= \\varphi_{S}(s_5(2)) = \\sigma(0) = 0.5 \\\\\n",
    "  o_6(2) &= \\varphi_{S}(s_6(2)) = \\sigma(0) = 0.5\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bcfcd9",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdb6bee338dc2a58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f717f8",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b44af9b40e98afb6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The loss value is given by the (halved) mean squared error between the network output $\\vec{y}=(0.5, 0.5)$ and the target value $\\vec{t}=(1.0,0.0)$:\n",
    "\\begin{align*}\n",
    "  E[{w}](\\vec{t},\\vec{y}) \n",
    "  & = \\tfrac12\\|\\vec{t}-\\vec{y}\\|^2\\\\\n",
    "  & = \\tfrac12\\sum_{i=1}^{2}(t_i-y_i)^2\\\\\n",
    "  & = \\tfrac12\\left[(1-0.5)^2 + (0-0.5)^2\\right]\\\\\n",
    "  & = \\tfrac12\\left[0.25 + 0.25\\right]\\\\\n",
    "  & = 0.25\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd525b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46c4ab2f01383e11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351928fd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ef70bf1de9d0bf22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For the output layer we get the following error signal:\n",
    "\\begin{align*}\n",
    "  \\delta_5(2) & = \\varphi_{S}'(s_5)\\cdot (t_1-y_1(\\vec{x})) && = \\sigma'(0)\\cdot(1.0-0.5) && = \\sigma(0)(1-\\sigma(0))\\cdot 0.5 && = .5 \\cdot .5 \\cdot .5 && = 0.125 \\\\\n",
    "  \\delta_6(2) & = \\varphi_{S}'(s_6)\\cdot (t_2-y_2(\\vec{x})) && = \\sigma'(0)\\cdot(0.0-0.5) && = \\sigma(0)(1-\\sigma(0))\\cdot -0.5 && = .5 \\cdot .5 \\cdot -.5 && = -0.125\n",
    "\\end{align*}\n",
    "From this we can obtain the second layer weight gradients:\n",
    "\\begin{align*}\n",
    "  -\\partial E/\\partial w_{53}(2,1) & = \\delta_5(2)o_3(1) &&= 0.125 \\cdot 1 &&= 0.125 \\\\\n",
    "  -\\partial E/\\partial w_{54}(2,1) & = \\delta_5(2)o_4(1) &&= 0.125 \\cdot 4 &&= 0.5 \\\\\n",
    "  -\\partial E/\\partial w_{63}(2,1) & = \\delta_6(2)o_3(1) && = -0.125 \\cdot 1 &&= -0.125 \\\\\n",
    "  -\\partial E/\\partial w_{64}(2,1) & = \\delta_6(2)o_4(1) && = -0.125 \\cdot 4 &&= -0.5 \n",
    "\\end{align*}\n",
    "\n",
    "For layer 1 the error signal is:\n",
    "\\begin{align*}\n",
    "  \\delta_3(1) &= \\varphi_{R}'(s_3(1))\\cdot\\sum_{j=5}^{6}w_{j3}(2,1)\\delta_j(2) && = \\varphi_{R}'(1)\\cdot\\left[4\\cdot 0.125 + -2\\cdot-0.125\\right] &&= 1\\cdot [0.5 + 0.25] &&= 0.75\\\\\n",
    "  \\delta_4(1) &= \\varphi_{R}'(s_4(1))\\cdot\\sum_{j=5}^{6}w_{j4}(2,1)\\delta_j(2) && = \\varphi_{R}'(4)\\cdot\\left[-1\\cdot 0.125 + 0.5\\cdot-0.125\\right] &&= 1\\cdot [-0.125-0.0625] &&= -0.1875\n",
    "\\end{align*}\n",
    "yielding the following gradients:\n",
    "\\begin{align*}\n",
    "  -\\partial E/\\partial w_{31}(1,0) &= \\delta_3(1)o_1(0) &&= 0.75 \\cdot 1.0 && = 0.75 \\\\\n",
    "  -\\partial E/\\partial w_{32}(1,0) &= \\delta_3(1)o_2(0) &&= 0.75 \\cdot 2.0 && = 1.5 \\\\\n",
    "  -\\partial E/\\partial w_{41}(1,0) &= \\delta_4(1)o_1(0) &&= -0.1875 \\cdot 1.0 &&= -0.1875 \\\\\n",
    "  -\\partial E/\\partial w_{42}(1,0) &= \\delta_4(1)o_2(0) &&= -0.1875 \\cdot 2.0 &&= -0.375\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1e08e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aed85a1c01fe5ed5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b8d5b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f7cf68d9d7e6271",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The adaptation rule is\n",
    "\n",
    "$$ w_{ji}(k+1,k) \\mapsto w_{ji}(k+1,k) + \\Delta w_{ji}(k+1,k)$$\n",
    "\n",
    "with the update term\n",
    "\n",
    "$$ \\Delta w_{ji}(k+1,k) = - \\varepsilon \\partial E/\\partial w_{ji}(k+1,k) = \\varepsilon \\delta_j(k+1)o_j(k)$$\n",
    "\n",
    "For the first layer that is\n",
    "\\begin{align*}\n",
    "  w_{31}(1,0): &&-3 \\mapsto & -3 + 1\\cdot 0.75   && = -2.25 \\\\\n",
    "  w_{32}(1,0): && 2 \\mapsto &  2 + 1\\cdot1.5    && = 3.5 \\\\\n",
    "  w_{41}(1,0): && 2 \\mapsto &  2 + 1\\cdot(-0.1875) && = 1.8125 \\\\\n",
    "  w_{42}(1,0): && 1 \\mapsto &  1 + 1\\cdot(-0.375)  && = 0.625\n",
    "\\end{align*}\n",
    "and for the second layer\n",
    "\\begin{align*}\n",
    "  w_{53}(2,1): &&  4 \\mapsto &    4 + 1\\cdot 0.125    && = 4.125 \\\\\n",
    "  w_{54}(2,1): && -1 \\mapsto &   -1 + 1\\cdot 0.5      && = 0.5 \\\\\n",
    "  w_{63}(2,1): && -2 \\mapsto &   -2 + 1\\cdot (-0.125) && = 2.125 \\\\\n",
    "  w_{64}(2,1): && 0.5 \\mapsto & 0.5 + 1\\cdot (-0.5)   && = 0.0\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
