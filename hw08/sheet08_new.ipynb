{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e28dcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44fca659c1e85aa18c1a44bcb17af98a",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe483d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0363ecd47553f55d98a7188069b1912a",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34181188",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a13c00c64a07091b88c34e1fa3dbf653",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 9, 2024**. If you need help (and Google and other resources were not enough), ask in the forum, contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65ecd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8e62347c7b28f3e99d6deed053b2320",
     "grade": false,
     "grade_id": "cell-78e418c8c7c6b9cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Conditional Probability) [0 Points]\n",
    "\n",
    "This exercise is supposed to be very easy and is voluntary. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c65af2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "855316bdf01ce31dab06b89382d0c83f",
     "grade": false,
     "grade_id": "math-cprob-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the idea of conditional probability. How is it defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da0a06",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdd8edfccc04a31d9a983866800a57ca",
     "grade": true,
     "grade_id": "math-cprob-a1",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb08732",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46198461fba50cb38ba7278aca6b935c",
     "grade": false,
     "grade_id": "math-cprob-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is Bayes' theorem? What are its applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc0a1f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe506eb4d056aad3a69041b3e97ee42c",
     "grade": true,
     "grade_id": "math-cprob-a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f73b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b1e731d96f98286b8a7c10c0ce192ee",
     "grade": false,
     "grade_id": "math-cprob-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What does the law of total probability state? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6cbbc6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c694bdf071196e589958260e49ab3fdd",
     "grade": true,
     "grade_id": "math-cprob-a3",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16ba28",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1380caef375a9bc8348b02c55b0264e",
     "grade": false,
     "grade_id": "cell-ex-perceptron-xor1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: The Logic Perceptron: XOR (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212efb0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d696194bc93f447c1bc54c03a7390ae1",
     "grade": false,
     "grade_id": "cell-ex-perceptron-xor2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Explain in your own words, why the XOR, in contrast to AND, OR, and NAND, can not be implemented by a single perceptron. What other logical operators face the same problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97652e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18eb9303f3a1c14b924878ac8186c833",
     "grade": true,
     "grade_id": "cell-313b53617562fdb6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61974391",
   "metadata": {},
   "source": [
    "**b)** Create two multi-layer perceptrons that encode the XOR function, applying the solutions sketched on (ML-7 slide 37): the first should distort the input space, while the second should add another axis. Explain the operation of your MLP on a geometric level. What is the minimal number of units to be placed in the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bdeac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8a336019620cab658542c4461e4f221",
     "grade": true,
     "grade_id": "cell-035f1ff2152088cc",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af02c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b25ff9bb5f060ad9e1502feb4c26e24a",
     "grade": false,
     "grade_id": "ex_perceptron",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Perceptron (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed2e72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afd95267ec178d9a6563144232e7280f",
     "grade": false,
     "grade_id": "ex_perceptron_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this exercise you will implement a simple perceptron as described in the lecture [ML-07 Slide 31]. As with  previous exercises it is possible to not use our premade code blocks but write the single Perceptron completely from scratch (an empty cell to do so can be found [below](#Own-Implementation)). \n",
    "\n",
    "Use the following output function:\n",
    "$$y = \\begin{cases}1 \\quad \\text{if} \\ s \\ge 0\\\\0 \\quad \\text{else}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241daf2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93311394694ed0852a17e33883676ecd",
     "grade": false,
     "grade_id": "ex_perceptron_x3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The `TODO`'s in the following code segments guide you through what has to be done.\n",
    "\n",
    "*Hint*: If you have problems with `np.arrays` (which usually have shapes like `(13,)`, thus with one degenerate dimension, either set the shapes manually (`my_np_array.shape = (13, 1)`) or use [np.atleast_2d](https://numpy.org/doc/stable/reference/generated/numpy.atleast_2d.html). Other useful functions might be\n",
    "* [lambda functions](https://docs.python.org/3/reference/expressions.html#lambda)\n",
    "* [np.hstack](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html?)\n",
    "* [np.append](https://numpy.org/doc/stable/reference/generated/numpy.append.html?highlight=append#numpy.append)\n",
    "* [np.apply_along_axis](https://numpy.org/doc/stable/reference/generated/numpy.apply_along_axis.html)\n",
    "* [try except](https://docs.python.org/3/tutorial/errors.html?highlight=try%20except#handling-exceptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805311bb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc89ce18dfbb66de462686b66e32785",
     "grade": true,
     "grade_id": "ex_perceptron_a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# TODO: Write the input activation (called net_input) and the output function (called out_fun).\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# TODO: Write a function generate_weights that generates N (= number of dimensions) + 1 (w_0) random weights.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86d8a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac43c1d488044d0678401226886fcffe",
     "grade": false,
     "grade_id": "ex_perceptron_a_assert",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Testing the perceptron with a concrete example ##\n",
    "####################################################\n",
    "\n",
    "# Dimensions for our test.\n",
    "dims = 12\n",
    "\n",
    "# Input is a row vector. (Shape is (1, 13).)\n",
    "D = np.hstack((1, rnd.rand(dims) - 0.5))\n",
    "\n",
    "# Weights are stored in a vector.\n",
    "W = generate_weights(dims)\n",
    "\n",
    "out = out_fun(net_input(D, W))\n",
    "\n",
    "assert out == 1 or out == 0, \"The output has to be either 1 or 0, but was {}\".format(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52982cdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa4afc40055da2eaa0d6618a5290a176",
     "grade": false,
     "grade_id": "ex_perceptron_b_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following `eval_network(t, D, W)` function is used to measure the performance of your perceptron for the upcoming task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_network(t, D, W):\n",
    "    \"\"\"\n",
    "    This function takes the trained weights of a perceptron\n",
    "    and the input data (D) as well as the correct target values (t)\n",
    "    and computes the overall error rate of the perceptron.\n",
    "    \"\"\"\n",
    "    error = 0.0\n",
    "    size = max(D.shape)\n",
    "    for i in range(size):\n",
    "        out = out_fun(net_input(D[i], W))\n",
    "        error = error + abs(t[i] - out)\n",
    "    # Normalize the error.\n",
    "    try:\n",
    "        return error.item(0) / size\n",
    "    except AttributeError:\n",
    "        return error / size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea78ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72a6ab6418ee68c0e2b6493287e47ed0",
     "grade": false,
     "grade_id": "ex_perceptron_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we will use the above defined functions to train the perceptron to one of the following logical functions: OR, NAND or NOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784efc2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5150a083a1438bef84abceadbde1f1fe",
     "grade": false,
     "grade_id": "ex_perceptron_b_plotting",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def function_to_learn(selector, function):\n",
    "    \"\"\"\n",
    "    Functional definitions for the perceptron to learn\n",
    "    Instantiates plots for visualization of the decision boundary\n",
    "    :param selector: selects which function to activate\n",
    "    :return function:\n",
    "    \"\"\"\n",
    "    plot_points = [[0,0],[0,1],[1,0],[1,1]]\n",
    "    plot_colors = []\n",
    "\n",
    "    for point in plot_points:\n",
    "        plot_colors.append(function(point[0], point[1]))\n",
    "    for color, point in enumerate(plot_points):\n",
    "        plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "    print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf37617",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9afa22c65c962fa99dbcc3f6de4f6349",
     "grade": true,
     "grade_id": "ex_perceptron_b_solution",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change this line to choose other operators:\n",
    "op = 'and'\n",
    "\n",
    "\n",
    "###################################################\n",
    "## Now we train our perceptron! [ML-07 Slide 33] ##\n",
    "###################################################\n",
    "\n",
    "# TODO: Write the update function (name it 'delta_fun')\n",
    "#       for the weights dependent on epsilon, the target,\n",
    "#       the output and the input vector.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Define suitable parameters for your problem.\n",
    "# Use the following names:\n",
    "#   ϵ: learning rate\n",
    "#   dims: dimensions\n",
    "#   training_size: the number of training samples\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Generate the weights (in a variable called W).\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Generate a matrix D of truthvalue pairs.\n",
    "# The shape should be (training_size, dims).\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Pad the input D with ones for the bias. The bias should always be\n",
    "# w_0, i. e. the first column of the data should be ones.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Learn one of the logical functions OR, NAND, NOR\n",
    "# (the lambda keyword is just a short way to define functions).\n",
    "log_operators = {\n",
    "    'and': lambda x1, x2: x1 and x2,\n",
    "    'or': lambda x1, x2: x1 or x2,\n",
    "    'nand': lambda x1, x2: not (x1 and x2),\n",
    "    'nor': lambda x1, x2: not (x1 or x2),\n",
    "    'xor': lambda x1, x2: (x1 and not x2) or (not x1 and x2)\n",
    "}\n",
    "\n",
    "log_operator = log_operators[op]\n",
    "function_to_learn(op, log_operator)\n",
    "\n",
    "row_operator = lambda row: log_operator(row[0], row[1])\n",
    "labels = np.apply_along_axis(row_operator, 1, D[:, 1:])\n",
    "\n",
    "epochs = 200    # Extra question: What effects do changes in the epochs \n",
    "samp_size = 5  #                 and sample sizes have on our training?\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Sample random from the training data.\n",
    "    for idx in rnd.choice(range(training_size), samp_size, replace=False):\n",
    "        y = out_fun(net_input(D[idx], W))\n",
    "        W += delta_fun(ϵ, labels[idx], y, D[idx])\n",
    "    # Plotting code    \n",
    "    y_point = (0, (-W[0] / W[2]))\n",
    "    x_point = ((-W[0] / W[1]), 0)\n",
    "    try:\n",
    "        slope = (y_point[1] - x_point[1]) / (y_point[0] - x_point[0]) # will not work if x and y intercepts are 0\n",
    "    except ZeroDivisionError:\n",
    "        print(\"X and Y intercepts are both zero.  Due to the way slope is calculated, this causes a division by zero.  Sorry.\")\n",
    "    y_out = lambda points: slope * points\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    plt.plot(x, y_out(x) + y_point[1], 'g--', linewidth=3, alpha=i/epochs +.2 if i/epochs +.2 < 1 else 1)\n",
    "    \n",
    "plt.ylim([-.2, 1.2])\n",
    "plt.xlim([-.2, 1.2])\n",
    "plt.title(\"Logic Perceptron (Blue=True)\")\n",
    "plt.xlabel(\"True(1) or False(0)\")\n",
    "plt.ylabel(\"True(1) or False(0)\")\n",
    "plt.show()\n",
    "\n",
    "# Print the overall performance of the Perceptron.\n",
    "print(\"Overall error of the Perceptron: {:.2%}\".format(eval_network(labels, D, W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c9f7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41c3950f547c0c8ad5fcb79d853afad3",
     "grade": false,
     "grade_id": "ex_perceptron_x4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Own Implementation\n",
    "\n",
    "Skip this if you already implemented the perceptron above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8938019",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fbb2c7f4f5962bbfdf4ebff78c8770f",
     "grade": true,
     "grade_id": "ex_perceptron_x4_solution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Space for complete own implementation\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c2914",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc043db7d846b1fb26c3a71990090e5b",
     "grade": false,
     "grade_id": "ex_sigmoid",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Sigmoid Activation & Backpropagation Delta Functions (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3488489",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be0847cfc9c07c4e664e010ef158e51c",
     "grade": false,
     "grade_id": "ex_sigmoid_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this exercise we are first going to take the derivative of a famous activation function - the sigmoid function:\n",
    "\n",
    "$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "This function is commonly used because of its nice analytical properties: Its image is the interval $(0,1)$, it is non-linear, strictly monotonous, continuous, differentiable and the derivative can be expressed in terms of the original function at the given point. This allows us to avoid redundant calculations. The sigmoid function is a special case of the more general *Logistic function* which can be found in many different fields: Biology, chemistry, economics, demography and recently most prominently: artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11879c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c41d026a37d947cfbea9aa95cbc334d",
     "grade": false,
     "grade_id": "ex_sigmoid_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(a)** Computing the derivative of the sigmoid activation function:\n",
    "\n",
    "Proof that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial t} = \\frac{1}{\\left({1 + e^{-t}}\\right)} \\cdot \\frac{e^{-t}}{\\left({1 + e^{-t}}\\right)}\n",
    "$$\n",
    "\n",
    "and that it can be rewritten as an expression in terms of $\\sigma(t)$ resulting in:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial t} = \\sigma(t) \\left(1 - \\sigma(t) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9572b16",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "445373493f313264124bc6f80dc229ad",
     "grade": true,
     "grade_id": "ex_sigmoid_a_solution",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06042d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c66971529e096fb63a1ee20729a94770",
     "grade": false,
     "grade_id": "ex_sigmoid_b_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(b)** Multilayer perceptrons (MLPs) can be regarded as a simple concatenation (and parallelization) of several perceptrons, each having a specified activation function $\\sigma$ and a set of weights $\\mathbf{w}_{ij}$. The idea that this can be done was discovered early after the invention of the perceptron, but people didn't really use it in practice because nobody really knew how to figure out the appropriate $\\mathbf{w}_{ij}$. The solution to this problem was the discovery of the backpropagation algorithm which consists of two steps: first propagating the input forward through the layers of the MLP and storing the intermediate results and then propagating the error backwards and adjusting the weights of the units accordingly.\n",
    "\n",
    "An updating rule for the output layer can be derived straightforward. The rules for the intermediate layers can be derived very similarly and only require a slight shift in perspective - the mathematics for that are however not in the standard toolkit so we are going to omit the calculations and refer you to the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4229ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a806cd44fa34afe95dc6039c980610d",
     "grade": false,
     "grade_id": "ex_sigmoid_b_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We take the (halvedleast) least-squares approach to derive the updating rule, i.e. we want to minimize the Loss function\n",
    "$$L = \\frac{1}{2}(y-t)^2$$\n",
    "where t is the given (true) label from the dataset and y is the (single) output produced by the MLP. To find the weights that minimize this expression we want to take the derivative of $L$ w.r.t. $\\mathbf{w}_{i}$ where we are now going to assume that the $\\mathbf{w}_{i}$ are the ones directly before the output layer:\n",
    "$$y = \\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)$$\n",
    "Calculate $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}}$.\n",
    "\n",
    "*Hint*: Start here if you don't know what to do: $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial \\mathbf{w}_{i}}$\n",
    "\n",
    "*Hint*: Remember that the derivative of the sigmoid activation function in its general form (from part **a**) is defined as: $\\frac{\\partial \\sigma}{\\partial t} = \\sigma(t) \\left(1 - \\sigma(t) \\right)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de4e9e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a3015d8c59e656fd7e39cf141e5ebab",
     "grade": true,
     "grade_id": "ex_sigmoid_b_solution",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12addf97",
   "metadata": {},
   "source": [
    "## Assignment 4: Training a MLP by hand (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6bbee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "217c13940de743710fbb8dc948e60cf5",
     "grade": false,
     "grade_id": "cell-9124cf5d238c3fe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Consider the following multilayer perceptron (notation from ML-7 slides 46ff), consisting of an input layer (layer $k=0$, with two neurons 1 & 2), a hidden layer ($k=1$ with two neurons 3 & 4) and an output layer ($k=2$ with two neurons 5 & 6).  The connection weights are given by the following image and connectivity matrix:\n",
    "\n",
    "![mlp-large.png](mlp-large.png)\n",
    "\n",
    "to\\from|1  |2  |3  |4  |5  |66\n",
    "-------|---|---|---|---|---|--\n",
    "1      |-  |-  |-  |-  |-  |-\n",
    "2      |-  |-  |-  |-  |-  |-\n",
    "3      |-3 |2  |-  |-  |-  |-\n",
    "4      |2  |1  |-  |-  |-  |-\n",
    "5      |-  |-  |4  |-1 |-  |-\n",
    "6      |-  |-  |-2 |0.5|-  |-\n",
    "\n",
    "The hidden layer (neurons 3 & 4) applies the [rectifier](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as activation function.\n",
    "$$\n",
    "    \\varphi(x)=\\max(0,x)\n",
    "$$\n",
    "\n",
    "The output layer (neurons 5 & 6) uses the sigmoid ([standard logistic function](https://en.wikipedia.org/wiki/Logistic_function), [Fermi function](https://en.wikipedia.org/wiki/Fermi%E2%80%93Dirac_statistics)) as activation function.\n",
    "$$\n",
    "    \\varphi(x)={\\frac {1}{1+e^{-x}}}\n",
    "$$\n",
    "\n",
    "To measure the error, following the lecture (ML-7 slide 48), the (halved) [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) is used, that is\n",
    "$$E[\\{w\\}](\\vec{t},\\vec{y}, ) = \n",
    "\\tfrac{1}{2}\\left\\|\\vec{t}-\\vec{y}\\right\\|_2^2 =\n",
    "\\frac{1}{2}\\sum_{i=1}^{d}(t_i-y_i)^2$$\n",
    "with $\\vec{y}$ being the values predicted by the network, $\\vec{t}$ the target value (\"ground truth\"), and $d=2$ the dimensionality of the output space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd600a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c32ac5030c1aead6cb4a5250bccfc15",
     "grade": false,
     "grade_id": "cell-a3e8b72921e20aa9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9f18f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "966880dbe8ccbbd0284a860f63e8928c",
     "grade": true,
     "grade_id": "cell-55cef3a66de71874",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463f5e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "811c118824dd4ae72c324a7abc5deaf4",
     "grade": false,
     "grade_id": "cell-bdb6bee338dc2a58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4affb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a00be825a28779ee12440ae05b35373",
     "grade": true,
     "grade_id": "cell-b44af9b40e98afb6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8c672",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c17721dcfd038a3fa65e7a6cd3031d47",
     "grade": false,
     "grade_id": "cell-46c4ab2f01383e11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3612fd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddd32d2a742e8e9bda11e5dcefa43ad7",
     "grade": true,
     "grade_id": "cell-ef70bf1de9d0bf22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d109406",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff2c41102ceb5e95093919d5afdf998c",
     "grade": false,
     "grade_id": "cell-aed85a1c01fe5ed5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252758b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d847abd4d77a5e08a05845fddf8108f",
     "grade": true,
     "grade_id": "cell-4f7cf68d9d7e6271",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
