{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6740d9be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (Summer 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45ab8d",
   "metadata": {},
   "source": [
    "## Practice Session 3: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42508d8",
   "metadata": {},
   "source": [
    "April, 30th 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76704e4f",
   "metadata": {},
   "source": [
    "Ulf Krumnack & Lukas Niehaus\n",
    "\n",
    "Institute of Cognitive Science,\n",
    "University of Osnabr√ºck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037d10c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230d7c5",
   "metadata": {},
   "source": [
    "* New sheet 03 \n",
    "* Discussion sheet 02\n",
    "    - Coin Flip\n",
    "* Discussion sheet 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9c52c",
   "metadata": {},
   "source": [
    "# EM-Algorithm\n",
    "\n",
    "This exercise aims to develop a more intuitive understanding of the Expectation-Maximization-algorithm (EM-algorithm). The exercise is based on [this paper](https://www.nature.com/articles/nbt1406).\n",
    "\n",
    "Explanation:\n",
    "Assume we'd have two coins A and B which have different probabilities of showing heads or tails (the coins are skewed!). For the dataset, someone picked one coin and flipped it ten times and recorded whether the coin showed heads or tails. This is repeated five times so we get five series of 10 coin tosses. The results are stored in a list like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# coin tosses in list: 1 represents Heads\n",
    "data = np.array([\n",
    "    [1, 0, 0, 0, 1, 1, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0, 1, 1, 1, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f37ba0",
   "metadata": {},
   "source": [
    "What we don't know is which series came from which coin. Also we don't know the coin's probabilities of showing heads. The EM-Algorithm serves us a procedure to estimate the probabilities of our coins. Your task is to implement the algorithm in order to assimilate the true probabilites of coins A and B. (To begin with, you may compute the probabilities by hand)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1a1a2",
   "metadata": {},
   "source": [
    "### 2. Make an initial guess for $\\theta_{A}$ and $\\theta_{B}$\n",
    "\n",
    "In order to start the algorithm you need to make an initial guess for $\\theta_{A}$ and $\\theta_{B}$. Later you can experiment with these variables in order to see how the algorithm behaves. $\\theta_{A}$ and $\\theta_{B}$ have to be different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your guess:\n",
    "theta0 = np.array((.6, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da9fb2",
   "metadata": {},
   "source": [
    "### 3. Implement the Expectation-Step\n",
    "Now that you have guessed initial probabilities, you can compute the probability for each series (10 tosses) whether it came from coin A or B. More precisely, you need to compute the two probabilities of a series being the outcome of your selected distribution with your guessed $\\theta_{A}$ or $\\theta_{B}$. Once you have these probabilities you can complete the expectation step! For each series, compute the amount of heads and tails you'd expect from coin A and B respectively. Store these heads and tails with respect to the coin you expect them from so you can use these numbers later on if you need them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f847ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def e_step(data, theta):\n",
    "    \"\"\"Apply the expectation step to create new weighted\n",
    "    training examples from a given example. The weights are\n",
    "    obtained as the conditional probability that `data` was\n",
    "    generated with coin A or coin B respectively, assuming\n",
    "    current model parameters `theta_A` and `theta_B`.\n",
    "    The two new weighted training examples are then created\n",
    "    by splitting the given data according the these weights.\n",
    "    \n",
    "    Return:\n",
    "        A 2x2 matrix, listing in each row the data for one coin,\n",
    "        that is the number of heads and tails attributed to that\n",
    "        coin.\n",
    "    \"\"\"\n",
    "    # get amount of heads and tails in the given data\n",
    "    heads, tails = data.sum(axis=-1), data.shape[-1] - data.sum(axis=-1)\n",
    "    if data.ndim == 2:\n",
    "        heads, tails = heads[:,np.newaxis], tails[:,np.newaxis]\n",
    "    # compute liklihood of observing the data\n",
    "    likelihood = (theta**heads) * ((1-theta)**tails)\n",
    "    # normalize to get weights (conditional probabilities)\n",
    "    weights = likelihood/likelihood.sum(axis=-1)[:,np.newaxis]\n",
    "    print(weights)\n",
    "    # distribute heads and tails according to weights\n",
    "    return np.stack((heads*weights, tails*weights), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e02fe",
   "metadata": {},
   "source": [
    "Run the E-step on the given data, to obtain a split version of the data (compare to table in figure 1 (b)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_data = e_step(data, theta=theta0)\n",
    "print(expected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116cc67",
   "metadata": {},
   "source": [
    "### 4. Implement the M-Step\n",
    "\n",
    "Now that you have access to all expected heads and tails from each coin for each series, you can update your initial guess of $\\theta_{A}$ and $\\theta_{B}$. The goal of doing so, is to find the distribution parameters that model the expected data best! Compute a new $\\theta_{A}$ and $\\theta_{B}$ so that you can start a new iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(expected_data):\n",
    "    \"\"\"Maximization step: choose new parameters to maximize\n",
    "    the likelihood of observing the aggregated data for both coins.\n",
    "    \n",
    "    Returns:\n",
    "        The new model parameters `theta_A` and `theta_B` derived\n",
    "        from the given split dataset.\n",
    "    \"\"\"\n",
    "    aggregated = expected_data.sum(axis=0)\n",
    "    return aggregated[:,0] / aggregated.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416651d4",
   "metadata": {},
   "source": [
    "### 5. Putting it all together\n",
    "\n",
    "You have implemented the E-Step and M-Step. You may have computed a new $\\theta_{A}$ and $\\theta_{B}$ which are closer to the true probabilties of coins A and B. The EM-Algorithm repeats these two steps until it finds $\\theta_{A}$ and $\\theta_{B}$ converging. Transform what you have implemented so far into a funtion which takes the coin_tosses list and your guesses for $\\theta_{A}$ and $\\theta_{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def EM_algorithm(data, theta):\n",
    "\n",
    "    # terminate if runs exceeds 50\n",
    "    for run in range(50):\n",
    "        # use E-step from above\n",
    "        expected_data = e_step(data, theta)\n",
    "        # use M-step from above\n",
    "        new_theta = m_step(expected_data)\n",
    "\n",
    "        print(f\"New Theta: A:{new_theta[0]:.2f}, B:{new_theta[1]:.2f}\")\n",
    "\n",
    "        # terminate if theta_A and theta_B converge\n",
    "        if np.allclose(new_theta, theta):\n",
    "            print(f\"Algorithm terminated after {run} runs.\")\n",
    "            return new_theta\n",
    "\n",
    "        theta = new_theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa138fd",
   "metadata": {},
   "source": [
    "Now run the EM algorithm on the given example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219d197",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "theta = EM_algorithm(data, theta=theta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687775a7",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
