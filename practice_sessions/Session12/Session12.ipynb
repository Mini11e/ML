{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a69388",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning (Summer 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9fcbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Practice Session 12: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48c27e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "June 22nd, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e72286",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Ulf Krumnack & Lukas Niehaus\n",
    "\n",
    "Institute of Cognitive Science,\n",
    "University of Osnabrück"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e1084",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5692a10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* Organization\n",
    "* Neural Network Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f73b7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Announcements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246346f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Final exam\n",
    "\n",
    "* You have to have completed the practice sheets succesfully to be admitted to the exam.\n",
    "\n",
    "* Time: Thursday, July 4th, 10:00 to 12:00 (lecture timeslot). Please be there at 10:00 sharp.\n",
    "\n",
    "* The exam will take 90 minutes.\n",
    "\n",
    "* Bring your own pen! No additional material (like calculators,\n",
    "  paper, cell phones, etc.) is allowed.\n",
    "\n",
    "* The exam will cover lectures ML-01 up to ML-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663828a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Redoing the exam\n",
    "\n",
    "* There will be a retry exam for those who fail the written exam. This will most probably be an oral exam, taking place later during the semester break (people who succeed in the written exam are not admitted to the retry exam).\n",
    "\n",
    "* If you cannot make it to the exam and can provide a medical certificate, you will be admitted to the retry exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8c402",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If you do not want to participate in the exam this year, you can participate next year without redoing the practice sheets. (However, if you participate in this year's exam and want to repeat the exam next year, you will have to redo the exercises!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d51f50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Registering for the final exam\n",
    "\n",
    "* If (and only if) you intend to participate in the exam, please register\n",
    "   in EXA latest by **Monday, July 1st, 2024**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58f9f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap / Q&A\n",
    "\n",
    "* there will be recap sessions on Tuesday and Wednesday\n",
    "* please post your questions in advance in the forum (there is dedicated area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd125ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network computation\n",
    "\n",
    "Consider the following multilayer perceptron (notation from ML-7 slides 46ff), consisting of an input layer (layer $k=0$, with two neurons 1 & 2), a hidden layer ($k=1$ with two neurons 3 & 4) and an output layer ($k=2$ with two neurons 5 & 6).\n",
    "\n",
    "![mlp-large.png](mlp-large.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d49c4",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9124cf5d238c3fe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "The connection weights are given by the following connectivity matrix:\n",
    "\n",
    "to\\from|1  |2  |3  |4  |5  |6\n",
    "-------|---|---|---|---|---|--\n",
    "1      |-  |-  |-  |-  |-  |-\n",
    "2      |-  |-  |-  |-  |-  |-\n",
    "3      |-3 |2  |-  |-  |-  |-\n",
    "4      |2  |1  |-  |-  |-  |-\n",
    "5      |-  |-  |4  |-1 |-  |-\n",
    "6      |-  |-  |-2 |0.5|-  |-\n",
    "\n",
    "The hidden layer (neurons 3 & 4) applies the [rectifier](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as activation function.\n",
    "$$\n",
    "    \\varphi(x)=\\max(0,x)\n",
    "$$\n",
    "\n",
    "The output layer (neurons 5 & 6) uses the sigmoid ([standard logistic function](https://en.wikipedia.org/wiki/Logistic_function), [Fermi function](https://en.wikipedia.org/wiki/Fermi%E2%80%93Dirac_statistics)) as activation function.\n",
    "$$\n",
    "    \\varphi(x)={\\frac {1}{1+e^{-x}}}\n",
    "$$\n",
    "\n",
    "To measure the error, following the lecture (ML-7 slide 48), the (halved) [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) is used, that is\n",
    "$$E[\\{w\\}](\\vec{t},\\vec{y}, ) = \n",
    "\\tfrac{1}{2}\\left\\|\\vec{t}-\\vec{y}\\right\\|_2^2 =\n",
    "\\frac{1}{2}\\sum_{i=1}^{d}(t_i-y_i)^2$$\n",
    "with $\\vec{y}$ being the values predicted by the network, $\\vec{t}$ the target value (\"ground truth\"), and $d=2$ the dimensionality of the output space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a81ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Manual solution (lecture style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd769d3a",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3e8b72921e20aa9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d96a3a",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-55cef3a66de71874",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computing the weighted input for layer 1:\n",
    "\\begin{align*}\n",
    "  s_3(1) &= \\sum_{j=1}^{2} w_{3j}(1,0)o_j(0) && = -3\\cdot 1 + 2\\cdot 2 = -3 + 4 && = 1 \\\\\n",
    "  s_4(1) &= \\sum_{j=1}^{2} w_{4j}(1,0)o_j(0) && = 2\\cdot 1 + 1\\cdot 2 = 2+2 && = 4\n",
    "\\end{align*}\n",
    "The outputs of layer 1 are hence:\n",
    "\\begin{align*}\n",
    "  o_3(1) &= \\varphi_{R}(s_3(1)) &&= \\max(0,1) && = 1 \\\\\n",
    "  o_4(1) &= \\varphi_{R}(s_4(1)) &&= \\max(0,4) && = 4 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de70f28",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-55cef3a66de71874",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For layer 2 we then get the following weighted input:\n",
    "\\begin{align*}\n",
    "  s_5(2) &= \\sum_{j=3}^{4} w_{5j}(2,1)o_j(1) &&= 4\\cdot 1 - 1\\cdot 4 &&= 0 \\\\\n",
    "  s_6(2) &= \\sum_{j=3}^{4} w_{6j}(2,1)o_j(1) &&= -2\\cdot 1 + 0.5\\cdot 4 && = 0 \n",
    "\\end{align*}\n",
    "The outputs of layer 2 (which also are the network output) ar\n",
    "\\begin{align*}\n",
    "  o_5(2) &= \\varphi_{S}(s_5(2)) = \\sigma(0) = 0.5 \\\\\n",
    "  o_6(2) &= \\varphi_{S}(s_6(2)) = \\sigma(0) = 0.5\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed580e9",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdb6bee338dc2a58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{y}_{\\text{true}}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14230bb8",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b44af9b40e98afb6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The loss value is given by the (halved) mean squared error between the network output $\\vec{y}=(0.5, 0.5)$ and the target value $\\vec{t}=(1.0,0.0)$:\n",
    "\\begin{align*}\n",
    "  E[{w}](\\vec{t},\\vec{y}) \n",
    "  & = \\tfrac12\\|\\vec{t}-\\vec{y}\\|^2\\\\\n",
    "  & = \\tfrac12\\sum_{i=1}^{2}(t_i-y_i)^2\\\\\n",
    "  & = \\tfrac12\\left[(1-0.5)^2 + (0-0.5)^2\\right]\\\\\n",
    "  & = \\tfrac12\\left[0.25 + 0.25\\right]\\\\\n",
    "  & = 0.25\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6466082",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46c4ab2f01383e11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f47406",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ef70bf1de9d0bf22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the output layer we get the following error signal:\n",
    "\\begin{align*}\n",
    "  \\delta_5(2) & = \\varphi_{S}'(s_5)\\cdot (t_1-y_1(\\vec{x})) && = \\sigma'(0)\\cdot(1.0-0.5) && = \\sigma(0)(1-\\sigma(0))\\cdot 0.5 && = .5 \\cdot .5 \\cdot .5 && = 0.125 \\\\\n",
    "  \\delta_6(2) & = \\varphi_{S}'(s_6)\\cdot (t_2-y_2(\\vec{x})) && = \\sigma'(0)\\cdot(0.0-0.5) && = \\sigma(0)(1-\\sigma(0))\\cdot -0.5 && = .5 \\cdot .5 \\cdot -.5 && = -0.125\n",
    "\\end{align*}\n",
    "From this we can obtain the second layer weight gradients:\n",
    "\\begin{align*}\n",
    "  -\\partial E/\\partial w_{53}(2,1) & = \\delta_5(2)o_3(1) &&= 0.125 \\cdot 1 &&= 0.125 \\\\\n",
    "  -\\partial E/\\partial w_{54}(2,1) & = \\delta_5(2)o_4(1) &&= 0.125 \\cdot 4 &&= 0.5 \\\\\n",
    "  -\\partial E/\\partial w_{63}(2,1) & = \\delta_6(2)o_3(1) && = -0.125 \\cdot 1 &&= -0.125 \\\\\n",
    "  -\\partial E/\\partial w_{64}(2,1) & = \\delta_6(2)o_4(1) && = -0.125 \\cdot 4 &&= -0.5 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cea327",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ef70bf1de9d0bf22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "For layer 1 the error signal is:\n",
    "\\begin{align*}\n",
    "  \\delta_3(1) &= \\varphi_{R}'(s_3(1))\\cdot\\sum_{j=5}^{6}w_{j3}(2,1)\\delta_j(2) && = \\varphi_{R}'(1)\\cdot\\left[4\\cdot 0.125 + -2\\cdot-0.125\\right] &&= 1\\cdot [0.5 + 0.25] &&= 0.75\\\\\n",
    "  \\delta_4(1) &= \\varphi_{R}'(s_4(1))\\cdot\\sum_{j=5}^{6}w_{j4}(2,1)\\delta_j(2) && = \\varphi_{R}'(4)\\cdot\\left[-1\\cdot 0.125 + 0.5\\cdot-0.125\\right] &&= 1\\cdot [-0.125-0.0625] &&= -0.1875\n",
    "\\end{align*}\n",
    "yielding the following gradients:\n",
    "\\begin{align*}\n",
    "  -\\partial E/\\partial w_{31}(1,0) &= \\delta_3(1)o_1(0) &&= 0.75 \\cdot 1.0 && = 0.75 \\\\\n",
    "  -\\partial E/\\partial w_{32}(1,0) &= \\delta_3(1)o_2(0) &&= 0.75 \\cdot 2.0 && = 1.5 \\\\\n",
    "  -\\partial E/\\partial w_{41}(1,0) &= \\delta_4(1)o_1(0) &&= -0.1875 \\cdot 1.0 &&= -0.1875 \\\\\n",
    "  -\\partial E/\\partial w_{42}(1,0) &= \\delta_4(1)o_2(0) &&= -0.1875 \\cdot 2.0 &&= -0.375\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d4af1",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aed85a1c01fe5ed5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cf7ed",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f7cf68d9d7e6271",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The adaptation rule is\n",
    "\n",
    "$$ w_{ji}(k+1,k) \\mapsto w_{ji}(k+1,k) + \\Delta w_{ji}(k+1,k)$$\n",
    "\n",
    "with the update term\n",
    "\n",
    "$$ \\Delta w_{ji}(k+1,k) = - \\varepsilon \\partial E/\\partial w_{ji}(k+1,k) = \\varepsilon \\delta_j(k+1)o_j(k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2b7ad",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f7cf68d9d7e6271",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the first layer that is\n",
    "\\begin{align*}\n",
    "  w_{31}(1,0): &&-3 \\mapsto & -3 + 1\\cdot 0.75   && = -2.25 \\\\\n",
    "  w_{32}(1,0): && 2 \\mapsto &  2 + 1\\cdot1.5    && = 3.5 \\\\\n",
    "  w_{41}(1,0): && 2 \\mapsto &  2 + 1\\cdot(-0.1875) && = 1.8125 \\\\\n",
    "  w_{42}(1,0): && 1 \\mapsto &  1 + 1\\cdot(-0.375)  && = 0.625\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ab7f0",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f7cf68d9d7e6271",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and for the second layer\n",
    "\\begin{align*}\n",
    "  w_{53}(2,1): &&  4 \\mapsto &    4 + 1\\cdot 0.125    && = 4.125 \\\\\n",
    "  w_{54}(2,1): && -1 \\mapsto &   -1 + 1\\cdot 0.5      && = 0.5 \\\\\n",
    "  w_{63}(2,1): && -2 \\mapsto &   -2 + 1\\cdot (-0.125) && = 2.125 \\\\\n",
    "  w_{64}(2,1): && 0.5 \\mapsto & 0.5 + 1\\cdot (-0.5)   && = 0.0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b1f24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Manual Solution (vector calculus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86661c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01e4ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Computing the weighted input for layer 1:\n",
    "\\begin{align*}\n",
    "  \\vec{s}^{(1)} &= \\mathbf{W}^{(1)}\\cdot \\vec{o}^{(0)} = \n",
    "  \\begin{pmatrix} -3 & 2 \\\\ 2 & 1 \\end{pmatrix}\\cdot\n",
    "  \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n",
    "  = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{o}^{(1)} &= \\mathbf{\\varphi}_{R}(\\vec{s}^{(1)}) =\n",
    "  \\begin{pmatrix} \\varphi_R(1) \\\\ \\varphi_R(4) \\end{pmatrix} =\n",
    "  \\begin{pmatrix} \\max(1,0) \\\\ \\max(4,0) \\end{pmatrix} =\n",
    "  \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a5a6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Computing the weighted input for layer 1:\n",
    "\\begin{align*}\n",
    "  \\vec{s}^{(2)} &= \\mathbf{W}^{(2)}\\cdot \\vec{o}^{(1)} = \n",
    "  \\begin{pmatrix} 4 & -1 \\\\ -2 & 0.5 \\end{pmatrix}\\cdot\n",
    "  \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\n",
    "  = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{o}^{(2)} &= \\mathbf{\\varphi}_{S}(\\vec{s}^{(2)}) =\n",
    "  \\begin{pmatrix} \\varphi_S(0) \\\\ \\varphi_S(0) \\end{pmatrix} =\n",
    "  \\begin{pmatrix} \\sigma(0) \\\\ \\sigma(0) \\end{pmatrix} =\n",
    "  \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dcfa89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a17de0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "The loss value is given by the (halved) mean squared error between the network output $\\vec{y}=(0.5, 0.5)$ and the target value $\\vec{t}=(1.0,0.0)$:\n",
    "\\begin{align*}\n",
    "  E[\\{w\\}](\\vec{t},\\vec{y}) \n",
    "  & = \\tfrac12\\|\\vec{t}-\\vec{y}\\|^2\\\\\n",
    "  & = \\tfrac12\\sum_{i=1}^{2}(t_i-y_i)^2\\\\\n",
    "  & = \\tfrac12\\left[(1-0.5)^2 + (0-0.5)^2\\right]\\\\\n",
    "  & = \\tfrac12\\left[0.25 + 0.25\\right]\\\\\n",
    "  & = 0.25\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb89c2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487418f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Start with the error signal for the second (i.e. output) layer:\n",
    "\\begin{align*}\n",
    "  \\vec{\\delta}^{(2)} &= \n",
    "  \\frac{-\\partial E}{\\partial \\vec{s}^{(2)}} =\n",
    "  \\frac{\\partial \\vec{o}^{(2)}}{\\partial \\vec{s}^{(2)}} \\cdot\n",
    "  \\frac{-\\partial E}{\\partial \\vec{o}^{(2)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35bf1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "The required gradients are:\n",
    "\\begin{align*}\n",
    "  \\frac{-\\partial E}{\\partial \\vec{o}^{(2)}} &= \n",
    "  \\begin{pmatrix}\n",
    "    -\\frac{\\mathrm{d}E}{\\mathrm{d}o^{(2)}_1} \\\\\n",
    "    -\\frac{\\mathrm{d}E}{\\mathrm{d}o^{(2)}_2}\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    t_1-o^{(2)}_1 \\\\ t_2-o^{(2)}_2\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    0.5 \\\\ -0.5\n",
    "  \\end{pmatrix}\n",
    "  \\\\\n",
    "  \\frac{\\partial \\vec{o}^{(2)}}{\\partial \\vec{s}^{(2)}} &= \n",
    "  \\begin{pmatrix}\n",
    "    \\frac{\\mathrm{d}o^{(2)}_1}{\\mathrm{d}s^{(2)}_1} &\n",
    "    \\frac{\\mathrm{d}o^{(2)}_2}{\\mathrm{d}s^{(2)}_1} \\\\\n",
    "    \\frac{\\mathrm{d}o^{(2)}_1}{\\mathrm{d}s^{(2)}_2} &\n",
    "    \\frac{\\mathrm{d}o^{(2)}_2}{\\mathrm{d}s^{(2)}_2}\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    \\sigma'(s^{(2)}_1) & 0 \\\\\n",
    "    0 &  \\sigma'(s^{(2)}_2)\n",
    "  \\end{pmatrix}\n",
    "  \\\\\n",
    "  &=\n",
    "  \\begin{pmatrix}\n",
    "    \\sigma(s^{(2)}_1)(1-\\sigma(s^{(2)}_1)) & 0 \\\\\n",
    "    0 &  \\sigma(s^{(2)}_1)(1-\\sigma(s^{(2)}_1))\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    0.25 & 0 \\\\\n",
    "    0 &  0.25\n",
    "  \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ec527",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "so the error signals is:\n",
    "\\begin{align*}\n",
    "  \\vec{\\delta}^{(2)} &= \n",
    "  \\frac{\\partial \\vec{o}^{(2)}}{\\partial \\vec{s}^{(2)}} \\cdot\n",
    "  \\frac{-\\partial E}{\\partial \\vec{o}^{(2)}}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    0.25 & 0 \\\\\n",
    "    0 &  0.25\n",
    "  \\end{pmatrix}\n",
    "  \\cdot\n",
    "  \\begin{pmatrix}\n",
    "    0.5 \\\\ -0.5\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    0.125 \\\\ -0.125\n",
    "  \\end{pmatrix}  \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c8bff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "This allows to compute the gradient with respect to the second layer weights $\\mathbf{W}^{(2)}$:\n",
    "\\begin{align*}\n",
    "  \\frac{-\\partial E}{\\partial \\mathbf{W}^{(2)}}\n",
    "  &=\n",
    "  \\frac{-\\partial E}{\\partial \\vec{s}^{(2)}}\n",
    "  \\frac{\\partial \\vec{s}^{(2)}}{\\partial \\mathbf{W}^{(2)}}\n",
    "  \\\\\n",
    "  &=\n",
    "  \\vec{\\delta}^{(2)} \\cdot (\\vec{o}^{(1)})^{T}\n",
    "  \\\\\n",
    "  & =\n",
    "  \\begin{pmatrix}\n",
    "    0.125 \\\\ -0.125\n",
    "  \\end{pmatrix}  \n",
    "  \\cdot\n",
    "  \\begin{pmatrix}\n",
    "    1 & 4\n",
    "  \\end{pmatrix}  \n",
    "  \\\\\n",
    "  &= \n",
    "  \\begin{pmatrix}\n",
    "    0.125 & 0.5 \\\\\n",
    "    -0.125 & -0.5\n",
    "  \\end{pmatrix}  \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f871134",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "For the first layer the error signal is:\n",
    "\\begin{align*}\n",
    "  \\vec{\\delta}^{(1)} &= \n",
    "  \\frac{-\\partial E}{\\partial \\vec{s}^{(1)}} =\n",
    "  \\frac{\\partial \\vec{o}^{(1)}}{\\partial \\vec{s}^{(1)}} \\cdot\n",
    "  \\frac{\\partial \\vec{s}^{(2)}}{\\partial \\vec{o}^{(1)}} \\cdot\n",
    "  \\frac{-\\partial E}{\\partial \\vec{s}^{(2)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c1185",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "The gradients are:\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial \\vec{s}^{(2)}}{\\partial \\vec{o}^{(1)}} &= \n",
    "  \\begin{pmatrix}\n",
    "    \\frac{\\mathrm{d}s^{(2)}_1}{\\mathrm{d}o^{(1)}_1} &\n",
    "    \\frac{\\mathrm{d}s^{(2)}_2}{\\mathrm{d}o^{(1)}_1} \\\\\n",
    "    \\frac{\\mathrm{d}s^{(2)}_1}{\\mathrm{d}o^{(1)}_2} &\n",
    "    \\frac{\\mathrm{d}s^{(2)}_2}{\\mathrm{d}o^{(1)}_2} \n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    w^{(2)}_{11} & w^{(2)}_{21} \\\\\n",
    "    w^{(2)}_{12} & w^{(2)}_{12}\n",
    "  \\end{pmatrix}\n",
    "  = (\\mathbf{W}^{(2)})^T =\n",
    "  \\begin{pmatrix}\n",
    "     4 &  -2\\\\\n",
    "     -1 &  0.5\n",
    "  \\end{pmatrix}\n",
    "  \\\\\n",
    "  \\frac{\\partial \\vec{o}^{(1)}}{\\partial \\vec{s}^{(1)}} &= \n",
    "  \\begin{pmatrix}\n",
    "    \\frac{\\mathrm{d}o^{(1)}_1}{\\mathrm{d}s^{(1)}_1} &\n",
    "    \\frac{\\mathrm{d}o^{(1)}_2}{\\mathrm{d}s^{(1)}_1} \\\\\n",
    "    \\frac{\\mathrm{d}o^{(1)}_1}{\\mathrm{d}s^{(1)}_2} &\n",
    "    \\frac{\\mathrm{d}o^{(1)}_2}{\\mathrm{d}s^{(1)}_2}\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    \\varphi_R'(s^{(1)}_1) & 0 \\\\\n",
    "    0 & \\varphi_R'(s^{(1)}_2)\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "  \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a5142",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "so the error signal amounts to:\n",
    "\\begin{align*}\n",
    "  \\vec{\\delta}^{(1)} &= \n",
    "  \\begin{pmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "  \\end{pmatrix}\n",
    "  \\cdot\n",
    "  \\begin{pmatrix}\n",
    "     4 &  -2\\\\\n",
    "     -1 &  0.5\n",
    "  \\end{pmatrix}\n",
    "  \\cdot\n",
    "  \\begin{pmatrix}\n",
    "    0.125 \\\\ -0.125\n",
    "  \\end{pmatrix}  \n",
    "  =   \n",
    "  \\begin{pmatrix}\n",
    "    0.75 \\\\ -0.1875\n",
    "  \\end{pmatrix}  \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01ed3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "This yields the following gradient for the $\\mathbf{W}^{(1)}$, the weights for layer 1:\n",
    "\\begin{align*}\n",
    "  \\frac{-\\partial E}{\\partial \\mathbf{W}^{(1)}}\n",
    "  &=\n",
    "  \\frac{-\\partial E}{\\partial \\vec{s}^{(1)}}\n",
    "  \\frac{\\partial \\vec{s}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\n",
    "  \\\\\n",
    "  &=\n",
    "  \\vec{\\delta}^{(1)} \\cdot (\\vec{o}^{(0)})^{T}\n",
    "  \\\\\n",
    "  & =\n",
    "  \\begin{pmatrix}\n",
    "    0.75 \\\\ -0.1875\n",
    "  \\end{pmatrix}  \n",
    "  \\cdot\n",
    "  \\begin{pmatrix}\n",
    "    1 & 2\n",
    "  \\end{pmatrix}  \n",
    "  \\\\\n",
    "  &= \n",
    "  \\begin{pmatrix}\n",
    "    0.75 & 1.5 \\\\\n",
    "    -0.1875 & -0.375\n",
    "  \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a94a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168904ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "\\begin{align*}\n",
    "  \\mathbf{W}^{(1)}\n",
    "  \\mapsto & \\mathbf{W}^{(1)}\n",
    "    + \\varepsilon \\frac{-\\partial E}{\\partial\\mathbf{W}^{(1)}}\n",
    "  = \n",
    "  \\begin{pmatrix}\n",
    "    -3 & 2 \\\\\n",
    "    2 & 1\n",
    "  \\end{pmatrix}\n",
    "  + 1\\cdot\n",
    "  \\begin{pmatrix}\n",
    "    0.75 & 1.5 \\\\\n",
    "    -0.1875 & -0.375\n",
    "  \\end{pmatrix}\n",
    "  \\\\ & =\n",
    "  \\begin{pmatrix}\n",
    "    -2.25 & 3.5 \\\\\n",
    "    1.8125 & 0.625\n",
    "  \\end{pmatrix}\n",
    "  \\\\\n",
    "  \\mathbf{W}^{(2)}\n",
    "  \\mapsto & \\mathbf{W}^{(2)}\n",
    "    + \\varepsilon \\frac{-\\partial E}{\\partial\\mathbf{W}^{(2)}}\n",
    "  = \n",
    "  \\begin{pmatrix}\n",
    "    4 & -1 \\\\\n",
    "    -2 & 0.5\n",
    "  \\end{pmatrix}\n",
    "  + 1\\cdot\n",
    "  \\begin{pmatrix}\n",
    "    0.125 & 0.5 \\\\\n",
    "    -0.125 & -0.5\n",
    "  \\end{pmatrix}\n",
    "  \\\\ & =\n",
    "  \\begin{pmatrix}\n",
    "    4.125 & -0.5 \\\\\n",
    "    -2.125 & 0.0\n",
    "  \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cce54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numpy solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ec361",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.array([[-3., 2.], [ 2., 1.]])\n",
    "W2 = np.array([[4., -1.], [-2., .5]])\n",
    "\n",
    "relu = lambda x: np.maximum(1,x)\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264e6a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf694f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[1., 2.]]).T\n",
    "\n",
    "s1 = W1 @ x\n",
    "o1 = relu(s1)\n",
    "\n",
    "print(\"s1:\", s1.T)\n",
    "print(\"o1:\", o1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e777bc66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s2 = W2 @ o1\n",
    "o2 = sigmoid(s2)\n",
    "\n",
    "print(\"s2:\", s2.T)\n",
    "print(\"o2:\", o2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0398b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf3c9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "t = np.array([[1., 0.]]).T\n",
    "# error_func = lambda x, t: 0.5 * np.linalg.norm(x-t)**2\n",
    "error_func = lambda x, t: ((x-t)**2).sum() / 2\n",
    "\n",
    "E = error_func(o2, t)\n",
    "print(\"Error:\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77666e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ab95e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid_derivative = lambda x: sigmoid(x) * (1-sigmoid(x))\n",
    "relu_derivative = lambda x: x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70decf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "δ2 = sigmoid_derivative(s2) * (t - o2)\n",
    "print(\"δ2:\", δ2.T)\n",
    "\n",
    "δ1 = relu_derivative(s1) * W2.T @ δ2\n",
    "print(\"δ1:\", δ1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8e3b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af414891",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ε = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baaaff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ΔW1 = ε * δ1 @ x.T\n",
    "print(\"ΔW1:\\n\", ΔW1)\n",
    "\n",
    "ΔW2 = ε * δ2 @ o1.T\n",
    "print(\"\\nΔW2:\\n\", ΔW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab28c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "W1 += ΔW1\n",
    "print(\"Updated W1:\\n\", W1)\n",
    "\n",
    "W2 += ΔW2\n",
    "print(\"\\nUpdated W2:\\n\", W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfe84b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "assert np.array_equal(W1, np.array([[-2.2500,  3.5000], [ 1.8125,  0.6250]]))\n",
    "assert np.array_equal(W2, np.array([[ 4.1250, -0.5000], [-2.1250,  0.0000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b69381",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Torch solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a6b74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e72b2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(2, 2, bias=False)\n",
    "layer2 = nn.Linear(2, 2, bias=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    layer1.weight.copy_(torch.tensor([[-3., 2.], [ 2., 1.]]))\n",
    "    layer2.weight.copy_(torch.tensor([[4., -1.], [-2., .5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b5893",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e89ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a20566",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s1 = layer1(x)\n",
    "o1 = nn.ReLU()(s1)\n",
    "\n",
    "print(\"s1 =\", s1)\n",
    "print(\"o1 =\", o1)\n",
    "assert torch.equal(o1, torch.tensor([[1., 4.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa9a64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s2 = layer2(o1)\n",
    "o2 = nn.Sigmoid()(s2)\n",
    "print(\"s2 =\", s2)\n",
    "print(\"o2 =\", o2)\n",
    "assert torch.equal(o2, torch.tensor([[0.5, 0.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfec10d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d23dc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss(reduction=\"sum\")\n",
    "t = torch.tensor([[1., 0.]])\n",
    "error = loss_func(o2, t) / 2\n",
    "\n",
    "print(\"\\ntarget output (ground truth):\", t)\n",
    "print(\"predicted output:\", o2)\n",
    "print(\"error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587e850",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cad63e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "negative_error = -error\n",
    "negative_error.backward()\n",
    "\n",
    "print(\"\\nGradients:\")\n",
    "print(\"dL/dW2\\n\", layer2.weight.grad)\n",
    "print(\"dL/dW1\\n\", layer1.weight.grad)\n",
    "\n",
    "assert torch.equal(layer2.weight.grad, torch.tensor([[0.1250, 0.5000], [ -0.1250,  -0.5000]]))\n",
    "assert torch.equal(layer1.weight.grad, torch.tensor([[0.7500, 1.5000], [ -0.1875,  -0.3750]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01e7ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e311cf8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ε = 1.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    layer1.weight.copy_(layer1.weight + ε * layer1.weight.grad)\n",
    "    layer2.weight.copy_(layer2.weight + ε * layer2.weight.grad)\n",
    "\n",
    "print(\"\\nupdated weights:\")\n",
    "print(\"W1\", layer1.weight)\n",
    "print(\"W2\", layer2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94285f99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert torch.equal(layer1.weight, torch.tensor([[-2.2500,  3.5000], [ 1.8125,  0.6250]]))\n",
    "assert torch.equal(layer2.weight, torch.tensor([[ 4.1250, -0.5000], [-2.1250,  0.0000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e812b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More \"torchish\" formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170b9d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2, 2, bias=False),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718f13b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model[0].weight.copy_(torch.tensor([[-3., 2.], [ 2., 1.]]))\n",
    "    model[2].weight.copy_(torch.tensor([[4., -1.], [-2., .5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afa164",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "x = torch.tensor([[1.,2.]])\n",
    "y_true = torch.tensor([[1., 0.]])\n",
    "y_pred = model(x)\n",
    "error = loss_func(y_pred, y_true) / 2\n",
    "\n",
    "print(\"target output (ground truth):\", y_true)\n",
    "print(\"predicted output:\", y_pred)\n",
    "print(\"error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afe3a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "error.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nupdated weights:\")\n",
    "print(\"W1\", layer1.weight)\n",
    "print(\"W2\", layer2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf260950",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert torch.equal(layer1.weight, torch.tensor([[-2.2500,  3.5000], [ 1.8125,  0.6250]]))\n",
    "assert torch.equal(layer2.weight, torch.tensor([[ 4.1250, -0.5000], [-2.1250,  0.0000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc108738",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JAX (numpy like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324db05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "W1 = jnp.array([[-3., 2.], [ 2., 1.]])\n",
    "W2 = jnp.array([[4., -1.], [-2., .5]])\n",
    "\n",
    "relu = lambda x: jnp.maximum(0,x)\n",
    "sigmoid = lambda x: 1/(1+jnp.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368500eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c59825",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = jnp.array([[1., 2.]]).T\n",
    "\n",
    "s1 = W1 @ x\n",
    "o1 = relu(s1)\n",
    "\n",
    "print(\"s1:\", s1.T)\n",
    "print(\"o1:\", o1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bed02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s2 = W2 @ o1\n",
    "o2 = sigmoid(s2)\n",
    "\n",
    "print(\"s2:\", s2.T)\n",
    "print(\"o2:\", o2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77fdca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c1c54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Error computation:\n",
    "t = jnp.array([[1., 0.]]).T\n",
    "#error_func = lambda x, t: 0.5 * jnp.linalg.norm(x-t)**2\n",
    "error_func = lambda x, t: ((x-t)**2).sum() / 2\n",
    "\n",
    "E = error_func(o2, t)\n",
    "print(\"Error:\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db973aca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae490e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid_derivative = lambda x: sigmoid(x) * (1-sigmoid(x))\n",
    "relu_derivative = lambda x: x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9f65a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "δ2 = sigmoid_derivative(s2) * (t - o2)\n",
    "print(\"δ2:\", δ2.T)\n",
    "\n",
    "δ1 = relu_derivative(s1) * W2.T @ δ2\n",
    "print(\"δ1:\", δ1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991e465",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946b8f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ε = 1.0\n",
    "\n",
    "ΔW1 = ε * δ1 @ x.T\n",
    "print(\"ΔW1:\\n\", ΔW1)\n",
    "\n",
    "ΔW2 = ε * δ2 @ o1.T\n",
    "print(\"\\nΔW2:\\n\", ΔW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeac639",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W1 += ΔW1\n",
    "print(\"Updated W1:\\n\", W1)\n",
    "\n",
    "W2 += ΔW2\n",
    "print(\"\\nUpdated W2:\\n\", W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338dead",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert jnp.array_equal(W1, jnp.array([[-2.2500,  3.5000], [ 1.8125,  0.6250]]))\n",
    "assert jnp.array_equal(W2, jnp.array([[ 4.1250, -0.5000], [-2.1250,  0.0000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ad44c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The \"jaxish\" version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495cf7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda432f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "relu = lambda x: jnp.maximum(0,x)\n",
    "sigmoid = lambda x: 1/(1+jnp.exp(-x))\n",
    "\n",
    "layer1 = lambda W, x: relu(W @ x)\n",
    "layer2 = lambda W, x: sigmoid(W @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131ec1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe2a92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W1 = jnp.array([[-3., 2.], [ 2., 1.]])\n",
    "W2 = jnp.array([[4., -1.], [-2., .5]])\n",
    "x = jnp.array([[1., 2.]]).T\n",
    "\n",
    "print(\"o1:\", layer1(W1, x).T)\n",
    "print(\"o2:\", layer2(W2, layer1(W1, x)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04dacc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d03e3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = jnp.array([[1., 0.]]).T\n",
    "\n",
    "model = lambda W, x: layer2(W[1], layer1(W[0], x))\n",
    "\n",
    "error_func = lambda x, t: ((x-t)**2).sum() / 2\n",
    "neg_error = lambda W, x, t: -error_func(model(W, x), t)\n",
    "\n",
    "print(\"negative error:\", neg_error((W1, W2), x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55bd1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5abdae0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_error = jax.grad(neg_error)\n",
    "W1_grad, W2_grad = grad_error((W1, W2), x, t)\n",
    "\n",
    "print(\"-dE/dW2:\\n\", W2_grad)\n",
    "print(\"-dE/dW1:\\n\", W1_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369aa817",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert jnp.array_equal(W2_grad, [[0.1250, 0.5000], [ -0.1250,  -0.5000]])\n",
    "assert jnp.array_equal(W1_grad, [[0.7500, 1.5000], [ -0.1875,  -0.3750]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdffd54c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ecb3c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ε = 1.0\n",
    "\n",
    "W1 += ε * W1_grad\n",
    "print(\"Updated W1:\\n\", W1)\n",
    "\n",
    "W2 += ε * W2_grad\n",
    "print(\"\\nUpdated W2:\\n\", W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd401e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert jnp.array_equal(W1, jnp.array([[-2.2500,  3.5000], [ 1.8125,  0.6250]]))\n",
    "assert jnp.array_equal(W2, jnp.array([[ 4.1250, -0.5000], [-2.1250,  0.0000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9f485",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Flax implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98a7bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dad48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "W1 = jnp.array([[-3., 2.], [ 2., 1.]])\n",
    "W2 = jnp.array([[4., -1.], [-2., .5]])\n",
    "\n",
    "net = nn.Sequential([\n",
    "    nn.Dense(features=2, use_bias=False,\n",
    "             kernel_init=nn.initializers.constant(W1.T)),\n",
    "    nn.relu,\n",
    "    nn.Dense(features=2, use_bias=False,\n",
    "             kernel_init=nn.initializers.constant(W2.T)),\n",
    "    nn.sigmoid\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10089293",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(net.tabulate(jax.random.PRNGKey(0), jnp.ones((2, ))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467acc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(a)** Assume the input $\\vec{x} = (1.0, 2.0)$ is given to the network (notice that in contrast to the lecture slides, we only consider a single input vector here, instead of a full dataset). Compute the weighted input $s_i(k)$ as well as the output values $o_i(k)$ for all neurons in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f5adc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = jnp.array([1., 2.])\n",
    "\n",
    "params = net.init(jax.random.PRNGKey(0), jnp.ones((2,)))\n",
    "net.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1eacad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(b)** Compute the loss value for the predicted output, assuming that the target value is $\\vec{t}=(1.0, 0.0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cea001",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = jnp.array([1., 0.])\n",
    "\n",
    "error_func = lambda x, t: ((x-t)**2).sum() / 2\n",
    "\n",
    "def neg_error(params, x, t):\n",
    "    return -error_func(net.apply(params, x), t)\n",
    "\n",
    "loss_grad_fn = jax.value_and_grad(neg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347cbe7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_val, grads = loss_grad_fn(params, x, t)\n",
    "print(loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a94906",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(c)** Now perform backpropagation: compute the errror signals $\\delta_i(k)$ and the partial derivatives $\\partial E/\\partial w_{ik}$ for the weights in layer $k=2$ and $k=1$ (for layer $k=1$ remember to use the ReLU function, which has a quite simple derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00922a17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3200b1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(d)** Finish your training with an update step: apply the adaptation rule with a learning rate $\\varepsilon=1$ to obtain the updated network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d63536",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ε = 1.0\n",
    "params = jax.tree_util.tree_map(lambda p, g: p +  ε * g, params, grads)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584c146",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert jnp.array_equal(params['params']['layers_0']['kernel'].T,\n",
    "                       jnp.array([[-2.2500,  3.5000], [ 1.8125,  0.6250]]))\n",
    "assert jnp.array_equal(params['params']['layers_2']['kernel'].T,\n",
    "                       jnp.array([[ 4.1250, -0.5000], [-2.1250,  0.0000]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
