{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6740d9be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (Summer 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328c969",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Practice Session 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41c9bb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "May 7th, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e57c5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lukas Niehaus & Ulf Krumnack\n",
    "\n",
    "Institute of Cognitive Science,\n",
    "University of OsnabrÃ¼ck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06680e34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ee9c8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* New Sheet04\n",
    "* kMeans Introduction\n",
    "* kMeans RGB\n",
    "* kMeans Handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00771f74",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447871c0",
   "metadata": {},
   "source": [
    "$$ \\text{Minimize E}(D, \\mathbf{w}_i) = \\frac{1}{|D|} \\sum_{i=1...|D|}||\\mathbf{x}_i - \\mathbf{w}_{m(\\mathbf{x}_i})||^2  $$\n",
    "with\n",
    "\n",
    "Dataset: $D = \\{\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_{|D|}\\}$\n",
    "\n",
    "Clusters: $C_1, C_2, ..., C_K$\n",
    "\n",
    "Cluster centers: $\\mathbf{w}_1, \\mathbf{w}_2, ..., \\mathbf{w}_K$ with $\\mathbf{w}_k = \\frac{1}{|C_i|}\\sum_{\\mathbf{x}_i \\in C_k} \\mathbf{x_i}$\n",
    "\n",
    "Best matching cluster center for a $\\mathbf{x}_i$: $\\mathbf{w}_m$ with $\\mathbf{w}_m(\\mathbf{x}_i) = argmin_j||\\mathbf{x}_i - \\mathbf{w}_j||$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567f6c1",
   "metadata": {},
   "source": [
    "![kmeans.png](kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916d4db",
   "metadata": {},
   "source": [
    "## K-means for color clustering in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6400c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import imageio.v3 as iio\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "%matplotlib ipympl\n",
    "# from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "\n",
    "img = iio.imread('peppers.png', pilmode = 'RGB')\n",
    "\n",
    "def kmeans_rgb(img, k, threshold=0, do_display=None):\n",
    "    \"\"\"\n",
    "    k-means clustering in RGB space.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): an RGB image\n",
    "        k (int): the number of clusters\n",
    "        threshold (float): Maximal change for convergence criterion.\n",
    "        do_display (bool): Whether or not to plot, intermediate steps.\n",
    "        \n",
    "    Results:\n",
    "        cluster (numpy.ndarray): an array of the same size as `img`,\n",
    "            containing for each pixel the cluster it belongs to\n",
    "        centers (numpy.ndarray): 'number of clusters' x 3 array. \n",
    "            RGB color for each cluster center.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transform image into n_pixels 3-dimensional vectors.\n",
    "    vec = img.reshape((-1, img.shape[2]))\n",
    "    n_pixels = vec.shape[0]\n",
    "\n",
    "    # Initialize random center vectors from data set.\n",
    "    random_indices = np.random.choice(n_pixels, size=k, replace=False)\n",
    "    centers = vec[random_indices]\n",
    "    print(centers.shape)\n",
    "\n",
    "    change = float('Inf')\n",
    "    while change > threshold:\n",
    "        # Remember previous centers.\n",
    "        old_centers = centers.copy()\n",
    "            \n",
    "        # Calculate distance and best matching center vector.\n",
    "        cluster = distance.cdist(vec, centers).argmin(axis=1)\n",
    "\n",
    "        # Recalculate cluster centers.\n",
    "        for i in range(k):\n",
    "            idx = cluster == i\n",
    "            if idx.any():\n",
    "                centers[i] = vec[idx].mean(axis=0)\n",
    "            else:\n",
    "                # No vector is a match for this center vector.\n",
    "                # Re-initialize center vector.\n",
    "                centers[i] = vec[np.random.randint(n_pixels)]\n",
    "\n",
    "        change = np.sum(np.linalg.norm(centers - old_centers))\n",
    "        \n",
    "        if do_display:\n",
    "            plt.imshow(centers[cluster].reshape(img.shape))\n",
    "            plt.title('change: {:.2f}'.format(change))\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            time.sleep(0.1)\n",
    "        elif do_display is not None:\n",
    "            print(change)\n",
    "        \n",
    "    cluster = cluster.reshape(img.shape[:2])\n",
    "   \n",
    "    return cluster, centers\n",
    "\n",
    "theta = 0.01\n",
    "def cb(k):\n",
    "    cluster, centers_rgb = kmeans_rgb(img, k, theta)\n",
    "    \n",
    "\n",
    "    centers_random = np.random.rand(centers_rgb.shape[0], centers_rgb.shape[1])\n",
    "\n",
    "    plt.subplot(312)\n",
    "    plt.axis('off'); \n",
    "    plt.imshow(centers_rgb[cluster])\n",
    "    plt.title('clustered')\n",
    "    plt.subplot(313)\n",
    "    plt.axis('off'); \n",
    "    plt.imshow(centers_random[cluster])\n",
    "    plt.title('pseudo')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c5b92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 15))\n",
    "plt.subplot(311); plt.axis('off'); plt.imshow(img); plt.title('original')\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.show()\n",
    "interact(cb, k=widgets.IntSlider(min=1,max=32,step=1,value=7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cacbc",
   "metadata": {},
   "source": [
    "# Demo 1: Clustering hadwritten digits\n",
    "\n",
    "Based on [A demo of K-Means clustering on the handwritten digits data](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html) from the scikit learn website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ef104",
   "metadata": {},
   "source": [
    "In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.\n",
    "\n",
    "As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e93f4e",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We will start by loading the `digits` dataset. This dataset contains handwritten digits from 0 to 9. In the context of clustering, one would like to group images such that the handwritten digits on the image are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60225938",
   "metadata": {},
   "source": [
    "## Inspect the data\n",
    "\n",
    "It is usually a good idea to first inspect the data a bit before doing further processing. This way you can confirm that data is loaded as expected, and it may give hints what problems to expect and which methods to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n",
    "print(f\"Shape and dtype the data: {data.shape}, dtype={data.dtype}, ({data.min()}-{data.max()})\")\n",
    "print(f\"Shape and dtype of the labels: {labels.shape}, dtype={labels.dtype}, ({labels.min()}-{labels.max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90318a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rows, columns = 2,4\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.gray()\n",
    "for row, column in np.ndindex(rows, columns):\n",
    "    plt.subplot(rows, columns, row*columns + column + 1)\n",
    "    index = np.random.choice(n_samples)\n",
    "    plt.title(f\"labels[{index}] = {labels[index]}\")\n",
    "    plt.imshow(data[index].reshape(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c472f",
   "metadata": {},
   "source": [
    "## Define our evaluation benchmark\n",
    "\n",
    "We will first our evaluation benchmark. During this benchmark, we intend to compare different initialization methods for KMeans. Our benchmark will:\n",
    " * create a pipeline which will scale the data using a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler);\n",
    " * train and time the pipeline fitting;\n",
    " * measure the performance of the clustering obtained via different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f689a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def bench_k_means(kmeans, name, data, labels):\n",
    "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kmeans : KMeans instance\n",
    "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
    "        already set.\n",
    "    name : str\n",
    "        Name given to the strategy. It will be used to show the results in a\n",
    "        table.\n",
    "    data : ndarray of shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    labels : ndarray of shape (n_samples,)\n",
    "        The labels used to compute the clustering metrics which requires some\n",
    "        supervision.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - t0\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "\n",
    "    # Define the metrics which require only the true labels and estimator\n",
    "    # labels\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "\n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(data, estimator[-1].labels_,\n",
    "                                 metric=\"euclidean\", sample_size=300)\n",
    "    ]\n",
    "\n",
    "    # Show the results\n",
    "    formatter_result = (\"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\"\n",
    "                        \"\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\")\n",
    "    print(formatter_result.format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54acde0",
   "metadata": {},
   "source": [
    "Cluster quality metrics evaluated (see [Clustering performance evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation) for definitions and discussions of the metrics):\n",
    "* homogeneity score (homo): how homegenous are the clusters (do they contain points with only one label - conditional entropy of classes given cluster asignments)? Best is 1.0, worst is 0.0\n",
    "* completeness score (compl): are all members of a class assigned to one cluster (conditional entropy of cluster given class)? Best is 1.0, worst is 0.0\n",
    "* V measure (v-meas): the harmonic mean of homogeneity and completeness. Best is 1.0.\n",
    "* adjusted Rand index (ARI): compares cluster assignments to ground truth class labels (by counting how all pairs of points are labeled). 1.0 is best and 0.0 is worst.\n",
    "* adjusted mutual information (AMI): agreement of the cluster assignment and the ground truth assignment. Best is 1.0.\n",
    "* silhouette coefficient (silhouette): relates similarity of a sample and all other points in its cluster to the similarity of that sample to points in the next neighboring cluster. 1.0 means highly dense and well separated clustering, -1.0 means incorrect clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550a507",
   "metadata": {},
   "source": [
    "## Run the benchmark\n",
    "\n",
    "We will compare three approaches:\n",
    "* an initialization using `kmeans++`. This method is stochastic and we will run the initialization `n_init` times;\n",
    "* a random initialization. This method is stochastic as well and we will run the initialization `n_init` times;\n",
    "* an initialization based on a PCA projection. Indeed, we will use the components of the PCA to initialize KMeans. This method is deterministic and a single initialization suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_init=4\n",
    "\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=n_init,\n",
    "                random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\n",
    "\n",
    "kmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=n_init, random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\n",
    "\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\n",
    "bench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\n",
    "\n",
    "print(82 * '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6d2578",
   "metadata": {},
   "source": [
    "## Visualize the results on PCA-reduced data\n",
    "\n",
    "PCA allows to project the data from the original 64-dimensional space into a lower dimensional space. Subsequently, we can use PCA to project into a 2-dimensional space and plot the data and the clusters in this new space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0843a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation=\"nearest\",\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n",
    "\n",
    "print(reduced_data.shape, labels.shape)\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=169, linewidths=3,\n",
    "            color=\"w\", zorder=10)\n",
    "plt.title(\"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n",
    "          \"Centroids are marked with white cross\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd57774",
   "metadata": {},
   "source": [
    "## Clustering after dimension reduction\n",
    "\n",
    "Dimension reduction may improve the results of many machine learning algorithm, as it reduces redundancy and removes small variations which often are due to noise.  The following code compares different degrees of dimension reduction with PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa02272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=4)\n",
    "bench_k_means(kmeans=kmeans, name=f\"Original ({data.shape[1]})\", data=data, labels=labels)\n",
    "\n",
    "# compute principal components\n",
    "for n_components in (32,16,8,4,2,1):\n",
    "    pca = PCA(n_components=n_components).fit(data.T)\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=4)\n",
    "    bench_k_means(kmeans=kmeans, name=f\"{n_components} PCs\", data=pca.components_.T, labels=labels)\n",
    "\n",
    "print(82 * '_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
