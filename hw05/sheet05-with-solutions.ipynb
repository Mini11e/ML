{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2b7a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2024) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Lukas Niehaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e64200",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 05: Concept Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccd89c",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 19, 2024**. If you need help (and Google and other resources were not enough), feel free to post in the StudIP forum, contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45231d0",
   "metadata": {},
   "source": [
    "This sheet focuses on lecture **ML-02_Concept_Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f2305",
   "metadata": {},
   "source": [
    "# Changed Practice/Lecture Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39972c1b",
   "metadata": {},
   "source": [
    "there are shifts for this and the next weeks for the practice and lecture sessions.\n",
    "This week (20) will be completely lecture sessions.\n",
    "The next week (21) will be completely practice sessions.\n",
    "The week after that (22) will be completely lecture sessions.\n",
    "After that the usual cycle of practice at tuesdays and lecture on wednesdays and thursdays will continue.\n",
    "\n",
    "The information as a list looks as follows:\n",
    "<ol>\n",
    "    <li>Lectures in calender week 20:</li>\n",
    "        <ol>\n",
    "            <li>14.05.2024 14:00-16:00</li>\n",
    "            <li>15.05.2024 10:00-12:00</li>\n",
    "            <li>16.05.2024 10:00-12:00</li>\n",
    "        </ol>\n",
    "    <li>Practices in calender week 21:</li>\n",
    "        <ol>\n",
    "            <li>21.05.2024 14:00-16:00</li>\n",
    "            <li>22.05.2024 10:00-12:00</li>\n",
    "            <li>23.05.2024 10:00-12:00</li>\n",
    "        </ol>\n",
    "    <li>Lectures in calender week 22:</li>\n",
    "        <ol>\n",
    "            <li>28.05.2024 14:00-16:00</li>\n",
    "            <li>29.05.2024 10:00-12:00</li>\n",
    "            <li>30.05.2024 10:00-12:00</li>\n",
    "        </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fdf54",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aad21d3e70ca29e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: Learning Tasks (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de97dd2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-573de581339f4602",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(a)** Give three computer applications for which machine learning approaches seem \n",
    "appropriate and three for which they seem inappropriate. Pick applications that are not\n",
    "already mentioned in the lecture, and include a one-sentence justification for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622cada",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7daad0c311c13e13",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "***Appropriate applications for machine learning:**\n",
    "\n",
    "**Medical diagnosis:** Machine learning can help to improve the accuracy of medical diagnoses by analyzing large amounts of patient data and identifying patterns that humans might miss. For example, machine learning can be used to detect early signs of diseases like cancer or Alzheimer's.\n",
    "\n",
    "**Fraud detection:** Machine learning algorithms can learn to detect patterns of fraudulent behavior in financial transactions, such as credit card fraud or insurance fraud, and flag suspicious activity for further investigation.\n",
    "\n",
    "**Natural language processing:** Machine learning can be used to develop applications that understand and process human language, such as chatbots or voice assistants, by analyzing large datasets of human language and learning how to interpret and respond to it.\n",
    "\n",
    "***Inappropriate applications for machine learning:**\n",
    "\n",
    "**Mathematical proof verification:** While machine learning can be used for mathematical problem-solving, it is not well-suited to verify the correctness of mathematical proofs, which requires a high degree of logical reasoning and formalism.\n",
    "\n",
    "**Artistic creativity:** While machine learning algorithms can generate art, music, and other creative works, they do not possess the creativity, intuition, and emotional intelligence required to produce truly original and meaningful works of art.\n",
    "\n",
    "<!--\n",
    "**Self-driving cars:** While machine learning can be used to improve the performance of self-driving cars, it is not sufficient on its own to ensure their safety, as it is difficult to anticipate all possible scenarios that a self-driving car might encounter on the road.\n",
    "-->\n",
    "\n",
    "**Simple Calculations:** While machine learning can be used to calculate simple mathematical problems like adding numbers or computing trigonometric functions it is inappropriate because these tasks can be solved much more efficient by standard programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9caee",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75f5683d29ef7800",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(b)** Pick some learning task not mentioned in the lecture. Describe it informally in a paragraph in English. Now describe it by stating as precisely as possible the task, performance measure, and training experience. Finally, propose a target function to be learned and a target representation. Discuss the main tradeoffs you considered in formulating this learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c577f40",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ded8a09479800067",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Learning Task:** Predicting Housing Prices\n",
    "\n",
    "In this task, the goal is to predict the selling price of a house given its features such as number of bedrooms, bathrooms, square footage, etc. The task is to learn a function that maps the features of the house to the predicted selling price. The performance measure could be the mean squared error between the predicted selling price and the actual selling price. The training experience could involve a dataset of historical housing prices and their features.\n",
    "\n",
    "Formal Task Description:\\\n",
    "**Task:** Given a set of features of a house, predict its selling price.\\\n",
    "**Performance measure:** Mean squared error between predicted selling price and actual selling price.\\\n",
    "**Training experience:** A dataset of historical housing prices and their features.\\\n",
    "**Target function:** f: features -> selling price\\\n",
    "**Target representation:** A supervised learning algorithm that maps the features of the house to the predicted selling price.\n",
    "\n",
    "**Tradeoffs:**\n",
    "One of the main tradeoffs in this task is the choice of features to include in the model. Including too many features could lead to overfitting and poor generalization to new data. On the other hand, not including enough features could result in a model that is too simple and does not capture all the important factors that affect the housing prices. Another tradeoff is the choice of model complexity. A more complex model such as a deep neural network could potentially achieve better performance on the training data but could also lead to overfitting and poor generalization. A simpler model such as linear regression may not achieve the best performance on the training data, but may generalize better to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8b246",
   "metadata": {},
   "source": [
    "**(c)** Prove that the LMS weight update rule described in (ML-01, slide 19) performs a gradient descent to minimize the squared error given on that slide: calculate the derivative of $E$ with respect to the weight $w_i$, assuming that $V(b)$ is a linear function. Gradient descent is achieved by updating each weight in proportion to $-\\partial E/\\partial w_i$. Therefore, you must show that the LMS training rule alters weights in this proportion for each training example it encounters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269cf55",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ed7d79641fd2df77",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "To show that the LMS weight update rule performs a gradient descent to minimize the squared error, we need to demonstrate that it updates the weights in proportion to the negative gradient of the squared error.\n",
    "\n",
    "Recall that the LMS weight update rule is: \n",
    "$w_i \\gets w_i + \\varepsilon(t-y)f_i$\n",
    "where $\\mathbf{w}$ is the weight vector, $\\varepsilon$ is the learning rate, $t$ is the true output for the training example, $y$ is the predicted output, and $\\mathbf{f}$ is the feature vector.The squared error for this training example is: \n",
    "\n",
    "$$E = (t-y)^2$$\n",
    "To minimize this squared error with respect to the weight $w_i$, we take the derivative of $E$ with respect to $w_i$:\\\n",
    "$$\\partial E/\\partial w_i  = ‚àí2(t-y)\\partial y/\\partial w_i $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "which is precisely the negative gradient of the squared error scaled by the learning rate $\\varepsilon$. Therefore, the LMS weight update rule performs a gradient descent to minimize the squared error. Now, assuming that $V(b)$ is a linear function, we can express the predicted output $y$ as:\n",
    "\n",
    "$$y = V(b) = \\sum_i w_if_i$$\n",
    "\n",
    "\n",
    "To compute $\\frac{\\partial y}{\\partial w_i}$, since $V(b)$ is a linear function, we have:\n",
    "\n",
    "\n",
    "\n",
    "$$\\partial y/\\partial w_i = f_i$$\n",
    "\n",
    "\n",
    "Substituting these expressions into our derivative of $E$ with respect to $w_i$, we get:\n",
    "\n",
    "$$\\partial E/\\partial w=-2(t-y)f_i$$\n",
    "\n",
    "To pluck the formula into the update rule we have to divide by $-\\frac12$\n",
    "$$-\\tfrac12\\partial E/\\partial w=(t-y)f_i$$\n",
    "\n",
    "Thus, the weight update rule becomes:\n",
    "$$w_i \\gets w_i + \\varepsilon(t-y)f_i = w_i \\gets w_i - \\varepsilon \\frac12 \\partial E/\\partial w_i$$\n",
    "\n",
    "Hence, we have proved that the LMS weight update rule performs gradient descent to minimize the squared error for each training example it encounters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a2fb8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0903956896707a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Candidate Elimination by Hand (6 points)\n",
    "\n",
    "Candidate Elimination is a learning algorithm that, in each step, tries to generate a description which is consistent with all previously observed examples in a training set. That description could hypothetically then be used to classify examples outside the training set.\n",
    "\n",
    "Consider the following situation:\n",
    "\n",
    "Earl and Fran have made it their mission to visit as many amusement parks as possible after the pandemic. However, to maximize their enjoyment and not have any unnecessary arguments break out, they make a list of previous park visits and if they would go there again, to have a few criteria to decide if a park is worth their time.\n",
    "\n",
    "This is the set of attributes along with their possible values Earl and Fran came up with:\n",
    "\n",
    "| Attribute           | driving distance | ticket price      | rollercoasters | dinosaurs |\n",
    "|---------------------|------------------|-------------------|----------------|-----------|\n",
    "| **Possible Values** | short / far      | cheap / expensive | many / none    | yes / no  |\n",
    "\n",
    "This is Earl and Fran's accumulated data from previous visits. The list will allow you to come to a learning decision which properties have to be fulfilled such that the two will enjoy a visit to an amusement park.\n",
    "\n",
    "| Sample No. | driving distance | ticket price | rollercoasters | dinosaurs | go again? |\n",
    "|------------|------------------|--------------|----------------|-----------|-----------|\n",
    "| 1          | far              | cheap        | many           | no        | yes       |\n",
    "| 2          | short            | expensive    | many           | no        | yes       |\n",
    "| 3          | far              | expensive    | none           | yes       | no        |\n",
    "| 4          | short            | cheap        | none           | yes       | no        |\n",
    "| 5          | short            | cheap        | many           | yes       | yes       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef149b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f5cd6d758c895950",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Apply Candidate Elimination to the samples 1-5 below and provide the version space boundaries $S_n$ and $G_n$ after each new training sample.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76da90c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ab417b404920afe1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$G_0 = \\{<?, \\,?, \\,?, \\,?>\\} \\quad S_0 = \\{<\\emptyset, \\,\\emptyset, \\,\\emptyset, \\,\\emptyset>\\}$\n",
    "\n",
    "Insert sample 1: `(far, cheap, many, no) = yes`\n",
    "\n",
    "$G_1 = \\{<?, \\,?, \\,?, \\,?>\\} \\quad S_1 = \\{<\\textrm{far}, \\,\\textrm{cheap}, \\,\\textrm{many}, \\,\\textrm{no}>\\}$\n",
    "\n",
    "Insert sample 2: `(short, expensive, many, no) = yes`\n",
    "\n",
    "$G_2 = \\{<?, \\,?, \\,?, \\,?>\\} \\quad S_2 = \\{<?, \\,?, \\,\\textrm{many}, \\,\\textrm{no}>\\}$\n",
    "\n",
    "Insert sample 3: `(far, expensive, none, yes) = no`\n",
    "\n",
    "$G_3 = \\{<?, \\,?, \\,\\textrm{many}, \\,?>,~<?, \\,?, \\,?, \\,\\textrm{no}>\\} \\quad \n",
    "S_3 = \\{<?, \\,?, \\,\\textrm{many}, \\,\\textrm{no}>\\}$\n",
    "\n",
    "Insert sample 4: `(short, cheap, none, yes) = no`\n",
    "\n",
    "$G_4 = \\{<?, \\,?, \\,\\textrm{many}, \\,?>,~<?, \\,?, \\,?, \\,\\textrm{no}>\\} \\quad \n",
    "S_4 = \\{<?, \\,?, \\,\\textrm{many}, \\,\\textrm{no}>\\}$\n",
    "\n",
    "Insert sample 5: `(short, cheap, many, yes) = yes`\n",
    "\n",
    "$G_5 = \\{<?, \\,?, \\,\\textrm{many}, \\,?>\\}\\quad \n",
    "S_5 = \\{<?, \\,?, \\,\\textrm{many}, \\,?>\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4981cc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5db1c57702dcd02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Provide the complete version space bounded by $S_2$ and $G_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5afbda",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ef42d1e895e9cff6",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$\\quad \\quad \\quad \\quad <?, \\,?, \\,?, \\,?>$\n",
    "\n",
    "$<?, \\,?, \\,\\textrm{many}, \\,?> \\quad <?, \\,?, \\,?, \\,\\textrm{no}>$\n",
    "\n",
    "$\\quad \\quad \\quad <?, \\,?, \\,\\textrm{many}, \\,\\textrm{no}>$\n",
    "\n",
    "The version space is made up of all four sets, including the boundaries. See ML-02 slide 18 and 27. The complete version space therefore is the following set:\n",
    "\n",
    "$VS_{D_2, \\mathcal{H}} = \\{<?, \\,?, \\,?, \\,?>,~ <?, \\,?, \\,\\textrm{many}, \\,?>,~ <?, \\,?, \\,?, \\,\\textrm{no}>,~ <?, \\,?, \\,\\textrm{many}, \\,\\textrm{no}>\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e8aaa",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2375670feb946ca2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** To what kind of amusement park should Earl and Fran go?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a91db",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8aa8761eb3b1780c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Earl and Fran should choose amusement parks with many rollercoasters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df987a50",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0e07cdee1f5cee5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: Candidate Elimination in Python (6 points)\n",
    "\n",
    "Now let's get to real fun part: Programming our first machine learning algorithm!\n",
    "\n",
    "But first some general remarks on coding style:\n",
    "\n",
    "In general, try to write code that's consistent with [PEP8](https://www.python.org/dev/peps/pep-0008/), the offical Python Style Guide. Have a look a the [Google Style Guide](https://google.github.io/styleguide/pyguide.html) as well. It's based on PEP8 and has some nice examples. As a bonus always document your function and classes with docstrings. It is best practice to stick to some docstring convention for example [Google's](http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html). This has the big advantages that it allows for automated generation of documentation. We will follow that convention in the code we provide you, so you know what kind of objects to expect and what to return.\n",
    "\n",
    "If, during the programming tasks, you run into a `NameError`, make sure that you executed all prior code cells beforehand. Later cells might rely on variables and function from prior cells. To see all currently defined variables you can make use of the `%whos` [magic function](https://github.com/lmmx/devnotes/wiki/IPython-'magic'-function-documentation#whos) anywhere in code cells. Additionally, it is sometimes handy to run all cells from the beginning by opeining the command palette typing `run all cells`. Moreover, using <kbd>b</kbd> to add new cells below and <kbd>a</kbd> for adding new cells above your current cell will make your life often easier. Finally, using <kbd>l</kbd>  to show line numbers is helpful for locating errors from error messages.\n",
    "\n",
    "In the following Python code we have provided the building blocks for the `CANDIDATE-ELIMINATION` algorithm and put them together in a single function. Now you have to fill those building blocks with actual code. There are places marked with \n",
    "\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "``` \n",
    "\n",
    "where you have to add some code to make the code work. Finish the code to automate the decision making for Earl and Fran!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021f8b82",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_solution1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def is_consistent(h, datum):\n",
    "    \"\"\"\n",
    "    Checks if a general hypothesis is consistent with a datum.\n",
    "    \n",
    "    Args:\n",
    "        h (tuple): Hypothesis. '?' indicates all values possible, '0' indicates no value possible.\n",
    "        datum (dict): Datum with 'values' (tuple) and 'target' (bool).\n",
    "        \n",
    "    Returns:\n",
    "        bool: Whether the hypothesis correctly predicts the target value.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    prediction = all(h_attr == datum_attr or h_attr == '?' for h_attr, datum_attr in zip(h, datum['values']))\n",
    "    return prediction == datum['target']\n",
    "    ### END SOLUTION\n",
    "\n",
    "assert is_consistent(('far', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': True })\n",
    "assert not is_consistent(('far', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': False })\n",
    "assert is_consistent(('short', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': False })\n",
    "assert not is_consistent(('short', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': True })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce5736a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_solution2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def is_more_general_or_equal(h1, h2):\n",
    "    \"\"\"\n",
    "    Checks whether hypothesis h1 is more general than hypothesis h2 or equally general.\n",
    "    \n",
    "    Args:\n",
    "        h1 (tuple): Hypothesis 1.\n",
    "        h2 (tuple): Hypothesis 2.\n",
    "           '?' indicates all values possible, '0' indicates no value possible.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the predicate is satisfied.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return all(h1_attr == '?' or h1_attr == h2_attr or h2_attr == '0' for h1_attr, h2_attr in zip(h1, h2))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "assert is_more_general_or_equal(('?', '?', '?', '?'), ('far', 'cheap', 'many', 'no'))\n",
    "assert not is_more_general_or_equal(('?', '?', 'many', 'no'), ('far', 'cheap', 'many', '?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e467421",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_solution3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def is_more_specific_or_equal(h1, h2):\n",
    "    \"\"\"\n",
    "    Checks whether hypothesis h1 is more specific than hypothesis h2 or equally specific.\n",
    "    \n",
    "    Args:\n",
    "        h1 (tuple): Hypothesis 1.\n",
    "        h2 (tuple): Hypothesis 2.\n",
    "           '?' indicates all values possible, '0' indicates no value possible.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the predicate is satisfied.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return is_more_general_or_equal(h2, h1)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "assert is_more_specific_or_equal(('?', '?', 'many', 'no'), ('?', '?', 'many', '?'))\n",
    "assert not is_more_specific_or_equal(('?', 'cheap', 'many', 'no'), ('far', '?', '?', '?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f308a7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_solution4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generalize_minimally(h, datum):\n",
    "    \"\"\"Generalize hypothesis h so it becomes consitent with the datum.\n",
    "    \n",
    "    Args:\n",
    "        h (tuple): The hypothesis to be generalized.\n",
    "                   '?' indicates all values possible, '0' indicates no value possible.\n",
    "        datum (tuple): Attribute values of a datum. The datum is assumed to have a positive target value.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The generalized hypothesis.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    if '0' in h:\n",
    "        # If the hypothesis classifies everything as negative replace it by the sample.\n",
    "        return datum\n",
    "    else:\n",
    "        # Else generalize every non-matching attribute contraint, i.e. replace with ?. \n",
    "        h = list(h)\n",
    "        for i in range(len(h)):\n",
    "            if h[i] != datum[i]:\n",
    "                h[i] = '?'\n",
    "        return tuple(h)\n",
    "    ### END SOLUTION\n",
    "\n",
    "assert generalize_minimally(('?', '?', 'many', 'no'), ('short', 'cheap', 'many', 'yes')) == ('?', '?', 'many', '?')\n",
    "assert generalize_minimally(('0', '0', '0', '0'), ('short', 'cheap', 'many', 'yes')) == ('short', 'cheap', 'many', 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f5520a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_solution5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def specialize_minimally(h, datum, attr_values):\n",
    "    \"\"\"\n",
    "    Generate all consistent minimal specialization of hypothesis h.\n",
    "    \n",
    "    Args:\n",
    "        h (tuple): The hypothesis to be specialized.\n",
    "        datum (tuple): Attribute values of a datum. The datum is assumed to have a negative target value.\n",
    "        attr_values (tuple of tuples): All possible attribute values for each attribute.\n",
    "    \n",
    "    Returns:\n",
    "        tuple of tuples: Tuple of the specialized hypotheses.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    minmal_specializations = []\n",
    "\n",
    "    for attr_idx in range(len(attr_values)):\n",
    "        # Replace each questionmark with the opposite of the datum.\n",
    "        if h[attr_idx] == '?':\n",
    "            # Convert to list to be able to manipulate the hypothesis.\n",
    "            specialized_h = list(h)\n",
    "            # 1 - for taking the alternative to the current example.\n",
    "            specialized_h[attr_idx] = attr_values[attr_idx][\n",
    "                1 - attr_values[attr_idx].index(datum[attr_idx])]\n",
    "            minmal_specializations.append(tuple(specialized_h))\n",
    "\n",
    "    return tuple(minmal_specializations)\n",
    "    ### END SOLUTION\n",
    "\n",
    "attr_values = (('short', 'far'), ('cheap', 'expensive'), ('many', 'none'), ('yes', 'no'))\n",
    "assert specialize_minimally(('?', '?', 'many', 'no'), ('short', 'cheap', 'many', 'no'), attr_values) == (('far', '?', 'many', 'no'), ('?', 'expensive', 'many', 'no'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9193a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eed96524af5d38e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now check that the algorithm works in the intended way by excecuting the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694b1d42",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ff73aa56439d69d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def eliminate_candidates(data, attr_values):\n",
    "    \"\"\"\n",
    "    Candidate elimination algorithm printing its progress at each step.\n",
    "    \n",
    "    Args:\n",
    "        data (list of dicts): The dataset.\n",
    "        attr_values (tuple of tuples): All possible attribute values for each attribute in the data.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: The general and the specific boundary of the version space.\n",
    "    \"\"\"\n",
    "    # Initialize general and specific boundary.\n",
    "    \n",
    "    # Maximally general hypothesis.\n",
    "    general_boundary = [tuple('?') * len(attr_values)]\n",
    "    # Maximally specific hypothesis.\n",
    "    specific_boundary = [tuple('0') * len(attr_values)]\n",
    "    \n",
    "    # Fit the version space to the data.\n",
    "    for datum in data:\n",
    "        if datum['target']:\n",
    "            # If the sample is classified as positive...\n",
    "            \n",
    "            # Remove all inconsistent hypotheses from G.\n",
    "            general_boundary = [g for g in general_boundary if is_consistent(g, datum)]\n",
    "            \n",
    "            for s in specific_boundary:\n",
    "                # Remove all inconsistent hypothesis from S.\n",
    "                if not is_consistent(s, datum):\n",
    "                    specific_boundary.remove(s)\n",
    "\n",
    "                    # Add to S all minimal generalizations s. In this case only one.\n",
    "                    s_generalized = generalize_minimally(s, datum['values'])\n",
    "                    # Add the new hypothesis to the specific boundary, if it is not more general \n",
    "                    # than the general boundary. We do not need to check for consistency again\n",
    "                    # as the hypothesis was constructed in such a way that it must be consistent.\n",
    "                    if any(is_more_general_or_equal(g, s_generalized) for g in general_boundary):\n",
    "                        specific_boundary.append(s_generalized)\n",
    "\n",
    "            # Remove from S any hypothesis that is more general than another hypothesis in S.\n",
    "            for s in specific_boundary:\n",
    "                if any(is_more_general_or_equal(s, s2) \n",
    "                       and not s == s2 for s2  \n",
    "                       in specific_boundary):\n",
    "                    \n",
    "                    specific_boundary.remove(s)\n",
    "\n",
    "        else:\n",
    "            # If the sample is classified as negative...\n",
    "            \n",
    "            # Remove all inconsistent hypotheses from S.\n",
    "            specific_boundary = [s for s in specific_boundary if is_consistent(s, datum)]\n",
    "            for g in general_boundary:\n",
    "                # Remove all inconsistent hypotheses from G.\n",
    "                if not is_consistent(g, datum):\n",
    "                    general_boundary.remove(g)\n",
    "\n",
    "                    # Add to G all minimal specializations of g.\n",
    "                    for specialized_g in specialize_minimally(g, datum['values'], attr_values):\n",
    "                        # Add the new specialized hypothesis to the general boundary, if it is not more \n",
    "                        # specific than the specific boundary.\n",
    "                        # We do not need to check for consistency again\n",
    "                        # as the hypothesis was constructed in such a way that it must be consistent.\n",
    "                        if any(is_more_specific_or_equal(s, specialized_g) for s in specific_boundary):\n",
    "                                \n",
    "                                general_boundary.append(specialized_g)\n",
    "                \n",
    "                # Remove from G any hypothesis that is less general than another hypothesis in G.\n",
    "                for g in general_boundary:\n",
    "                    if any(is_more_specific_or_equal(g, g2) \n",
    "                           and not g == g2 for g2 \n",
    "                           in general_boundary):\n",
    "                        \n",
    "                        general_boundary.remove(g)\n",
    "        \n",
    "        # Print progress of algorithm at each iteration.\n",
    "        print('Sample: {} {}\\nG: {}\\nS: {}\\n'.format('+' if datum['target'] else '-', datum['values'],\n",
    "                                                     general_boundary, specific_boundary))\n",
    "        \n",
    "    return general_boundary, specific_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fdf5e9",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2_test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: + ('far', 'cheap', 'many', 'no')\n",
      "G: [('?', '?', '?', '?')]\n",
      "S: [('far', 'cheap', 'many', 'no')]\n",
      "\n",
      "Sample: + ('short', 'expensive', 'many', 'no')\n",
      "G: [('?', '?', '?', '?')]\n",
      "S: [('?', '?', 'many', 'no')]\n",
      "\n",
      "Sample: - ('far', 'expensive', 'none', 'yes')\n",
      "G: [('?', '?', 'many', '?'), ('?', '?', '?', 'no')]\n",
      "S: [('?', '?', 'many', 'no')]\n",
      "\n",
      "Sample: - ('short', 'cheap', 'none', 'yes')\n",
      "G: [('?', '?', 'many', '?'), ('?', '?', '?', 'no')]\n",
      "S: [('?', '?', 'many', 'no')]\n",
      "\n",
      "Sample: + ('short', 'cheap', 'many', 'yes')\n",
      "G: [('?', '?', 'many', '?')]\n",
      "S: [('?', '?', 'many', '?')]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('?', '?', 'many', '?')], [('?', '?', 'many', '?')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_values = (('short', 'far'), ('cheap', 'expensive'), ('many', 'none'), ('yes', 'no'))\n",
    "\n",
    "# samples\n",
    "data = [ \n",
    "    {'values': ('far',   'cheap',     'many', 'no' ), 'target': True },\n",
    "    {'values': ('short', 'expensive', 'many', 'no' ), 'target': True },\n",
    "    {'values': ('far',   'expensive', 'none', 'yes'), 'target': False},\n",
    "    {'values': ('short', 'cheap',     'none', 'yes'), 'target': False},\n",
    "    {'values': ('short', 'cheap',     'many', 'yes'), 'target': True }\n",
    "]\n",
    "\n",
    "eliminate_candidates(data, attr_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5468a90",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8704cc99397489d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Inductive Bias (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22534f87",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0cac3f19657cf08e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** What is an inductive bias? Describe the concept in your own words! Why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a6935",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ef3d6719a20fcc57",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The *inductive bias* (also *learning bias*) is any form of prior assumption regarding the\n",
    "identity of the target function. As the goal of machine learning is to predict this function\n",
    "based on limited evidence, the learner has to make some additional assumptions that\n",
    "allow her to deal with cases that reach beyond what has been seen during training. The\n",
    "assumptions the learner makes are the inductive bias. It determines the inferences the learner can make.\n",
    "Without such a bias, a learner has no rational basis for dealing with any unseen data.\n",
    "\n",
    "That means that any preference of the algorithm for a specific set of hypotheses is considered as *inductive bias*. Knowing the *inductive bias* and the training observations one should be able to predict how the learner behaves. More formally, given a dataset $D$ the *inductive bias* is the mininmal set of assumptions $B$ such that the output of the learner $L$ over the instance space $X$ follows deductively\n",
    "\n",
    "$$\\forall x \\in X: (B \\land D \\land x) \\vdash L(x, D)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821511ad",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2f09c6e12b491df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Describe and compare the inductive bias for the learning algorithms you heard about in the lecture (Candidate Elimination and Find-S)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ca901",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4ebc56901ad0ad00",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Both algorithms restrict the hypothesis space by restricting hypotheses to concepts of a specific form (restriction bias). In the examples from the lecture this was the assumption that hypotheses can be expressed in the form $\\langle c_1,\\ldots,c_n\\rangle$ with $c_i\\in\\{\\emptyset,a_j,?\\}$. One assumption of both algorithms is, that the target concept is contained in this restricted hypothesis space.\n",
    "\n",
    "The inductive bias of the Find-S Algorithm is that the resulting hypothesis is the most specific hypothesis (and the hidden assumption that the target concept is consistent with all training examples). The specific structure of the hypothesis space allows Find-S to always come up with exactly one result.\n",
    "\n",
    "The Candidate Elimination algorithm needs no further assumptions besides the assumption that the target concept is contained in the hypotheses space (and again that the target concept is consistent with all training examples). In many cases this does not allow to single out one result and hence the algorithm may be inconclusive how to judge certain new instances.\n",
    "\n",
    "In this sense the Find-S has the stronger bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee53fd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c30597e059264a70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** A concept can be considered as a boolean-valued function on the instance space. Asumming a representation using $n$ binary features, how many different concepts exist? Using the concept representation introduced in the lecture (conjuntion of constraints on the attributes), how many of these concepts can be expressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab8491",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b79bd62c4d58ce6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "With $n$ binary features, there are $2^n$ possible feature combinations, each representing a different entity in the underlying universe. (The extension of) a concept is a subset of the universe.  There are in total $2^{(2^n)}$ possible concepts. Using the representation from the lecture, a (conjunctive) concepts has the form $\\langle c_1,\\ldots,c_n\\rangle$ with $c_i\\in\\{0,1,?\\}$ as well as the inconsistent concept $\\square$. This amounts to $3^n+1$ concepts that can be expressed in this form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02101f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e21f11a5856571ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**d)** The *power* of a hypotheses space is defined as the cardinality of the largest set of inputs that can be labeled by the hypotheses in all possible ways. Consider now a representation with $n=2$ binary features. What is the power of the unrestricted hypotheses space? Using the concept representation from the lecture, what is the power of the restricted hypotheses space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794e1cf",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-281f1d049ce89888",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "With $n=2$ binary features, there are $2^n=4$ different entities in the universe, so the largest set of possible inputs is $4$. The unrestricted hypotheses space contains all concepts and hence can label arbitrary sets of inputs in all possible ways, that is the unrestricted hypotheses space has a power of **4**. With the restricted hypotheses space of conjunctive features it is always possible to label input sets with 2 elements in all possible ways: the concept $\\langle?,?\\rangle$ labels both elements as true, the concept $\\langle c_1,c_2\\rangle$ with $c_i\\in\\{0,1\\}$ can label one element as true and the other element as false, and the inconsistent concept $\\square$ labels both elements as false.  The conjunctive hypotheses space can not label 3-element sets in all possible way: e.g. for the set $\\{\\langle 0,0\\rangle,\\langle 1,0\\rangle,\\langle 1,1\\rangle\\}$ there is no conjunctive concept that labels $\\langle0,0\\rangle$ and $\\langle 1,1\\rangle$ as true and $\\langle 1,0\\rangle$ as false. Hence the power of the conjunctive hypotheses space is **2**.\n",
    "\n",
    "![example-binary-lattice.png](example-binary-lattice.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2e7da-bad5-4f59-88ef-cf59c6fbb3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
